{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "bio_file = '../bioASQ/bioASQ.json'\n",
    "\n",
    "def get_data_from_json(filename):\n",
    "    jsonfile = open(covid_file, 'r')\n",
    "    data = jsonfile.read()\n",
    "    jsonfile.close()\n",
    "    return json.loads(data)\n",
    "\n",
    "covid_data = get_data_from_json(covid_file)\n",
    "bio_data = get_data_from_json(bio_file)\n",
    "\n",
    "#datasets.set_caching_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def convert_to_squad_format(data):\n",
    "    covid_qa_squad_format = []\n",
    "    for rows in data['data']:\n",
    "      for context in rows['paragraphs']:\n",
    "        for qa_pairs in context['qas']:\n",
    "\n",
    "          features = {'answers': {'text':[qa_pairs['answers'][0]['text']],\n",
    "                                  'answer_start': np.array([qa_pairs['answers'][0]['answer_start']], dtype=np.int32)},\n",
    "                      'context':str(context['context']),\n",
    "                      'id':str(context['document_id']),\n",
    "                      'question':qa_pairs['question'],\n",
    "                      'title': 'COVID_19'\n",
    "                      }\n",
    "          covid_qa_squad_format.append(features)\n",
    "    covid_df = pd.DataFrame(covid_qa_squad_format)\n",
    "    return (datasets.Dataset.from_dict(covid_df))\n",
    "\n",
    "\n",
    "def make_and_save_full_dataset(covid,squad, path):\n",
    "    full_data = datasets.dataset_dict.DatasetDict({'covid':covid, 'squad':squad})\n",
    "    full_data.save_to_disk(path)\n",
    "\n",
    "def update_dataset_dict(to_remove_per_step, covid,squad, path):\n",
    "    squad.shuffle()\n",
    "    #covid.shuffle()\n",
    "    if squad.num_rows > to_remove_per_step:\n",
    "        #covid = datasets.Dataset.from_dict(covid[:])\n",
    "        squad = datasets.Dataset.from_dict(squad[:-to_remove_per_step])\n",
    "        make_and_save_full_dataset(covid,squad,path)\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return datasets.load_dataset('custom_squad.py', data_files= {'train':filename})['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a396986d6b3a2375\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "Using custom data configuration default-8fdbe041288a2f4d\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-8fdbe041288a2f4d\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n"
     ]
    }
   ],
   "source": [
    "covid_qa = get_dataset(covid_file)\n",
    "bioASQ = get_dataset(bio_file)\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "covid_and_squad_dataset_path = \"../data/squad_and_covidQA/\"\n",
    "#this is just for testing purposes, I am going to make both of these files very small only at max 3000 datasets\n",
    "squad_qa = datasets.Dataset.from_dict(squad_qa[:100])\n",
    "covid_qa = datasets.Dataset.from_dict(covid_qa[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_and_save_full_dataset(covid_qa,squad_qa,covid_and_squad_dataset_path)\n",
    "\n",
    "K = 3\n",
    "to_remove_per_step = int(squad_qa.num_rows / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import run_qa\n",
    "\n",
    "def run_gradual_ft(output_dir, checkpoint, k_fold):\n",
    "    !python run_qa.py \\\n",
    "      --model_name_or_path {checkpoint} \\\n",
    "      --dataset_name ../data/squad_bioASQ_covidQA/ \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --do_predict \\\n",
    "      --per_device_train_batch_size 32\\\n",
    "      --per_device_eval_batch_size 32\\\n",
    "      --evaluation_strategy \"no\" \\\n",
    "      --save_strategy \"no\" \\\n",
    "      --logging_strategy \"epoch\" \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --max_seq_length 384 \\\n",
    "      --doc_stride 128 \\\n",
    "      --k_fold_cross_valid {k_fold} \\\n",
    "      --output_dir {output_dir} \\\n",
    "      --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "==================================================\n",
      "          At Gradual Fine Tuning Step:  1\n",
      "**************************************************\n",
      "==================================================\n",
      "../data/squad_bioASQ_covidQA/\n",
      "../data/squad_bioASQ_covidQA/\n",
      "06/29/2021 01:20:09 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 01:20:09 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline/runs\\Jun29_01-20-09_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gradual_ft_baseline,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline/runs\\Jun29_01-20-09_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gradual_ft_baseline,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 67\n",
      "    })\n",
      "    covid: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    bio: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "06/29/2021 01:20:13 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-a1d9a7b34384b8aa.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-c9a849f723ed8a84.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:20:13 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-ceef28dfb7087b68.arrow\n",
      "06/29/2021 01:20:13 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-3d05abe8550112cf.arrow\n",
      "06/29/2021 01:20:13 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-c10fb48dfe434022.arrow\n",
      "{'loss': 5.0435, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 13.3911, 'train_samples_per_second': 66.686, 'train_steps_per_second': 2.091, 'train_loss': 5.043510981968471, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.0435\n",
      "  train_runtime            = 0:00:13.39\n",
      "  train_samples            =        893\n",
      "  train_samples_per_second =     66.686\n",
      "  train_steps_per_second   =      2.091\n",
      "06/29/2021 01:20:30 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:20:31 - INFO - utils_qa -   Post-processing 5 example predictions split into 160 features.\n",
      "06/29/2021 01:20:32 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-1\\eval_predictions.json.\n",
      "06/29/2021 01:20:32 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     = 160\n",
      "06/29/2021 01:20:32 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:20:33 - INFO - utils_qa -   Post-processing 5 example predictions split into 162 features.\n",
      "06/29/2021 01:20:33 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-1\\predict_predictions.json.\n",
      "06/29/2021 01:20:33 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  = 162\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  2\n",
      "***************************\n",
      "06/29/2021 01:20:37 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-cfbafb359b3c0703.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-16b28a96785d024a.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:20:37 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-643e17ae9af89acd.arrow\n",
      "06/29/2021 01:20:37 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-70ab43307be6c2b0.arrow\n",
      "06/29/2021 01:20:37 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-09a7ff39c6530fda.arrow\n",
      "{'loss': 5.325, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 11.0123, 'train_samples_per_second': 71.284, 'train_steps_per_second': 2.27, 'train_loss': 5.32504150390625, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      5.325\n",
      "  train_runtime            = 0:00:11.01\n",
      "  train_samples            =        785\n",
      "  train_samples_per_second =     71.284\n",
      "  train_steps_per_second   =       2.27\n",
      "06/29/2021 01:20:50 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:20:51 - INFO - utils_qa -   Post-processing 5 example predictions split into 224 features.\n",
      "06/29/2021 01:20:52 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-2\\eval_predictions.json.\n",
      "06/29/2021 01:20:52 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     = 224\n",
      "06/29/2021 01:20:52 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:20:53 - INFO - utils_qa -   Post-processing 5 example predictions split into 206 features.\n",
      "06/29/2021 01:20:54 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-2\\predict_predictions.json.\n",
      "06/29/2021 01:20:54 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  = 206\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  3\n",
      "***************************\n",
      "06/29/2021 01:20:58 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-3b0cd53601404272.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-d687d62e9f999f0f.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:20:58 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-464faa6635f2808d.arrow\n",
      "06/29/2021 01:20:58 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-c50379ffd54ec7fc.arrow\n",
      "06/29/2021 01:20:58 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-b42244981932d7ab.arrow\n",
      "{'loss': 4.7462, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 15.2819, 'train_samples_per_second': 70.933, 'train_steps_per_second': 2.225, 'train_loss': 4.74619338091682, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     4.7462\n",
      "  train_runtime            = 0:00:15.28\n",
      "  train_samples            =       1084\n",
      "  train_samples_per_second =     70.933\n",
      "  train_steps_per_second   =      2.225\n",
      "06/29/2021 01:21:15 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:21:15 - INFO - utils_qa -   Post-processing 5 example predictions split into 65 features.\n",
      "06/29/2021 01:21:15 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-3\\eval_predictions.json.\n",
      "06/29/2021 01:21:15 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  65\n",
      "06/29/2021 01:21:15 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:21:16 - INFO - utils_qa -   Post-processing 5 example predictions split into 66 features.\n",
      "06/29/2021 01:21:16 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-3\\predict_predictions.json.\n",
      "06/29/2021 01:21:16 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  66\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  4\n",
      "***************************\n",
      "06/29/2021 01:21:20 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-4657b09970548b03.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-d160df52d0da7923.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:21:20 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-cf94c8ab04107cae.arrow\n",
      "06/29/2021 01:21:20 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-89f28fdfcaeea23c.arrow\n",
      "06/29/2021 01:21:20 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-9e43393caac2fe92.arrow\n",
      "{'loss': 4.5233, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 16.4144, 'train_samples_per_second': 70.609, 'train_steps_per_second': 2.254, 'train_loss': 4.523338111671242, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     4.5233\n",
      "  train_runtime            = 0:00:16.41\n",
      "  train_samples            =       1159\n",
      "  train_samples_per_second =     70.609\n",
      "  train_steps_per_second   =      2.254\n",
      "06/29/2021 01:21:38 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:21:38 - INFO - utils_qa -   Post-processing 5 example predictions split into 28 features.\n",
      "06/29/2021 01:21:38 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-4\\eval_predictions.json.\n",
      "06/29/2021 01:21:38 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  28\n",
      "06/29/2021 01:21:38 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:21:39 - INFO - utils_qa -   Post-processing 5 example predictions split into 28 features.\n",
      "06/29/2021 01:21:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-4\\predict_predictions.json.\n",
      "06/29/2021 01:21:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  28\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  5\n",
      "***************************\n",
      "06/29/2021 01:21:43 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-61002a654c90adbf.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-edd19fa93b25fd2d.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:21:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-5c3afc77a89e5f21.arrow\n",
      "06/29/2021 01:21:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-5a02969842d6231c.arrow\n",
      "06/29/2021 01:21:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-f03ff87ba934bc8f.arrow\n",
      "{'loss': 4.7021, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 15.1453, 'train_samples_per_second': 69.857, 'train_steps_per_second': 2.245, 'train_loss': 4.7020712459788605, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     4.7021\n",
      "  train_runtime            = 0:00:15.14\n",
      "  train_samples            =       1058\n",
      "  train_samples_per_second =     69.857\n",
      "  train_steps_per_second   =      2.245\n",
      "06/29/2021 01:21:59 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:22:00 - INFO - utils_qa -   Post-processing 5 example predictions split into 86 features.\n",
      "06/29/2021 01:22:00 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-5\\eval_predictions.json.\n",
      "06/29/2021 01:22:00 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  86\n",
      "06/29/2021 01:22:00 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:22:01 - INFO - utils_qa -   Post-processing 5 example predictions split into 71 features.\n",
      "06/29/2021 01:22:01 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-5\\predict_predictions.json.\n",
      "06/29/2021 01:22:01 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =     71\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 8.8889\n",
      "**************************************************\n",
      "==================================================\n",
      "          At Gradual Fine Tuning Step:  2\n",
      "**************************************************\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-29 01:20:10,136 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:20:10,137 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 01:20:10,457 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:20:10,783 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:20:10,784 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:12,731 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:12,731 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:12,731 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:12,731 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:12,732 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:12,732 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 01:20:13,120 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 01:20:13,834 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 01:20:13,834 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:20:16,646 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:20:16,646 >>   Num examples = 893\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:20:16,646 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:20:16,646 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:20:16,646 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:20:16,646 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:20:16,646 >>   Total optimization steps = 28\n",
      "\n",
      "  0%|          | 0/28 [00:00<?, ?it/s]\n",
      "  4%|3         | 1/28 [00:01<00:36,  1.36s/it]\n",
      "  7%|7         | 2/28 [00:01<00:28,  1.09s/it]\n",
      " 11%|#         | 3/28 [00:02<00:22,  1.12it/s]\n",
      " 14%|#4        | 4/28 [00:02<00:18,  1.32it/s]\n",
      " 18%|#7        | 5/28 [00:03<00:15,  1.50it/s]\n",
      " 21%|##1       | 6/28 [00:03<00:13,  1.67it/s]\n",
      " 25%|##5       | 7/28 [00:04<00:11,  1.80it/s]\n",
      " 29%|##8       | 8/28 [00:04<00:10,  1.91it/s]\n",
      " 32%|###2      | 9/28 [00:04<00:09,  2.00it/s]\n",
      " 36%|###5      | 10/28 [00:05<00:08,  2.07it/s]\n",
      " 39%|###9      | 11/28 [00:05<00:08,  2.12it/s]\n",
      " 43%|####2     | 12/28 [00:06<00:07,  2.16it/s]\n",
      " 46%|####6     | 13/28 [00:06<00:06,  2.18it/s]\n",
      " 50%|#####     | 14/28 [00:07<00:06,  2.20it/s]\n",
      " 54%|#####3    | 15/28 [00:07<00:05,  2.21it/s]\n",
      " 57%|#####7    | 16/28 [00:08<00:05,  2.22it/s]\n",
      " 61%|######    | 17/28 [00:08<00:04,  2.22it/s]\n",
      " 64%|######4   | 18/28 [00:08<00:04,  2.23it/s]\n",
      " 68%|######7   | 19/28 [00:09<00:04,  2.23it/s]\n",
      " 71%|#######1  | 20/28 [00:09<00:03,  2.23it/s]\n",
      " 75%|#######5  | 21/28 [00:10<00:03,  2.23it/s]\n",
      " 79%|#######8  | 22/28 [00:10<00:02,  2.24it/s]\n",
      " 82%|########2 | 23/28 [00:11<00:02,  2.24it/s]\n",
      " 86%|########5 | 24/28 [00:11<00:01,  2.24it/s]\n",
      " 89%|########9 | 25/28 [00:12<00:01,  2.24it/s]\n",
      " 93%|#########2| 26/28 [00:12<00:00,  2.23it/s]\n",
      " 96%|#########6| 27/28 [00:12<00:00,  2.24it/s]\n",
      "100%|##########| 28/28 [00:13<00:00,  2.30it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 28/28 [00:13<00:00,  2.30it/s][INFO|trainer.py:1349] 2021-06-29 01:20:30,037 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 28/28 [00:13<00:00,  2.30it/s]\n",
      "100%|##########| 28/28 [00:13<00:00,  2.09it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:20:30,039 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:20:30,040 >> Configuration saved in ../models/gradual_ft_baseline//Split-1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:20:30,669 >> Model weights saved in ../models/gradual_ft_baseline//Split-1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:20:30,670 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:20:30,670 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:20:30,758 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:20:30,760 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:20:30,760 >>   Num examples = 160\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:20:30,760 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      " 40%|####      | 2/5 [00:00<00:00, 13.42it/s]\n",
      " 60%|######    | 3/5 [00:00<00:00, 10.23it/s]\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.79it/s]\n",
      "100%|##########| 5/5 [00:00<00:00,  8.01it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00, 11.56it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 11.56it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 11.47it/s]\n",
      "\n",
      "100%|##########| 5/5 [00:01<00:00,  3.98it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:20:32,182 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:20:32,184 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:20:32,184 >>   Num examples = 162\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:20:32,184 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      " 33%|###3      | 2/6 [00:00<00:00, 13.16it/s]\n",
      " 50%|#####     | 3/6 [00:00<00:00, 10.12it/s]\n",
      " 67%|######6   | 4/6 [00:00<00:00,  8.74it/s]\n",
      " 83%|########3 | 5/6 [00:00<00:00,  8.01it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00, 11.70it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 11.76it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 11.55it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:20:33,955 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:20:33,956 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 01:20:34,287 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:20:34,611 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:20:34,611 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:36,577 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:36,577 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:36,577 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:36,577 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:36,578 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:36,578 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 01:20:36,965 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 01:20:37,679 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 01:20:37,679 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "100%|##########| 6/6 [00:06<00:00,  1.04s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:20:38,567 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:20:38,567 >>   Num examples = 785\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:20:38,567 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:20:38,567 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:20:38,567 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:20:38,567 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:20:38,567 >>   Total optimization steps = 25\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "  4%|4         | 1/25 [00:00<00:11,  2.16it/s]\n",
      "  8%|8         | 2/25 [00:00<00:10,  2.18it/s]\n",
      " 12%|#2        | 3/25 [00:01<00:09,  2.20it/s]\n",
      " 16%|#6        | 4/25 [00:01<00:09,  2.21it/s]\n",
      " 20%|##        | 5/25 [00:02<00:09,  2.22it/s]\n",
      " 24%|##4       | 6/25 [00:02<00:08,  2.22it/s]\n",
      " 28%|##8       | 7/25 [00:03<00:08,  2.23it/s]\n",
      " 32%|###2      | 8/25 [00:03<00:07,  2.23it/s]\n",
      " 36%|###6      | 9/25 [00:04<00:07,  2.23it/s]\n",
      " 40%|####      | 10/25 [00:04<00:06,  2.24it/s]\n",
      " 44%|####4     | 11/25 [00:04<00:06,  2.24it/s]\n",
      " 48%|####8     | 12/25 [00:05<00:05,  2.24it/s]\n",
      " 52%|#####2    | 13/25 [00:05<00:05,  2.23it/s]\n",
      " 56%|#####6    | 14/25 [00:06<00:04,  2.24it/s]\n",
      " 60%|######    | 15/25 [00:06<00:04,  2.24it/s]\n",
      " 64%|######4   | 16/25 [00:07<00:04,  2.24it/s]\n",
      " 68%|######8   | 17/25 [00:07<00:03,  2.24it/s]\n",
      " 72%|#######2  | 18/25 [00:08<00:03,  2.24it/s]\n",
      " 76%|#######6  | 19/25 [00:08<00:02,  2.23it/s]\n",
      " 80%|########  | 20/25 [00:08<00:02,  2.23it/s]\n",
      " 84%|########4 | 21/25 [00:09<00:01,  2.23it/s]\n",
      " 88%|########8 | 22/25 [00:09<00:01,  2.23it/s]\n",
      " 92%|#########2| 23/25 [00:10<00:00,  2.23it/s]\n",
      " 96%|#########6| 24/25 [00:10<00:00,  2.23it/s]\n",
      "100%|##########| 25/25 [00:11<00:00,  2.55it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 25/25 [00:11<00:00,  2.55it/s][INFO|trainer.py:1349] 2021-06-29 01:20:49,579 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 25/25 [00:11<00:00,  2.55it/s]\n",
      "100%|##########| 25/25 [00:11<00:00,  2.27it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:20:49,581 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:20:49,582 >> Configuration saved in ../models/gradual_ft_baseline//Split-2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:20:50,196 >> Model weights saved in ../models/gradual_ft_baseline//Split-2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:20:50,196 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:20:50,197 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:20:50,284 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:20:50,286 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:20:50,286 >>   Num examples = 224\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:20:50,286 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      " 29%|##8       | 2/7 [00:00<00:00, 13.24it/s]\n",
      " 43%|####2     | 3/7 [00:00<00:00, 10.16it/s]\n",
      " 57%|#####7    | 4/7 [00:00<00:00,  8.73it/s]\n",
      " 71%|#######1  | 5/7 [00:00<00:00,  7.95it/s]\n",
      " 86%|########5 | 6/7 [00:00<00:00,  7.50it/s]\n",
      "100%|##########| 7/7 [00:00<00:00,  7.21it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 20%|##        | 1/5 [00:00<00:00,  7.75it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00,  7.86it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00,  8.00it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.17it/s]\u001b[A\n",
      "\n",
      "100%|##########| 5/5 [00:00<00:00,  8.22it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00,  8.22it/s]\n",
      "\n",
      "100%|##########| 7/7 [00:01<00:00,  3.87it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:20:52,265 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:20:52,267 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:20:52,267 >>   Num examples = 206\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:20:52,267 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      " 29%|##8       | 2/7 [00:00<00:00, 13.16it/s]\n",
      " 43%|####2     | 3/7 [00:00<00:00, 10.09it/s]\n",
      " 57%|#####7    | 4/7 [00:00<00:00,  8.77it/s]\n",
      " 71%|#######1  | 5/7 [00:00<00:00,  7.99it/s]\n",
      " 86%|########5 | 6/7 [00:00<00:00,  7.52it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 20%|##        | 1/5 [00:00<00:00,  8.40it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00,  8.42it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.78it/s]\u001b[A\n",
      "\n",
      "100%|##########| 5/5 [00:00<00:00,  8.67it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00,  8.91it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:20:54,449 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:20:54,449 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 01:20:54,784 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:20:55,112 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:20:55,112 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:57,061 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:57,061 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:57,061 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:57,061 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:57,061 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:20:57,061 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 01:20:57,449 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 01:20:58,163 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 01:20:58,163 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "100%|##########| 7/7 [00:06<00:00,  1.07it/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:20:58,998 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:20:58,998 >>   Num examples = 1084\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:20:58,998 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:20:58,998 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:20:58,998 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:20:58,999 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:20:58,999 >>   Total optimization steps = 34\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/34 [00:00<00:15,  2.13it/s]\n",
      "  6%|5         | 2/34 [00:00<00:14,  2.17it/s]\n",
      "  9%|8         | 3/34 [00:01<00:14,  2.18it/s]\n",
      " 12%|#1        | 4/34 [00:01<00:13,  2.20it/s]\n",
      " 15%|#4        | 5/34 [00:02<00:13,  2.21it/s]\n",
      " 18%|#7        | 6/34 [00:02<00:12,  2.21it/s]\n",
      " 21%|##        | 7/34 [00:03<00:12,  2.22it/s]\n",
      " 24%|##3       | 8/34 [00:03<00:11,  2.22it/s]\n",
      " 26%|##6       | 9/34 [00:04<00:11,  2.22it/s]\n",
      " 29%|##9       | 10/34 [00:04<00:10,  2.22it/s]\n",
      " 32%|###2      | 11/34 [00:04<00:10,  2.22it/s]\n",
      " 35%|###5      | 12/34 [00:05<00:09,  2.22it/s]\n",
      " 38%|###8      | 13/34 [00:05<00:09,  2.22it/s]\n",
      " 41%|####1     | 14/34 [00:06<00:08,  2.22it/s]\n",
      " 44%|####4     | 15/34 [00:06<00:08,  2.22it/s]\n",
      " 47%|####7     | 16/34 [00:07<00:08,  2.23it/s]\n",
      " 50%|#####     | 17/34 [00:07<00:07,  2.23it/s]\n",
      " 53%|#####2    | 18/34 [00:08<00:07,  2.23it/s]\n",
      " 56%|#####5    | 19/34 [00:08<00:06,  2.22it/s]\n",
      " 59%|#####8    | 20/34 [00:08<00:06,  2.22it/s]\n",
      " 62%|######1   | 21/34 [00:09<00:05,  2.23it/s]\n",
      " 65%|######4   | 22/34 [00:09<00:05,  2.22it/s]\n",
      " 68%|######7   | 23/34 [00:10<00:04,  2.22it/s]\n",
      " 71%|#######   | 24/34 [00:10<00:04,  2.22it/s]\n",
      " 74%|#######3  | 25/34 [00:11<00:04,  2.22it/s]\n",
      " 76%|#######6  | 26/34 [00:11<00:03,  2.22it/s]\n",
      " 79%|#######9  | 27/34 [00:12<00:03,  2.21it/s]\n",
      " 82%|########2 | 28/34 [00:12<00:02,  2.21it/s]\n",
      " 85%|########5 | 29/34 [00:13<00:02,  2.22it/s]\n",
      " 88%|########8 | 30/34 [00:13<00:01,  2.21it/s]\n",
      " 91%|#########1| 31/34 [00:13<00:01,  2.20it/s]\n",
      " 94%|#########4| 32/34 [00:14<00:00,  2.20it/s]\n",
      " 97%|#########7| 33/34 [00:14<00:00,  2.20it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.29it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.29it/s][INFO|trainer.py:1349] 2021-06-29 01:21:14,280 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.29it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.23it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:21:14,282 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:21:14,283 >> Configuration saved in ../models/gradual_ft_baseline//Split-3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:21:14,914 >> Model weights saved in ../models/gradual_ft_baseline//Split-3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:21:14,914 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:21:14,915 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:21:15,000 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:21:15,002 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:21:15,002 >>   Num examples = 65\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:21:15,002 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.07it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 28.09it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.59it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:21:15,625 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:21:15,627 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:21:15,627 >>   Num examples = 66\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:21:15,627 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.16it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 20.27it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 28.57it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:21:16,593 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:21:16,593 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 01:21:16,927 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:21:17,269 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:21:17,269 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:19,227 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:19,227 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:19,228 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:19,228 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:19,228 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:19,228 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 01:21:19,620 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 01:21:20,349 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 01:21:20,349 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "100%|##########| 3/3 [00:05<00:00,  1.86s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:21:21,365 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:21:21,365 >>   Num examples = 1159\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:21:21,365 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:21:21,365 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:21:21,365 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:21:21,366 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:21:21,366 >>   Total optimization steps = 37\n",
      "\n",
      "  0%|          | 0/37 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/37 [00:00<00:16,  2.13it/s]\n",
      "  5%|5         | 2/37 [00:00<00:16,  2.15it/s]\n",
      "  8%|8         | 3/37 [00:01<00:15,  2.17it/s]\n",
      " 11%|#         | 4/37 [00:01<00:15,  2.19it/s]\n",
      " 14%|#3        | 5/37 [00:02<00:14,  2.19it/s]\n",
      " 16%|#6        | 6/37 [00:02<00:14,  2.20it/s]\n",
      " 19%|#8        | 7/37 [00:03<00:13,  2.21it/s]\n",
      " 22%|##1       | 8/37 [00:03<00:13,  2.21it/s]\n",
      " 24%|##4       | 9/37 [00:04<00:12,  2.21it/s]\n",
      " 27%|##7       | 10/37 [00:04<00:12,  2.22it/s]\n",
      " 30%|##9       | 11/37 [00:04<00:11,  2.22it/s]\n",
      " 32%|###2      | 12/37 [00:05<00:11,  2.22it/s]\n",
      " 35%|###5      | 13/37 [00:05<00:10,  2.22it/s]\n",
      " 38%|###7      | 14/37 [00:06<00:10,  2.21it/s]\n",
      " 41%|####      | 15/37 [00:06<00:09,  2.21it/s]\n",
      " 43%|####3     | 16/37 [00:07<00:09,  2.21it/s]\n",
      " 46%|####5     | 17/37 [00:07<00:09,  2.22it/s]\n",
      " 49%|####8     | 18/37 [00:08<00:08,  2.21it/s]\n",
      " 51%|#####1    | 19/37 [00:08<00:08,  2.22it/s]\n",
      " 54%|#####4    | 20/37 [00:09<00:07,  2.21it/s]\n",
      " 57%|#####6    | 21/37 [00:09<00:07,  2.22it/s]\n",
      " 59%|#####9    | 22/37 [00:09<00:06,  2.21it/s]\n",
      " 62%|######2   | 23/37 [00:10<00:06,  2.22it/s]\n",
      " 65%|######4   | 24/37 [00:10<00:05,  2.22it/s]\n",
      " 68%|######7   | 25/37 [00:11<00:05,  2.22it/s]\n",
      " 70%|#######   | 26/37 [00:11<00:04,  2.22it/s]\n",
      " 73%|#######2  | 27/37 [00:12<00:04,  2.22it/s]\n",
      " 76%|#######5  | 28/37 [00:12<00:04,  2.22it/s]\n",
      " 78%|#######8  | 29/37 [00:13<00:03,  2.22it/s]\n",
      " 81%|########1 | 30/37 [00:13<00:03,  2.22it/s]\n",
      " 84%|########3 | 31/37 [00:14<00:02,  2.22it/s]\n",
      " 86%|########6 | 32/37 [00:14<00:02,  2.22it/s]\n",
      " 89%|########9 | 33/37 [00:14<00:01,  2.21it/s]\n",
      " 92%|#########1| 34/37 [00:15<00:01,  2.22it/s]\n",
      " 95%|#########4| 35/37 [00:15<00:00,  2.21it/s]\n",
      " 97%|#########7| 36/37 [00:16<00:00,  2.21it/s]\n",
      "100%|##########| 37/37 [00:16<00:00,  2.78it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 37/37 [00:16<00:00,  2.78it/s][INFO|trainer.py:1349] 2021-06-29 01:21:37,780 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 37/37 [00:16<00:00,  2.78it/s]\n",
      "100%|##########| 37/37 [00:16<00:00,  2.25it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:21:37,782 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:21:37,783 >> Configuration saved in ../models/gradual_ft_baseline//Split-4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:21:38,436 >> Model weights saved in ../models/gradual_ft_baseline//Split-4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:21:38,437 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:21:38,437 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:21:38,542 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:21:38,547 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:21:38,548 >>   Num examples = 28\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:21:38,548 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 62.50it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.75it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:21:38,840 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:21:38,841 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:21:38,842 >>   Num examples = 28\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:21:38,842 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 52.08it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:21:39,488 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:21:39,488 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 01:21:39,815 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 01:21:40,138 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:21:40,139 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:42,081 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:42,081 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:42,081 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:42,081 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:42,081 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 01:21:42,081 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 01:21:42,472 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 01:21:43,218 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 01:21:43,218 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "100%|##########| 1/1 [00:05<00:00,  5.06s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:21:44,059 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:21:44,059 >>   Num examples = 1058\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:21:44,059 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:21:44,059 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:21:44,059 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:21:44,059 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:21:44,059 >>   Total optimization steps = 34\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/34 [00:00<00:15,  2.10it/s]\n",
      "  6%|5         | 2/34 [00:00<00:15,  2.12it/s]\n",
      "  9%|8         | 3/34 [00:01<00:14,  2.13it/s]\n",
      " 12%|#1        | 4/34 [00:01<00:13,  2.15it/s]\n",
      " 15%|#4        | 5/34 [00:02<00:13,  2.16it/s]\n",
      " 18%|#7        | 6/34 [00:02<00:12,  2.18it/s]\n",
      " 21%|##        | 7/34 [00:03<00:12,  2.17it/s]\n",
      " 24%|##3       | 8/34 [00:03<00:11,  2.17it/s]\n",
      " 26%|##6       | 9/34 [00:04<00:11,  2.17it/s]\n",
      " 29%|##9       | 10/34 [00:04<00:10,  2.18it/s]\n",
      " 32%|###2      | 11/34 [00:05<00:10,  2.18it/s]\n",
      " 35%|###5      | 12/34 [00:05<00:10,  2.19it/s]\n",
      " 38%|###8      | 13/34 [00:05<00:09,  2.20it/s]\n",
      " 41%|####1     | 14/34 [00:06<00:09,  2.19it/s]\n",
      " 44%|####4     | 15/34 [00:06<00:08,  2.19it/s]\n",
      " 47%|####7     | 16/34 [00:07<00:08,  2.19it/s]\n",
      " 50%|#####     | 17/34 [00:07<00:07,  2.18it/s]\n",
      " 53%|#####2    | 18/34 [00:08<00:07,  2.19it/s]\n",
      " 56%|#####5    | 19/34 [00:08<00:06,  2.18it/s]\n",
      " 59%|#####8    | 20/34 [00:09<00:06,  2.19it/s]\n",
      " 62%|######1   | 21/34 [00:09<00:05,  2.19it/s]\n",
      " 65%|######4   | 22/34 [00:10<00:05,  2.20it/s]\n",
      " 68%|######7   | 23/34 [00:10<00:04,  2.20it/s]\n",
      " 71%|#######   | 24/34 [00:10<00:04,  2.21it/s]\n",
      " 74%|#######3  | 25/34 [00:11<00:04,  2.21it/s]\n",
      " 76%|#######6  | 26/34 [00:11<00:03,  2.20it/s]\n",
      " 79%|#######9  | 27/34 [00:12<00:03,  2.20it/s]\n",
      " 82%|########2 | 28/34 [00:12<00:02,  2.21it/s]\n",
      " 85%|########5 | 29/34 [00:13<00:02,  2.20it/s]\n",
      " 88%|########8 | 30/34 [00:13<00:01,  2.20it/s]\n",
      " 91%|#########1| 31/34 [00:14<00:01,  2.21it/s]\n",
      " 94%|#########4| 32/34 [00:14<00:00,  2.21it/s]\n",
      " 97%|#########7| 33/34 [00:15<00:00,  2.21it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.21it/s][INFO|trainer.py:1349] 2021-06-29 01:21:59,204 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.21it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.25it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:21:59,206 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-5\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:21:59,207 >> Configuration saved in ../models/gradual_ft_baseline//Split-5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:21:59,850 >> Model weights saved in ../models/gradual_ft_baseline//Split-5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:21:59,851 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:21:59,851 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:21:59,936 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:21:59,937 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:21:59,937 >>   Num examples = 86\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:21:59,937 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "100%|##########| 3/3 [00:00<00:00, 11.54it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 22.56it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 22.22it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  4.94it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:22:00,713 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:22:00,715 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:22:00,715 >>   Num examples = 71\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:22:00,715 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.58it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 29.13it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 25.91it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/squad_bioASQ_covidQA/\n",
      "../data/squad_bioASQ_covidQA/\n",
      "06/29/2021 01:22:04 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 01:22:04 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline/runs\\Jun29_01-22-04_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gradual_ft_baseline,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline/runs\\Jun29_01-22-04_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gradual_ft_baseline,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 67\n",
      "    })\n",
      "    covid: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    bio: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "06/29/2021 01:22:05 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-a1d9a7b34384b8aa.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-c9a849f723ed8a84.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "{'loss': 2.7982, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 13.5897, 'train_samples_per_second': 65.711, 'train_steps_per_second': 2.06, 'train_loss': 2.798187255859375, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.7982\n",
      "  train_runtime            = 0:00:13.58\n",
      "  train_samples            =        893\n",
      "  train_samples_per_second =     65.711\n",
      "  train_steps_per_second   =       2.06\n",
      "06/29/2021 01:22:22 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:22:23 - INFO - utils_qa -   Post-processing 5 example predictions split into 160 features.\n",
      "06/29/2021 01:22:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-1\\eval_predictions.json.\n",
      "06/29/2021 01:22:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     = 160\n",
      "06/29/2021 01:22:24 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:22:25 - INFO - utils_qa -   Post-processing 5 example predictions split into 162 features.\n",
      "06/29/2021 01:22:25 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-1\\predict_predictions.json.\n",
      "06/29/2021 01:22:25 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  = 162\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  2\n",
      "***************************\n",
      "06/29/2021 01:22:26 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-cfbafb359b3c0703.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-16b28a96785d024a.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "{'loss': 2.9279, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 11.1666, 'train_samples_per_second': 70.299, 'train_steps_per_second': 2.239, 'train_loss': 2.9279354858398436, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.9279\n",
      "  train_runtime            = 0:00:11.16\n",
      "  train_samples            =        785\n",
      "  train_samples_per_second =     70.299\n",
      "  train_steps_per_second   =      2.239\n",
      "06/29/2021 01:22:40 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:22:41 - INFO - utils_qa -   Post-processing 5 example predictions split into 224 features.\n",
      "06/29/2021 01:22:42 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-2\\eval_predictions.json.\n",
      "06/29/2021 01:22:42 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     = 224\n",
      "06/29/2021 01:22:42 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:22:43 - INFO - utils_qa -   Post-processing 5 example predictions split into 206 features.\n",
      "06/29/2021 01:22:44 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-2\\predict_predictions.json.\n",
      "06/29/2021 01:22:44 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =    206\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 2.2222\n",
      "***************************\n",
      "Split  3\n",
      "***************************\n",
      "06/29/2021 01:22:44 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-3b0cd53601404272.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-d687d62e9f999f0f.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "{'loss': 2.4281, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 15.622, 'train_samples_per_second': 69.389, 'train_steps_per_second': 2.176, 'train_loss': 2.4280819612390854, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.4281\n",
      "  train_runtime            = 0:00:15.62\n",
      "  train_samples            =       1084\n",
      "  train_samples_per_second =     69.389\n",
      "  train_steps_per_second   =      2.176\n",
      "06/29/2021 01:23:02 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:23:03 - INFO - utils_qa -   Post-processing 5 example predictions split into 65 features.\n",
      "06/29/2021 01:23:03 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-3\\eval_predictions.json.\n",
      "06/29/2021 01:23:03 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  65\n",
      "06/29/2021 01:23:03 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:23:03 - INFO - utils_qa -   Post-processing 5 example predictions split into 66 features.\n",
      "06/29/2021 01:23:03 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-3\\predict_predictions.json.\n",
      "06/29/2021 01:23:03 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  66\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  4\n",
      "***************************\n",
      "06/29/2021 01:23:04 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-4657b09970548b03.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-d160df52d0da7923.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "{'loss': 2.3167, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 16.6624, 'train_samples_per_second': 69.558, 'train_steps_per_second': 2.221, 'train_loss': 2.316698847590266, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.3167\n",
      "  train_runtime            = 0:00:16.66\n",
      "  train_samples            =       1159\n",
      "  train_samples_per_second =     69.558\n",
      "  train_steps_per_second   =      2.221\n",
      "06/29/2021 01:23:23 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:23:24 - INFO - utils_qa -   Post-processing 5 example predictions split into 28 features.\n",
      "06/29/2021 01:23:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-4\\eval_predictions.json.\n",
      "06/29/2021 01:23:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  28\n",
      "06/29/2021 01:23:24 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:23:24 - INFO - utils_qa -   Post-processing 5 example predictions split into 28 features.\n",
      "06/29/2021 01:23:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-4\\predict_predictions.json.\n",
      "06/29/2021 01:23:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  28\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  5\n",
      "***************************\n",
      "06/29/2021 01:23:25 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-61002a654c90adbf.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-edd19fa93b25fd2d.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "{'loss': 2.3558, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 15.3025, 'train_samples_per_second': 69.139, 'train_steps_per_second': 2.222, 'train_loss': 2.355831595028148, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.3558\n",
      "  train_runtime            = 0:00:15.30\n",
      "  train_samples            =       1058\n",
      "  train_samples_per_second =     69.139\n",
      "  train_steps_per_second   =      2.222\n",
      "06/29/2021 01:23:42 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:23:43 - INFO - utils_qa -   Post-processing 5 example predictions split into 86 features.\n",
      "06/29/2021 01:23:43 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-5\\eval_predictions.json.\n",
      "06/29/2021 01:23:43 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  86\n",
      "06/29/2021 01:23:43 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:23:44 - INFO - utils_qa -   Post-processing 5 example predictions split into 71 features.\n",
      "06/29/2021 01:23:44 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-5\\predict_predictions.json.\n",
      "06/29/2021 01:23:44 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  71\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "**************************************************\n",
      "==================================================\n",
      "          At Gradual Fine Tuning Step:  3\n",
      "**************************************************\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 01:22:04,591 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:22:04,592 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:22:04,596 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:04,596 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:04,596 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:04,596 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:04,596 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:04,596 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:04,596 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:22:04,678 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:22:05,426 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:22:05,426 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.44ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.42ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.27ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.27ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:22:08,403 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:22:08,403 >>   Num examples = 893\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:22:08,403 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:22:08,403 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:22:08,403 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:22:08,403 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:22:08,403 >>   Total optimization steps = 28\n",
      "\n",
      "  0%|          | 0/28 [00:00<?, ?it/s]\n",
      "  4%|3         | 1/28 [00:01<00:36,  1.36s/it]\n",
      "  7%|7         | 2/28 [00:01<00:28,  1.09s/it]\n",
      " 11%|#         | 3/28 [00:02<00:22,  1.11it/s]\n",
      " 14%|#4        | 4/28 [00:02<00:18,  1.31it/s]\n",
      " 18%|#7        | 5/28 [00:03<00:15,  1.49it/s]\n",
      " 21%|##1       | 6/28 [00:03<00:13,  1.65it/s]\n",
      " 25%|##5       | 7/28 [00:04<00:11,  1.79it/s]\n",
      " 29%|##8       | 8/28 [00:04<00:10,  1.90it/s]\n",
      " 32%|###2      | 9/28 [00:04<00:09,  1.98it/s]\n",
      " 36%|###5      | 10/28 [00:05<00:08,  2.03it/s]\n",
      " 39%|###9      | 11/28 [00:05<00:08,  2.09it/s]\n",
      " 43%|####2     | 12/28 [00:06<00:07,  2.11it/s]\n",
      " 46%|####6     | 13/28 [00:06<00:07,  2.13it/s]\n",
      " 50%|#####     | 14/28 [00:07<00:06,  2.16it/s]\n",
      " 54%|#####3    | 15/28 [00:07<00:06,  2.16it/s]\n",
      " 57%|#####7    | 16/28 [00:08<00:05,  2.18it/s]\n",
      " 61%|######    | 17/28 [00:08<00:05,  2.18it/s]\n",
      " 64%|######4   | 18/28 [00:09<00:04,  2.18it/s]\n",
      " 68%|######7   | 19/28 [00:09<00:04,  2.18it/s]\n",
      " 71%|#######1  | 20/28 [00:10<00:03,  2.18it/s]\n",
      " 75%|#######5  | 21/28 [00:10<00:03,  2.19it/s]\n",
      " 79%|#######8  | 22/28 [00:10<00:02,  2.20it/s]\n",
      " 82%|########2 | 23/28 [00:11<00:02,  2.20it/s]\n",
      " 86%|########5 | 24/28 [00:11<00:01,  2.20it/s]\n",
      " 89%|########9 | 25/28 [00:12<00:01,  2.21it/s]\n",
      " 93%|#########2| 26/28 [00:12<00:00,  2.21it/s]\n",
      " 96%|#########6| 27/28 [00:13<00:00,  2.21it/s]\n",
      "100%|##########| 28/28 [00:13<00:00,  2.27it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 28/28 [00:13<00:00,  2.27it/s][INFO|trainer.py:1349] 2021-06-29 01:22:21,993 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 28/28 [00:13<00:00,  2.27it/s]\n",
      "100%|##########| 28/28 [00:13<00:00,  2.06it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:22:21,995 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:22:21,996 >> Configuration saved in ../models/gradual_ft_baseline//Split-1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:22:22,627 >> Model weights saved in ../models/gradual_ft_baseline//Split-1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:22:22,628 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:22:22,628 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:22:22,717 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:22:22,719 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:22:22,719 >>   Num examples = 160\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:22:22,719 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      " 40%|####      | 2/5 [00:00<00:00, 12.90it/s]\n",
      " 60%|######    | 3/5 [00:00<00:00, 10.05it/s]\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.65it/s]\n",
      "100%|##########| 5/5 [00:00<00:00,  7.90it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00, 11.11it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 11.17it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 11.21it/s]\n",
      "\n",
      "100%|##########| 5/5 [00:01<00:00,  3.92it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:22:24,170 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:22:24,172 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:22:24,172 >>   Num examples = 162\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:22:24,172 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      " 33%|###3      | 2/6 [00:00<00:00, 13.07it/s]\n",
      " 50%|#####     | 3/6 [00:00<00:00, 10.09it/s]\n",
      " 67%|######6   | 4/6 [00:00<00:00,  8.65it/s]\n",
      " 83%|########3 | 5/6 [00:00<00:00,  7.94it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00, 11.63it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 11.67it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 11.52it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:22:25,602 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:22:25,602 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:22:25,603 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:25,603 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:25,603 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:25,603 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:25,603 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:25,603 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:25,604 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:22:25,670 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:22:26,411 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:22:26,412 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  3.66ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  3.65ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  3.28ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  3.28ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  3.55ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  3.55ba/s]\n",
      "\n",
      "100%|##########| 6/6 [00:04<00:00,  1.49it/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:22:28,367 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:22:28,367 >>   Num examples = 785\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:22:28,367 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:22:28,367 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:22:28,367 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:22:28,367 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:22:28,367 >>   Total optimization steps = 25\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "  4%|4         | 1/25 [00:00<00:11,  2.12it/s]\n",
      "  8%|8         | 2/25 [00:00<00:10,  2.15it/s]\n",
      " 12%|#2        | 3/25 [00:01<00:10,  2.17it/s]\n",
      " 16%|#6        | 4/25 [00:01<00:09,  2.19it/s]\n",
      " 20%|##        | 5/25 [00:02<00:09,  2.20it/s]\n",
      " 24%|##4       | 6/25 [00:02<00:08,  2.20it/s]\n",
      " 28%|##8       | 7/25 [00:03<00:08,  2.20it/s]\n",
      " 32%|###2      | 8/25 [00:03<00:07,  2.20it/s]\n",
      " 36%|###6      | 9/25 [00:04<00:07,  2.20it/s]\n",
      " 40%|####      | 10/25 [00:04<00:06,  2.20it/s]\n",
      " 44%|####4     | 11/25 [00:04<00:06,  2.20it/s]\n",
      " 48%|####8     | 12/25 [00:05<00:05,  2.20it/s]\n",
      " 52%|#####2    | 13/25 [00:05<00:05,  2.20it/s]\n",
      " 56%|#####6    | 14/25 [00:06<00:05,  2.19it/s]\n",
      " 60%|######    | 15/25 [00:06<00:04,  2.20it/s]\n",
      " 64%|######4   | 16/25 [00:07<00:04,  2.20it/s]\n",
      " 68%|######8   | 17/25 [00:07<00:03,  2.20it/s]\n",
      " 72%|#######2  | 18/25 [00:08<00:03,  2.19it/s]\n",
      " 76%|#######6  | 19/25 [00:08<00:02,  2.20it/s]\n",
      " 80%|########  | 20/25 [00:09<00:02,  2.20it/s]\n",
      " 84%|########4 | 21/25 [00:09<00:01,  2.20it/s]\n",
      " 88%|########8 | 22/25 [00:09<00:01,  2.21it/s]\n",
      " 92%|#########2| 23/25 [00:10<00:00,  2.21it/s]\n",
      " 96%|#########6| 24/25 [00:10<00:00,  2.20it/s]\n",
      "100%|##########| 25/25 [00:11<00:00,  2.52it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 25/25 [00:11<00:00,  2.52it/s][INFO|trainer.py:1349] 2021-06-29 01:22:39,534 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 25/25 [00:11<00:00,  2.52it/s]\n",
      "100%|##########| 25/25 [00:11<00:00,  2.24it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:22:39,536 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:22:39,537 >> Configuration saved in ../models/gradual_ft_baseline//Split-2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:22:40,174 >> Model weights saved in ../models/gradual_ft_baseline//Split-2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:22:40,175 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:22:40,175 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:22:40,263 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:22:40,265 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:22:40,265 >>   Num examples = 224\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:22:40,265 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      " 29%|##8       | 2/7 [00:00<00:00, 13.24it/s]\n",
      " 43%|####2     | 3/7 [00:00<00:00, 10.03it/s]\n",
      " 57%|#####7    | 4/7 [00:00<00:00,  8.60it/s]\n",
      " 71%|#######1  | 5/7 [00:00<00:00,  7.84it/s]\n",
      " 86%|########5 | 6/7 [00:00<00:00,  7.38it/s]\n",
      "100%|##########| 7/7 [00:00<00:00,  7.10it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 20%|##        | 1/5 [00:00<00:00,  8.06it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00,  8.06it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00,  8.16it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.27it/s]\u001b[A\n",
      "\n",
      "100%|##########| 5/5 [00:00<00:00,  8.27it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00,  8.25it/s]\n",
      "\n",
      "100%|##########| 7/7 [00:01<00:00,  3.81it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:22:42,270 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:22:42,272 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:22:42,272 >>   Num examples = 206\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:22:42,272 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      " 29%|##8       | 2/7 [00:00<00:00, 13.25it/s]\n",
      " 43%|####2     | 3/7 [00:00<00:00, 10.16it/s]\n",
      " 57%|#####7    | 4/7 [00:00<00:00,  8.73it/s]\n",
      " 71%|#######1  | 5/7 [00:00<00:00,  7.95it/s]\n",
      " 86%|########5 | 6/7 [00:00<00:00,  7.48it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 20%|##        | 1/5 [00:00<00:00,  8.40it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00,  7.81it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.27it/s]\u001b[A\n",
      "\n",
      "100%|##########| 5/5 [00:00<00:00,  8.29it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00,  8.38it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:22:44,145 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:22:44,145 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:22:44,146 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:44,146 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:44,146 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:44,146 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:44,146 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:44,146 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:22:44,146 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:22:44,207 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:22:44,958 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:22:44,958 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  3.18ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  3.17ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.31ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.94ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  7.94ba/s]\n",
      "\n",
      "100%|##########| 7/7 [00:03<00:00,  1.77it/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:22:46,414 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:22:46,414 >>   Num examples = 1084\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:22:46,414 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:22:46,414 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:22:46,414 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:22:46,415 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:22:46,415 >>   Total optimization steps = 34\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/34 [00:00<00:16,  2.03it/s]\n",
      "  6%|5         | 2/34 [00:00<00:15,  2.07it/s]\n",
      "  9%|8         | 3/34 [00:01<00:14,  2.11it/s]\n",
      " 12%|#1        | 4/34 [00:01<00:14,  2.13it/s]\n",
      " 15%|#4        | 5/34 [00:02<00:13,  2.16it/s]\n",
      " 18%|#7        | 6/34 [00:02<00:12,  2.16it/s]\n",
      " 21%|##        | 7/34 [00:03<00:12,  2.18it/s]\n",
      " 24%|##3       | 8/34 [00:03<00:11,  2.18it/s]\n",
      " 26%|##6       | 9/34 [00:04<00:11,  2.18it/s]\n",
      " 29%|##9       | 10/34 [00:04<00:11,  2.18it/s]\n",
      " 32%|###2      | 11/34 [00:05<00:10,  2.18it/s]\n",
      " 35%|###5      | 12/34 [00:05<00:10,  2.19it/s]\n",
      " 38%|###8      | 13/34 [00:05<00:09,  2.19it/s]\n",
      " 41%|####1     | 14/34 [00:06<00:09,  2.18it/s]\n",
      " 44%|####4     | 15/34 [00:06<00:08,  2.18it/s]\n",
      " 47%|####7     | 16/34 [00:07<00:08,  2.18it/s]\n",
      " 50%|#####     | 17/34 [00:07<00:07,  2.15it/s]\n",
      " 53%|#####2    | 18/34 [00:08<00:07,  2.15it/s]\n",
      " 56%|#####5    | 19/34 [00:08<00:06,  2.15it/s]\n",
      " 59%|#####8    | 20/34 [00:09<00:06,  2.16it/s]\n",
      " 62%|######1   | 21/34 [00:09<00:05,  2.17it/s]\n",
      " 65%|######4   | 22/34 [00:10<00:05,  2.15it/s]\n",
      " 68%|######7   | 23/34 [00:10<00:05,  2.15it/s]\n",
      " 71%|#######   | 24/34 [00:11<00:04,  2.17it/s]\n",
      " 74%|#######3  | 25/34 [00:11<00:04,  2.17it/s]\n",
      " 76%|#######6  | 26/34 [00:12<00:03,  2.15it/s]\n",
      " 79%|#######9  | 27/34 [00:12<00:03,  2.16it/s]\n",
      " 82%|########2 | 28/34 [00:12<00:02,  2.17it/s]\n",
      " 85%|########5 | 29/34 [00:13<00:02,  2.16it/s]\n",
      " 88%|########8 | 30/34 [00:13<00:01,  2.16it/s]\n",
      " 91%|#########1| 31/34 [00:14<00:01,  2.18it/s]\n",
      " 94%|#########4| 32/34 [00:14<00:00,  2.19it/s]\n",
      " 97%|#########7| 33/34 [00:15<00:00,  2.19it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.25it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.25it/s][INFO|trainer.py:1349] 2021-06-29 01:23:02,037 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.25it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.18it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:23:02,039 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:23:02,040 >> Configuration saved in ../models/gradual_ft_baseline//Split-3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:23:02,668 >> Model weights saved in ../models/gradual_ft_baseline//Split-3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:23:02,669 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:23:02,669 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:23:02,753 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:23:02,755 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:23:02,755 >>   Num examples = 65\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:23:02,755 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.50it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 28.09it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.48it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:23:03,391 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:23:03,393 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:23:03,393 >>   Num examples = 66\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:23:03,393 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.16it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 20.27it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 28.57it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:23:03,989 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:23:03,990 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:23:03,990 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:03,991 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:03,991 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:03,991 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:03,991 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:03,991 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:03,991 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:23:04,054 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:23:04,796 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:23:04,796 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  3.06ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  3.05ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00, 26.32ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00, 14.71ba/s]\n",
      "\n",
      "100%|##########| 3/3 [00:02<00:00,  1.01it/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:23:06,529 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:23:06,529 >>   Num examples = 1159\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:23:06,529 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:23:06,529 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:23:06,529 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:23:06,529 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:23:06,529 >>   Total optimization steps = 37\n",
      "\n",
      "  0%|          | 0/37 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/37 [00:00<00:17,  2.07it/s]\n",
      "  5%|5         | 2/37 [00:00<00:16,  2.09it/s]\n",
      "  8%|8         | 3/37 [00:01<00:15,  2.13it/s]\n",
      " 11%|#         | 4/37 [00:01<00:15,  2.15it/s]\n",
      " 14%|#3        | 5/37 [00:02<00:14,  2.16it/s]\n",
      " 16%|#6        | 6/37 [00:02<00:14,  2.15it/s]\n",
      " 19%|#8        | 7/37 [00:03<00:13,  2.15it/s]\n",
      " 22%|##1       | 8/37 [00:03<00:13,  2.15it/s]\n",
      " 24%|##4       | 9/37 [00:04<00:12,  2.16it/s]\n",
      " 27%|##7       | 10/37 [00:04<00:12,  2.18it/s]\n",
      " 30%|##9       | 11/37 [00:05<00:11,  2.19it/s]\n",
      " 32%|###2      | 12/37 [00:05<00:11,  2.18it/s]\n",
      " 35%|###5      | 13/37 [00:05<00:10,  2.18it/s]\n",
      " 38%|###7      | 14/37 [00:06<00:10,  2.19it/s]\n",
      " 41%|####      | 15/37 [00:06<00:10,  2.18it/s]\n",
      " 43%|####3     | 16/37 [00:07<00:09,  2.19it/s]\n",
      " 46%|####5     | 17/37 [00:07<00:09,  2.19it/s]\n",
      " 49%|####8     | 18/37 [00:08<00:08,  2.20it/s]\n",
      " 51%|#####1    | 19/37 [00:08<00:08,  2.19it/s]\n",
      " 54%|#####4    | 20/37 [00:09<00:07,  2.19it/s]\n",
      " 57%|#####6    | 21/37 [00:09<00:07,  2.20it/s]\n",
      " 59%|#####9    | 22/37 [00:10<00:06,  2.21it/s]\n",
      " 62%|######2   | 23/37 [00:10<00:06,  2.20it/s]\n",
      " 65%|######4   | 24/37 [00:10<00:05,  2.20it/s]\n",
      " 68%|######7   | 25/37 [00:11<00:05,  2.20it/s]\n",
      " 70%|#######   | 26/37 [00:11<00:04,  2.20it/s]\n",
      " 73%|#######2  | 27/37 [00:12<00:04,  2.21it/s]\n",
      " 76%|#######5  | 28/37 [00:12<00:04,  2.18it/s]\n",
      " 78%|#######8  | 29/37 [00:13<00:03,  2.17it/s]\n",
      " 81%|########1 | 30/37 [00:13<00:03,  2.17it/s]\n",
      " 84%|########3 | 31/37 [00:14<00:02,  2.18it/s]\n",
      " 86%|########6 | 32/37 [00:14<00:02,  2.18it/s]\n",
      " 89%|########9 | 33/37 [00:15<00:01,  2.18it/s]\n",
      " 92%|#########1| 34/37 [00:15<00:01,  2.17it/s]\n",
      " 95%|#########4| 35/37 [00:16<00:00,  2.16it/s]\n",
      " 97%|#########7| 36/37 [00:16<00:00,  2.16it/s]\n",
      "100%|##########| 37/37 [00:16<00:00,  2.72it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 37/37 [00:16<00:00,  2.72it/s][INFO|trainer.py:1349] 2021-06-29 01:23:23,191 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 37/37 [00:16<00:00,  2.72it/s]\n",
      "100%|##########| 37/37 [00:16<00:00,  2.22it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:23:23,193 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:23:23,195 >> Configuration saved in ../models/gradual_ft_baseline//Split-4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:23:23,849 >> Model weights saved in ../models/gradual_ft_baseline//Split-4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:23:23,850 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:23:23,850 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:23:23,934 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:23:23,936 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:23:23,936 >>   Num examples = 28\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:23:23,936 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 65.77it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.85it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:23:24,237 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:23:24,238 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:23:24,238 >>   Num examples = 28\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:23:24,238 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 64.93it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:23:24,519 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:23:24,519 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:23:24,520 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:24,520 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:24,520 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:24,520 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:24,520 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:24,520 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:24,520 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:23:24,593 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:23:25,346 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:23:25,346 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  3.27ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  3.27ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.40ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.33ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.13ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.13ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.43s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:23:26,821 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:23:26,821 >>   Num examples = 1058\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:23:26,821 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:23:26,821 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:23:26,821 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:23:26,821 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:23:26,821 >>   Total optimization steps = 34\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/34 [00:00<00:15,  2.11it/s]\n",
      "  6%|5         | 2/34 [00:00<00:15,  2.13it/s]\n",
      "  9%|8         | 3/34 [00:01<00:14,  2.15it/s]\n",
      " 12%|#1        | 4/34 [00:01<00:13,  2.16it/s]\n",
      " 15%|#4        | 5/34 [00:02<00:13,  2.17it/s]\n",
      " 18%|#7        | 6/34 [00:02<00:12,  2.17it/s]\n",
      " 21%|##        | 7/34 [00:03<00:12,  2.18it/s]\n",
      " 24%|##3       | 8/34 [00:03<00:11,  2.18it/s]\n",
      " 26%|##6       | 9/34 [00:04<00:11,  2.18it/s]\n",
      " 29%|##9       | 10/34 [00:04<00:11,  2.18it/s]\n",
      " 32%|###2      | 11/34 [00:05<00:10,  2.19it/s]\n",
      " 35%|###5      | 12/34 [00:05<00:10,  2.19it/s]\n",
      " 38%|###8      | 13/34 [00:05<00:09,  2.18it/s]\n",
      " 41%|####1     | 14/34 [00:06<00:09,  2.17it/s]\n",
      " 44%|####4     | 15/34 [00:06<00:08,  2.17it/s]\n",
      " 47%|####7     | 16/34 [00:07<00:08,  2.17it/s]\n",
      " 50%|#####     | 17/34 [00:07<00:07,  2.17it/s]\n",
      " 53%|#####2    | 18/34 [00:08<00:07,  2.17it/s]\n",
      " 56%|#####5    | 19/34 [00:08<00:06,  2.17it/s]\n",
      " 59%|#####8    | 20/34 [00:09<00:06,  2.17it/s]\n",
      " 62%|######1   | 21/34 [00:09<00:05,  2.17it/s]\n",
      " 65%|######4   | 22/34 [00:10<00:05,  2.17it/s]\n",
      " 68%|######7   | 23/34 [00:10<00:05,  2.17it/s]\n",
      " 71%|#######   | 24/34 [00:11<00:04,  2.16it/s]\n",
      " 74%|#######3  | 25/34 [00:11<00:04,  2.16it/s]\n",
      " 76%|#######6  | 26/34 [00:11<00:03,  2.15it/s]\n",
      " 79%|#######9  | 27/34 [00:12<00:03,  2.16it/s]\n",
      " 82%|########2 | 28/34 [00:12<00:02,  2.16it/s]\n",
      " 85%|########5 | 29/34 [00:13<00:02,  2.16it/s]\n",
      " 88%|########8 | 30/34 [00:13<00:01,  2.16it/s]\n",
      " 91%|#########1| 31/34 [00:14<00:01,  2.16it/s]\n",
      " 94%|#########4| 32/34 [00:14<00:00,  2.16it/s]\n",
      " 97%|#########7| 33/34 [00:15<00:00,  2.15it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.15it/s][INFO|trainer.py:1349] 2021-06-29 01:23:42,123 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.15it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.22it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:23:42,124 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-5\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:23:42,125 >> Configuration saved in ../models/gradual_ft_baseline//Split-5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:23:42,741 >> Model weights saved in ../models/gradual_ft_baseline//Split-5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:23:42,742 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:23:42,743 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:23:42,828 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:23:42,830 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:23:42,830 >>   Num examples = 86\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:23:42,830 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.99it/s]\n",
      "100%|##########| 3/3 [00:00<00:00, 11.67it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 22.39it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 21.93it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  4.93it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:23:43,608 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:23:43,609 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:23:43,609 >>   Num examples = 71\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:23:43,610 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.07it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 27.27it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 24.88it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.43it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:23:47,499 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:23:47,499 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline/Split-5/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:23:47,503 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:47,503 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:47,503 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:47,503 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:47,503 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:47,503 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:23:47,503 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:23:47,581 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:23:48,319 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:23:48,319 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.25ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.25ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:23:50,852 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:23:50,852 >>   Num examples = 893\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:23:50,852 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:23:50,852 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:23:50,852 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:23:50,853 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:23:50,853 >>   Total optimization steps = 28\n",
      "\n",
      "  0%|          | 0/28 [00:00<?, ?it/s]\n",
      "  4%|3         | 1/28 [00:01<00:37,  1.38s/it]\n",
      "  7%|7         | 2/28 [00:01<00:28,  1.11s/it]\n",
      " 11%|#         | 3/28 [00:02<00:22,  1.09it/s]\n",
      " 14%|#4        | 4/28 [00:02<00:18,  1.28it/s]\n",
      " 18%|#7        | 5/28 [00:03<00:15,  1.45it/s]\n",
      " 21%|##1       | 6/28 [00:03<00:13,  1.60it/s]\n",
      " 25%|##5       | 7/28 [00:04<00:12,  1.73it/s]\n",
      " 29%|##8       | 8/28 [00:04<00:10,  1.84it/s]\n",
      " 32%|###2      | 9/28 [00:05<00:09,  1.93it/s]\n",
      " 36%|###5      | 10/28 [00:05<00:08,  2.00it/s]\n",
      " 39%|###9      | 11/28 [00:06<00:08,  2.06it/s]\n",
      " 43%|####2     | 12/28 [00:06<00:07,  2.09it/s]\n",
      " 46%|####6     | 13/28 [00:06<00:07,  2.11it/s]\n",
      " 50%|#####     | 14/28 [00:07<00:06,  2.13it/s]\n",
      " 54%|#####3    | 15/28 [00:07<00:06,  2.13it/s]\n",
      " 57%|#####7    | 16/28 [00:08<00:05,  2.14it/s]\n",
      " 61%|######    | 17/28 [00:08<00:05,  2.15it/s]\n",
      " 64%|######4   | 18/28 [00:09<00:04,  2.17it/s]\n",
      " 68%|######7   | 19/28 [00:09<00:04,  2.18it/s]\n",
      " 71%|#######1  | 20/28 [00:10<00:03,  2.18it/s]\n",
      " 75%|#######5  | 21/28 [00:10<00:03,  2.16it/s]\n",
      " 79%|#######8  | 22/28 [00:11<00:02,  2.16it/s]\n",
      " 82%|########2 | 23/28 [00:11<00:02,  2.14it/s]\n",
      " 86%|########5 | 24/28 [00:12<00:01,  2.15it/s]\n",
      " 89%|########9 | 25/28 [00:12<00:01,  2.14it/s]\n",
      " 93%|#########2| 26/28 [00:12<00:00,  2.14it/s]\n",
      " 96%|#########6| 27/28 [00:13<00:00,  2.15it/s]\n",
      "100%|##########| 28/28 [00:13<00:00,  2.21it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 28/28 [00:13<00:00,  2.21it/s][INFO|trainer.py:1349] 2021-06-29 01:24:04,732 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 28/28 [00:13<00:00,  2.21it/s]\n",
      "100%|##########| 28/28 [00:13<00:00,  2.02it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:24:04,734 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:24:04,735 >> Configuration saved in ../models/gradual_ft_baseline//Split-1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:24:05,372 >> Model weights saved in ../models/gradual_ft_baseline//Split-1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:24:05,372 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:24:05,373 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:24:05,456 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:24:05,458 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:24:05,458 >>   Num examples = 160\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:24:05,458 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      " 40%|####      | 2/5 [00:00<00:00, 12.05it/s]\n",
      " 60%|######    | 3/5 [00:00<00:00,  9.53it/s]\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.24it/s]\n",
      "100%|##########| 5/5 [00:00<00:00,  7.59it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00, 11.36it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 11.31it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 11.09it/s]\n",
      "\n",
      "100%|##########| 5/5 [00:01<00:00,  3.83it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:24:06,943 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:24:06,945 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:24:06,945 >>   Num examples = 162\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:24:06,945 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      " 33%|###3      | 2/6 [00:00<00:00, 12.74it/s]\n",
      " 50%|#####     | 3/6 [00:00<00:00,  9.69it/s]\n",
      " 67%|######6   | 4/6 [00:00<00:00,  8.27it/s]\n",
      " 83%|########3 | 5/6 [00:00<00:00,  7.59it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00, 11.17it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 11.27it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 11.16it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:24:08,439 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:24:08,440 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline/Split-5/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:24:08,440 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:08,441 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:08,441 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:08,441 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:08,441 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:08,441 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:08,441 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:24:08,508 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:24:09,253 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:24:09,253 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  4.46ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  4.46ba/s]\n",
      "\n",
      "100%|##########| 6/6 [00:03<00:00,  1.85it/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:24:10,353 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:24:10,353 >>   Num examples = 785\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:24:10,353 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:24:10,353 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:24:10,353 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:24:10,353 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:24:10,353 >>   Total optimization steps = 25\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "  4%|4         | 1/25 [00:00<00:11,  2.11it/s]\n",
      "  8%|8         | 2/25 [00:00<00:10,  2.14it/s]\n",
      " 12%|#2        | 3/25 [00:01<00:10,  2.15it/s]\n",
      " 16%|#6        | 4/25 [00:01<00:09,  2.15it/s]\n",
      " 20%|##        | 5/25 [00:02<00:09,  2.15it/s]\n",
      " 24%|##4       | 6/25 [00:02<00:08,  2.16it/s]\n",
      " 28%|##8       | 7/25 [00:03<00:08,  2.17it/s]\n",
      " 32%|###2      | 8/25 [00:03<00:07,  2.15it/s]\n",
      " 36%|###6      | 9/25 [00:04<00:07,  2.16it/s]\n",
      " 40%|####      | 10/25 [00:04<00:06,  2.16it/s]\n",
      " 44%|####4     | 11/25 [00:05<00:06,  2.17it/s]\n",
      " 48%|####8     | 12/25 [00:05<00:05,  2.19it/s]\n",
      " 52%|#####2    | 13/25 [00:05<00:05,  2.19it/s]\n",
      " 56%|#####6    | 14/25 [00:06<00:05,  2.20it/s]\n",
      " 60%|######    | 15/25 [00:06<00:04,  2.20it/s]\n",
      " 64%|######4   | 16/25 [00:07<00:04,  2.20it/s]\n",
      " 68%|######8   | 17/25 [00:07<00:03,  2.20it/s]\n",
      " 72%|#######2  | 18/25 [00:08<00:03,  2.19it/s]\n",
      " 76%|#######6  | 19/25 [00:08<00:02,  2.20it/s]\n",
      " 80%|########  | 20/25 [00:09<00:02,  2.20it/s]\n",
      " 84%|########4 | 21/25 [00:09<00:01,  2.19it/s]\n",
      " 88%|########8 | 22/25 [00:10<00:01,  2.19it/s]\n",
      " 92%|#########2| 23/25 [00:10<00:00,  2.19it/s]\n",
      " 96%|#########6| 24/25 [00:10<00:00,  2.19it/s]\n",
      "100%|##########| 25/25 [00:11<00:00,  2.50it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 25/25 [00:11<00:00,  2.50it/s][INFO|trainer.py:1349] 2021-06-29 01:24:21,621 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 25/25 [00:11<00:00,  2.50it/s]\n",
      "100%|##########| 25/25 [00:11<00:00,  2.22it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:24:21,623 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:24:21,624 >> Configuration saved in ../models/gradual_ft_baseline//Split-2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:24:22,245 >> Model weights saved in ../models/gradual_ft_baseline//Split-2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:24:22,246 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:24:22,246 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:24:22,334 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:24:22,336 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:24:22,336 >>   Num examples = 224\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:24:22,336 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      " 29%|##8       | 2/7 [00:00<00:00, 12.50it/s]\n",
      " 43%|####2     | 3/7 [00:00<00:00,  9.73it/s]\n",
      " 57%|#####7    | 4/7 [00:00<00:00,  8.42it/s]\n",
      " 71%|#######1  | 5/7 [00:00<00:00,  7.73it/s]\n",
      " 86%|########5 | 6/7 [00:00<00:00,  7.33it/s]\n",
      "100%|##########| 7/7 [00:00<00:00,  7.04it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 20%|##        | 1/5 [00:00<00:00,  8.06it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00,  8.06it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00,  8.01it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.16it/s]\u001b[A\n",
      "\n",
      "100%|##########| 5/5 [00:00<00:00,  8.11it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00,  8.10it/s]\n",
      "\n",
      "100%|##########| 7/7 [00:01<00:00,  3.75it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:24:24,380 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:24:24,382 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:24:24,382 >>   Num examples = 206\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:24:24,382 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      " 29%|##8       | 2/7 [00:00<00:00, 13.16it/s]\n",
      " 43%|####2     | 3/7 [00:00<00:00, 10.12it/s]\n",
      " 57%|#####7    | 4/7 [00:00<00:00,  8.74it/s]\n",
      " 71%|#######1  | 5/7 [00:00<00:00,  7.94it/s]\n",
      " 86%|########5 | 6/7 [00:00<00:00,  7.42it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 20%|##        | 1/5 [00:00<00:00,  8.20it/s]\u001b[A\n",
      "\n",
      " 40%|####      | 2/5 [00:00<00:00,  8.22it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00,  8.64it/s]\u001b[A\n",
      "\n",
      "100%|##########| 5/5 [00:00<00:00,  7.97it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00,  8.40it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:24:26,261 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:24:26,261 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline/Split-5/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:24:26,262 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:26,262 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:26,262 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:26,262 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:26,262 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:26,262 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:26,262 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:24:26,329 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:24:27,071 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:24:27,071 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  2.89ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  2.88ba/s]\n",
      "\n",
      "100%|##########| 7/7 [00:03<00:00,  1.86it/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:24:28,330 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:24:28,330 >>   Num examples = 1084\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:24:28,330 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:24:28,330 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:24:28,330 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:24:28,330 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:24:28,330 >>   Total optimization steps = 34\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/34 [00:00<00:15,  2.08it/s]\n",
      "  6%|5         | 2/34 [00:00<00:15,  2.11it/s]\n",
      "  9%|8         | 3/34 [00:01<00:14,  2.12it/s]\n",
      " 12%|#1        | 4/34 [00:01<00:14,  2.13it/s]\n",
      " 15%|#4        | 5/34 [00:02<00:13,  2.15it/s]\n",
      " 18%|#7        | 6/34 [00:02<00:12,  2.16it/s]\n",
      " 21%|##        | 7/34 [00:03<00:12,  2.16it/s]\n",
      " 24%|##3       | 8/34 [00:03<00:11,  2.17it/s]\n",
      " 26%|##6       | 9/34 [00:04<00:11,  2.18it/s]\n",
      " 29%|##9       | 10/34 [00:04<00:11,  2.18it/s]\n",
      " 32%|###2      | 11/34 [00:05<00:10,  2.18it/s]\n",
      " 35%|###5      | 12/34 [00:05<00:10,  2.17it/s]\n",
      " 38%|###8      | 13/34 [00:05<00:09,  2.18it/s]\n",
      " 41%|####1     | 14/34 [00:06<00:09,  2.19it/s]\n",
      " 44%|####4     | 15/34 [00:06<00:08,  2.20it/s]\n",
      " 47%|####7     | 16/34 [00:07<00:08,  2.17it/s]\n",
      " 50%|#####     | 17/34 [00:07<00:07,  2.18it/s]\n",
      " 53%|#####2    | 18/34 [00:08<00:07,  2.19it/s]\n",
      " 56%|#####5    | 19/34 [00:08<00:06,  2.20it/s]\n",
      " 59%|#####8    | 20/34 [00:09<00:06,  2.20it/s]\n",
      " 62%|######1   | 21/34 [00:09<00:05,  2.20it/s]\n",
      " 65%|######4   | 22/34 [00:10<00:05,  2.18it/s]\n",
      " 68%|######7   | 23/34 [00:10<00:05,  2.17it/s]\n",
      " 71%|#######   | 24/34 [00:11<00:04,  2.17it/s]\n",
      " 74%|#######3  | 25/34 [00:11<00:04,  2.18it/s]\n",
      " 76%|#######6  | 26/34 [00:11<00:03,  2.19it/s]\n",
      " 79%|#######9  | 27/34 [00:12<00:03,  2.19it/s]\n",
      " 82%|########2 | 28/34 [00:12<00:02,  2.20it/s]\n",
      " 85%|########5 | 29/34 [00:13<00:02,  2.17it/s]\n",
      " 88%|########8 | 30/34 [00:13<00:01,  2.18it/s]\n",
      " 91%|#########1| 31/34 [00:14<00:01,  2.18it/s]\n",
      " 94%|#########4| 32/34 [00:14<00:00,  2.19it/s]\n",
      " 97%|#########7| 33/34 [00:15<00:00,  2.17it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.25it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.25it/s][INFO|trainer.py:1349] 2021-06-29 01:24:43,887 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.25it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.19it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:24:43,888 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:24:43,889 >> Configuration saved in ../models/gradual_ft_baseline//Split-3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:24:44,518 >> Model weights saved in ../models/gradual_ft_baseline//Split-3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:24:44,518 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:24:44,519 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:24:44,609 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:24:44,611 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:24:44,611 >>   Num examples = 65\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:24:44,611 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.25it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 80%|########  | 4/5 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 28.57it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.90it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:24:45,217 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:24:45,219 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:24:45,219 >>   Num examples = 66\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:24:45,219 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.07it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 19.35it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 27.32it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:24:45,827 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:24:45,828 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline/Split-5/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:24:45,828 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:45,828 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:45,828 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:45,829 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:45,829 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:45,829 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:24:45,829 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:24:45,893 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:24:46,630 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:24:46,630 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  2.40ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  2.40ba/s]\n",
      "\n",
      "100%|##########| 3/3 [00:02<00:00,  1.19it/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:24:47,920 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:24:47,921 >>   Num examples = 1159\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:24:47,921 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:24:47,921 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:24:47,921 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:24:47,921 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:24:47,921 >>   Total optimization steps = 37\n",
      "\n",
      "  0%|          | 0/37 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/37 [00:00<00:17,  2.03it/s]\n",
      "  5%|5         | 2/37 [00:00<00:16,  2.08it/s]\n",
      "  8%|8         | 3/37 [00:01<00:16,  2.11it/s]\n",
      " 11%|#         | 4/37 [00:01<00:15,  2.14it/s]\n",
      " 14%|#3        | 5/37 [00:02<00:15,  2.12it/s]\n",
      " 16%|#6        | 6/37 [00:02<00:14,  2.13it/s]\n",
      " 19%|#8        | 7/37 [00:03<00:14,  2.14it/s]\n",
      " 22%|##1       | 8/37 [00:03<00:13,  2.15it/s]\n",
      " 24%|##4       | 9/37 [00:04<00:12,  2.17it/s]\n",
      " 27%|##7       | 10/37 [00:04<00:12,  2.16it/s]\n",
      " 30%|##9       | 11/37 [00:05<00:11,  2.17it/s]\n",
      " 32%|###2      | 12/37 [00:05<00:11,  2.18it/s]\n",
      " 35%|###5      | 13/37 [00:05<00:10,  2.19it/s]\n",
      " 38%|###7      | 14/37 [00:06<00:10,  2.19it/s]\n",
      " 41%|####      | 15/37 [00:06<00:10,  2.19it/s]\n",
      " 43%|####3     | 16/37 [00:07<00:09,  2.19it/s]\n",
      " 46%|####5     | 17/37 [00:07<00:09,  2.19it/s]\n",
      " 49%|####8     | 18/37 [00:08<00:08,  2.20it/s]\n",
      " 51%|#####1    | 19/37 [00:08<00:08,  2.21it/s]\n",
      " 54%|#####4    | 20/37 [00:09<00:07,  2.21it/s]\n",
      " 57%|#####6    | 21/37 [00:09<00:07,  2.21it/s]\n",
      " 59%|#####9    | 22/37 [00:10<00:06,  2.21it/s]\n",
      " 62%|######2   | 23/37 [00:10<00:06,  2.20it/s]\n",
      " 65%|######4   | 24/37 [00:10<00:05,  2.20it/s]\n",
      " 68%|######7   | 25/37 [00:11<00:05,  2.21it/s]\n",
      " 70%|#######   | 26/37 [00:11<00:04,  2.21it/s]\n",
      " 73%|#######2  | 27/37 [00:12<00:04,  2.20it/s]\n",
      " 76%|#######5  | 28/37 [00:12<00:04,  2.21it/s]\n",
      " 78%|#######8  | 29/37 [00:13<00:03,  2.21it/s]\n",
      " 81%|########1 | 30/37 [00:13<00:03,  2.21it/s]\n",
      " 84%|########3 | 31/37 [00:14<00:02,  2.21it/s]\n",
      " 86%|########6 | 32/37 [00:14<00:02,  2.21it/s]\n",
      " 89%|########9 | 33/37 [00:15<00:01,  2.21it/s]\n",
      " 92%|#########1| 34/37 [00:15<00:01,  2.21it/s]\n",
      " 95%|#########4| 35/37 [00:15<00:00,  2.21it/s]\n",
      " 97%|#########7| 36/37 [00:16<00:00,  2.20it/s]\n",
      "100%|##########| 37/37 [00:16<00:00,  2.78it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 37/37 [00:16<00:00,  2.78it/s][INFO|trainer.py:1349] 2021-06-29 01:25:04,485 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 37/37 [00:16<00:00,  2.78it/s]\n",
      "100%|##########| 37/37 [00:16<00:00,  2.23it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:25:04,487 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:25:04,488 >> Configuration saved in ../models/gradual_ft_baseline//Split-4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:25:05,119 >> Model weights saved in ../models/gradual_ft_baseline//Split-4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:25:05,120 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:25:05,120 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:25:05,202 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:25:05,204 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:25:05,204 >>   Num examples = 28\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:25:05,204 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 66.66it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.25it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:25:05,491 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:25:05,493 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:25:05,493 >>   Num examples = 28\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:25:05,493 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 65.79it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-06-29 01:25:05,782 >> loading configuration file ../models/gradual_ft_baseline/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 01:25:05,783 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline/Split-5/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 01:25:05,783 >> Didn't find file ../models/gradual_ft_baseline/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:25:05,784 >> loading file ../models/gradual_ft_baseline/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:25:05,784 >> loading file ../models/gradual_ft_baseline/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:25:05,784 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:25:05,784 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:25:05,784 >> loading file ../models/gradual_ft_baseline/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 01:25:05,784 >> loading file ../models/gradual_ft_baseline/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 01:25:05,852 >> loading weights file ../models/gradual_ft_baseline/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 01:25:06,592 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 01:25:06,592 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  3.32ba/s]\u001b[A\n",
      "100%|##########| 1/1 [00:00<00:00,  3.32ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.30s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 01:25:07,952 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 01:25:07,952 >>   Num examples = 1058\n",
      "[INFO|trainer.py:1155] 2021-06-29 01:25:07,952 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 01:25:07,953 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 01:25:07,953 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 01:25:07,953 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 01:25:07,953 >>   Total optimization steps = 34\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\n",
      "  3%|2         | 1/34 [00:00<00:15,  2.16it/s]\n",
      "  6%|5         | 2/34 [00:00<00:14,  2.17it/s]\n",
      "  9%|8         | 3/34 [00:01<00:14,  2.16it/s]\n",
      " 12%|#1        | 4/34 [00:01<00:13,  2.17it/s]\n",
      " 15%|#4        | 5/34 [00:02<00:13,  2.17it/s]\n",
      " 18%|#7        | 6/34 [00:02<00:12,  2.18it/s]\n",
      " 21%|##        | 7/34 [00:03<00:12,  2.17it/s]\n",
      " 24%|##3       | 8/34 [00:03<00:11,  2.18it/s]\n",
      " 26%|##6       | 9/34 [00:04<00:11,  2.18it/s]\n",
      " 29%|##9       | 10/34 [00:04<00:10,  2.19it/s]\n",
      " 32%|###2      | 11/34 [00:05<00:10,  2.19it/s]\n",
      " 35%|###5      | 12/34 [00:05<00:10,  2.19it/s]\n",
      " 38%|###8      | 13/34 [00:05<00:09,  2.20it/s]\n",
      " 41%|####1     | 14/34 [00:06<00:09,  2.20it/s]\n",
      " 44%|####4     | 15/34 [00:06<00:08,  2.20it/s]\n",
      " 47%|####7     | 16/34 [00:07<00:08,  2.19it/s]\n",
      " 50%|#####     | 17/34 [00:07<00:07,  2.19it/s]\n",
      " 53%|#####2    | 18/34 [00:08<00:07,  2.18it/s]\n",
      " 56%|#####5    | 19/34 [00:08<00:06,  2.19it/s]\n",
      " 59%|#####8    | 20/34 [00:09<00:06,  2.19it/s]\n",
      " 62%|######1   | 21/34 [00:09<00:05,  2.19it/s]\n",
      " 65%|######4   | 22/34 [00:10<00:05,  2.20it/s]\n",
      " 68%|######7   | 23/34 [00:10<00:04,  2.20it/s]\n",
      " 71%|#######   | 24/34 [00:10<00:04,  2.20it/s]\n",
      " 74%|#######3  | 25/34 [00:11<00:04,  2.20it/s]\n",
      " 76%|#######6  | 26/34 [00:11<00:03,  2.20it/s]\n",
      " 79%|#######9  | 27/34 [00:12<00:03,  2.19it/s]\n",
      " 82%|########2 | 28/34 [00:12<00:02,  2.17it/s]\n",
      " 85%|########5 | 29/34 [00:13<00:02,  2.17it/s]\n",
      " 88%|########8 | 30/34 [00:13<00:01,  2.18it/s]\n",
      " 91%|#########1| 31/34 [00:14<00:01,  2.16it/s]\n",
      " 94%|#########4| 32/34 [00:14<00:00,  2.17it/s]\n",
      " 97%|#########7| 33/34 [00:15<00:00,  2.18it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.18it/s][INFO|trainer.py:1349] 2021-06-29 01:25:23,132 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 34/34 [00:15<00:00,  2.18it/s]\n",
      "100%|##########| 34/34 [00:15<00:00,  2.24it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 01:25:23,133 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-5\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 01:25:23,134 >> Configuration saved in ../models/gradual_ft_baseline//Split-5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 01:25:23,766 >> Model weights saved in ../models/gradual_ft_baseline//Split-5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 01:25:23,767 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 01:25:23,768 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 01:25:23,865 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:25:23,867 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:25:23,867 >>   Num examples = 86\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:25:23,867 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.07it/s]\n",
      "100%|##########| 3/3 [00:00<00:00, 11.88it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 21.83it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  4.85it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 01:25:24,657 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 01:25:24,659 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 01:25:24,659 >>   Num examples = 71\n",
      "[INFO|trainer.py:2151] 2021-06-29 01:25:24,659 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.24it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 60%|######    | 3/5 [00:00<00:00, 28.04it/s]\u001b[A\n",
      "100%|##########| 5/5 [00:00<00:00, 25.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/squad_bioASQ_covidQA/\n",
      "../data/squad_bioASQ_covidQA/\n",
      "06/29/2021 01:23:47 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 01:23:47 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline/runs\\Jun29_01-23-47_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gradual_ft_baseline,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline/runs\\Jun29_01-23-47_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gradual_ft_baseline,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 67\n",
      "    })\n",
      "    covid: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    bio: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "06/29/2021 01:23:48 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-a1d9a7b34384b8aa.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-c9a849f723ed8a84.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:23:48 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-2d3c35e78b0c1c19.arrow\n",
      "06/29/2021 01:23:48 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-a6846b5e1329d8c6.arrow\n",
      "{'loss': 1.6014, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 13.8791, 'train_samples_per_second': 64.342, 'train_steps_per_second': 2.017, 'train_loss': 1.6014269420078822, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.6014\n",
      "  train_runtime            = 0:00:13.87\n",
      "  train_samples            =        893\n",
      "  train_samples_per_second =     64.342\n",
      "  train_steps_per_second   =      2.017\n",
      "06/29/2021 01:24:05 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:24:06 - INFO - utils_qa -   Post-processing 5 example predictions split into 160 features.\n",
      "06/29/2021 01:24:06 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-1\\eval_predictions.json.\n",
      "06/29/2021 01:24:06 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     = 160\n",
      "06/29/2021 01:24:06 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:24:07 - INFO - utils_qa -   Post-processing 5 example predictions split into 162 features.\n",
      "06/29/2021 01:24:08 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-1\\predict_predictions.json.\n",
      "06/29/2021 01:24:08 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  = 162\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  2\n",
      "***************************\n",
      "06/29/2021 01:24:09 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-cfbafb359b3c0703.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-16b28a96785d024a.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:24:09 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-6a9533e5f48420c7.arrow\n",
      "06/29/2021 01:24:09 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-0512dbded56d2811.arrow\n",
      "{'loss': 1.8412, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 11.2693, 'train_samples_per_second': 69.658, 'train_steps_per_second': 2.218, 'train_loss': 1.8411732482910157, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8412\n",
      "  train_runtime            = 0:00:11.26\n",
      "  train_samples            =        785\n",
      "  train_samples_per_second =     69.658\n",
      "  train_steps_per_second   =      2.218\n",
      "06/29/2021 01:24:22 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:24:23 - INFO - utils_qa -   Post-processing 5 example predictions split into 224 features.\n",
      "06/29/2021 01:24:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-2\\eval_predictions.json.\n",
      "06/29/2021 01:24:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     = 224\n",
      "06/29/2021 01:24:24 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:24:25 - INFO - utils_qa -   Post-processing 5 example predictions split into 206 features.\n",
      "06/29/2021 01:24:26 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-2\\predict_predictions.json.\n",
      "06/29/2021 01:24:26 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  = 206\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  3\n",
      "***************************\n",
      "06/29/2021 01:24:27 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-3b0cd53601404272.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-d687d62e9f999f0f.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:24:27 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-f70a1ef068b2fe78.arrow\n",
      "06/29/2021 01:24:27 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-bf79e3621aeb7087.arrow\n",
      "{'loss': 1.318, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 15.5568, 'train_samples_per_second': 69.68, 'train_steps_per_second': 2.186, 'train_loss': 1.3179525487563188, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      1.318\n",
      "  train_runtime            = 0:00:15.55\n",
      "  train_samples            =       1084\n",
      "  train_samples_per_second =      69.68\n",
      "  train_steps_per_second   =      2.186\n",
      "06/29/2021 01:24:44 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:24:45 - INFO - utils_qa -   Post-processing 5 example predictions split into 65 features.\n",
      "06/29/2021 01:24:45 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-3\\eval_predictions.json.\n",
      "06/29/2021 01:24:45 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  65\n",
      "06/29/2021 01:24:45 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:24:45 - INFO - utils_qa -   Post-processing 5 example predictions split into 66 features.\n",
      "06/29/2021 01:24:45 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-3\\predict_predictions.json.\n",
      "06/29/2021 01:24:45 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  66\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  4\n",
      "***************************\n",
      "06/29/2021 01:24:46 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-4657b09970548b03.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-d160df52d0da7923.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:24:47 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-258ac55bc66ace11.arrow\n",
      "06/29/2021 01:24:47 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-5cf8042e794a4a86.arrow\n",
      "{'loss': 1.27, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 16.5645, 'train_samples_per_second': 69.969, 'train_steps_per_second': 2.234, 'train_loss': 1.2699557123957455, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =       1.27\n",
      "  train_runtime            = 0:00:16.56\n",
      "  train_samples            =       1159\n",
      "  train_samples_per_second =     69.969\n",
      "  train_steps_per_second   =      2.234\n",
      "06/29/2021 01:25:05 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:25:05 - INFO - utils_qa -   Post-processing 5 example predictions split into 28 features.\n",
      "06/29/2021 01:25:05 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-4\\eval_predictions.json.\n",
      "06/29/2021 01:25:05 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =    1.0\n",
      "  eval_exact_match =    0.0\n",
      "  eval_f1          = 1.3333\n",
      "  eval_samples     =     28\n",
      "06/29/2021 01:25:05 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:25:05 - INFO - utils_qa -   Post-processing 5 example predictions split into 28 features.\n",
      "06/29/2021 01:25:05 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-4\\predict_predictions.json.\n",
      "06/29/2021 01:25:05 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =     28\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 2.3529\n",
      "***************************\n",
      "Split  5\n",
      "***************************\n",
      "06/29/2021 01:25:06 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at ../data/squad_bioASQ_covidQA/covid\\cache-61002a654c90adbf.arrow and ../data/squad_bioASQ_covidQA/covid\\cache-edd19fa93b25fd2d.arrow\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 5\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 157\n",
      "})}\n",
      "06/29/2021 01:25:07 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-ff4147a0902e19cf.arrow\n",
      "06/29/2021 01:25:07 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/covid\\cache-3993fae537b96ee6.arrow\n",
      "{'loss': 1.2743, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 15.1791, 'train_samples_per_second': 69.701, 'train_steps_per_second': 2.24, 'train_loss': 1.2743142071892233, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.2743\n",
      "  train_runtime            = 0:00:15.17\n",
      "  train_samples            =       1058\n",
      "  train_samples_per_second =     69.701\n",
      "  train_steps_per_second   =       2.24\n",
      "06/29/2021 01:25:23 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 01:25:24 - INFO - utils_qa -   Post-processing 5 example predictions split into 86 features.\n",
      "06/29/2021 01:25:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-5\\eval_predictions.json.\n",
      "06/29/2021 01:25:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  86\n",
      "06/29/2021 01:25:24 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 01:25:25 - INFO - utils_qa -   Post-processing 5 example predictions split into 71 features.\n",
      "06/29/2021 01:25:25 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-5\\predict_predictions.json.\n",
      "06/29/2021 01:25:25 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  71\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 3.2\n",
      "Finished process\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "directory = '../models/gradual_ft_baseline/'\n",
    "output_dir = '../models/gradual_ft_baseline/Split-' + str(k_fold)+'/'\n",
    "for i in range(K):\n",
    "    print('**************************************************')\n",
    "    print('==================================================')\n",
    "    print('          At Gradual Fine Tuning Step: ',i+1)\n",
    "    print('**************************************************')\n",
    "    print('==================================================')\n",
    "    if i < 1:\n",
    "        run_gradual_ft(directory,'roberta-base', k_fold )\n",
    "    else:\n",
    "        run_gradual_ft(directory, output_dir, k_fold)\n",
    "\n",
    "    if to_remove_per_step > squad_qa.num_rows:\n",
    "        print('not enough data')\n",
    "        break\n",
    "    squad_qa.shuffle()\n",
    "    covid_qa = datasets.Dataset.from_dict(covid_qa[:])\n",
    "    squad_qa = datasets.Dataset.from_dict(squad_qa[:-to_remove_per_step])\n",
    "    make_and_save_full_dataset(covid_qa,squad_qa,covid_and_squad_dataset_path)\n",
    "\n",
    "print('Finished process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
