{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "bio_file = '../bioASQ/bioASQ.json'\n",
    "\n",
    "def get_data_from_json(filename):\n",
    "    jsonfile = open(covid_file, 'r')\n",
    "    data = jsonfile.read()\n",
    "    jsonfile.close()\n",
    "    return json.loads(data)\n",
    "\n",
    "#datasets.set_caching_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def make_and_save_full_dataset(covid=None, squad = None, bioASQ = None, path = '../data/squad_bioASQ_covidQA/'):\n",
    "    squad = datasets.Dataset.from_dict(squad_qa[:])\n",
    "    bioASQ = datasets.Dataset.from_dict(bioASQ[:])\n",
    "    if covid is not None:\n",
    "        full_data = datasets.dataset_dict.DatasetDict({'squad':squad, 'covid':covid,  'bio':bioASQ})\n",
    "    else:\n",
    "        full_data = datasets.dataset_dict.DatasetDict({'squad':squad,'bio':bioASQ})\n",
    "\n",
    "    full_data.save_to_disk(path)\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return datasets.load_dataset('custom_squad.py', data_files= {'train':filename})['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a396986d6b3a2375\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "Using custom data configuration default-8fdbe041288a2f4d\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-8fdbe041288a2f4d\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n"
     ]
    }
   ],
   "source": [
    "covid_qa = get_dataset(covid_file)\n",
    "bio_qa = get_dataset(bio_file)\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "\n",
    "covid_bio_squad_dataset_path = \"../data/squad_bioASQ_covidQA_method_one\"\n",
    "\n",
    "#this is just for testing purposes, I am going to make both of these files very small only at max 3000 datasets\n",
    "squad_qa = datasets.Dataset.from_dict(squad_qa[:50])\n",
    "bio_qa = datasets.Dataset.from_dict(bio_qa[:20])\n",
    "covid_qa = datasets.Dataset.from_dict(covid_qa[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_and_save_full_dataset(squad=squad_qa,bioASQ=bio_qa,path=covid_bio_squad_dataset_path)\n",
    "\n",
    "K = 4\n",
    "to_remove_per_step = int(squad_qa.num_rows / K)\n",
    "bio_remove_per_step = int(bio_qa.num_rows / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_gradual_ft(output_dir, checkpoint, k_fold, dataset_path):\n",
    "    !python run_qa.py \\\n",
    "      --model_name_or_path {checkpoint} \\\n",
    "      --dataset_name {dataset_path}\\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --do_predict \\\n",
    "      --per_device_train_batch_size 32\\\n",
    "      --per_device_eval_batch_size 32\\\n",
    "      --evaluation_strategy \"no\" \\\n",
    "      --save_strategy \"no\" \\\n",
    "      --logging_strategy \"epoch\" \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --max_seq_length 384 \\\n",
    "      --doc_stride 128 \\\n",
    "      --k_fold_cross_valid {k_fold} \\\n",
    "      --output_dir {output_dir} \\\n",
    "      --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**************************************************\n",
      "==================================================\n",
      "          At Gradual Fine Tuning Step:  1\n",
      "==================================================\n",
      "**************************************************\n",
      "\n",
      "\n",
      "07/01/2021 11:30:34 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 11:30:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_method_one/checkpoint-1/runs\\Jul01_11-30-34_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_method_one/checkpoint-1/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint-1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_method_one/checkpoint-1/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    bio: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n",
      "////////////////////////////////////////////////////////////////////\n",
      "////////////////////////////////////////////////////////////////////\n",
      "             Using the default covid_qa values\n",
      "////////////////////////////////////////////////////////////////////\n",
      "////////////////////////////////////////////////////////////////////\n",
      "\n",
      "07/01/2021 11:30:34 - WARNING - datasets.builder -   Using custom data configuration default-a396986d6b3a2375\n",
      "07/01/2021 11:30:34 - WARNING - datasets.builder -   Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "{'loss': 5.715, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 6.5535, 'train_samples_per_second': 50.202, 'train_steps_per_second': 1.678, 'train_loss': 5.7149810791015625, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      5.715\n",
      "  train_runtime            = 0:00:06.55\n",
      "  train_samples            =        329\n",
      "  train_samples_per_second =     50.202\n",
      "  train_steps_per_second   =      1.678\n",
      "07/01/2021 11:30:50 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:30:50 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "07/01/2021 11:30:50 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\\eval_predictions.json.\n",
      "07/01/2021 11:30:50 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match =     0.0\n",
      "  eval_f1          = 13.3333\n",
      "  eval_samples     =      31\n",
      "07/01/2021 11:30:50 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:30:50 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:30:50 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\\predict_predictions.json.\n",
      "07/01/2021 11:30:50 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =     32\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 9.5238\n",
      "***************************\n",
      "Split  2\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "{'loss': 5.6748, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.5014, 'train_samples_per_second': 72.2, 'train_steps_per_second': 2.444, 'train_loss': 5.67484560879794, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.6748\n",
      "  train_runtime            = 0:00:04.50\n",
      "  train_samples            =        325\n",
      "  train_samples_per_second =       72.2\n",
      "  train_steps_per_second   =      2.444\n",
      "07/01/2021 11:31:01 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:31:01 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:31:01 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\\eval_predictions.json.\n",
      "07/01/2021 11:31:01 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match =     0.0\n",
      "  eval_f1          = 26.6667\n",
      "  eval_samples     =      32\n",
      "07/01/2021 11:31:01 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:31:01 - INFO - utils_qa -   Post-processing 1 example predictions split into 35 features.\n",
      "07/01/2021 11:31:01 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\\predict_predictions.json.\n",
      "07/01/2021 11:31:01 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =     35\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 6.8966\n",
      "***************************\n",
      "Split  3\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "{'loss': 5.6745, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.4267, 'train_samples_per_second': 74.322, 'train_steps_per_second': 2.485, 'train_loss': 5.6745473688299, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.6745\n",
      "  train_runtime            = 0:00:04.42\n",
      "  train_samples            =        329\n",
      "  train_samples_per_second =     74.322\n",
      "  train_steps_per_second   =      2.485\n",
      "07/01/2021 11:31:11 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:31:12 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "07/01/2021 11:31:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\\eval_predictions.json.\n",
      "07/01/2021 11:31:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =  1.0\n",
      "  eval_exact_match =  0.0\n",
      "  eval_f1          = 6.25\n",
      "  eval_samples     =   31\n",
      "07/01/2021 11:31:12 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:31:12 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:31:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\\predict_predictions.json.\n",
      "07/01/2021 11:31:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =     32\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 9.0909\n",
      "***************************\n",
      "Split  4\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "{'loss': 5.6811, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.3821, 'train_samples_per_second': 74.849, 'train_steps_per_second': 2.51, 'train_loss': 5.681053161621094, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.6811\n",
      "  train_runtime            = 0:00:04.38\n",
      "  train_samples            =        328\n",
      "  train_samples_per_second =     74.849\n",
      "  train_steps_per_second   =       2.51\n",
      "07/01/2021 11:31:22 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:31:23 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:31:23 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\\eval_predictions.json.\n",
      "07/01/2021 11:31:23 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =    1.0\n",
      "  eval_exact_match =    0.0\n",
      "  eval_f1          = 5.2632\n",
      "  eval_samples     =     32\n",
      "07/01/2021 11:31:23 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:31:23 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:31:23 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\\predict_predictions.json.\n",
      "07/01/2021 11:31:23 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =      32\n",
      "  test_exact_match =     0.0\n",
      "  test_f1          = 15.3846\n",
      "***************************\n",
      "Split  5\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "{'loss': 5.6681, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "\n",
      "\n",
      "**************************************************{'train_runtime': 4.3666, 'train_samples_per_second': 74.887, 'train_steps_per_second': 2.519, 'train_loss': 5.668061689897017, 'epoch': 1.0}\n",
      "\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.6681\n",
      "  train_runtime            = 0:00:04.36\n",
      "  train_samples            =        327\n",
      "  train_samples_per_second =     74.887\n",
      "  train_steps_per_second   =      2.519\n",
      "07/01/2021 11:31:33 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:31:33 - INFO - utils_qa -   Post-processing 1 example predictions split into 33 features.\n",
      "07/01/2021 11:31:34 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\\eval_predictions.json.\n",
      "==================================================07/01/2021 11:31:34 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\\eval_nbest_predictions.json.\n",
      "\n",
      "          At Gradual Fine Tuning Step: ***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match =     0.0 2\n",
      "\n",
      "  eval_f1          = 13.0435==================================================\n",
      "\n",
      "  eval_samples     =      33\n",
      "07/01/2021 11:31:34 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:31:34 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:31:34 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\\predict_predictions.json.\n",
      "**************************************************\n",
      "\n",
      "07/01/2021 11:31:34 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\\predict_nbest_predictions.json.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-07-01 11:30:34,947 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:30:34,947 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-01 11:30:35,273 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:30:35,611 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:30:35,611 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:37,566 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:37,566 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:37,566 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:37,566 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:37,566 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:37,566 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-07-01 11:30:37,967 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-07-01 11:30:38,690 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-07-01 11:30:38,690 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.58ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.58ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 19.61ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 19.61ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:30:42,625 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:30:42,625 >>   Num examples = 329\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:30:42,625 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:30:42,625 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:30:42,625 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:30:42,625 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:30:42,625 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:02<00:25,  2.57s/it]\n",
      " 18%|#8        | 2/11 [00:02<00:17,  1.93s/it]\n",
      " 27%|##7       | 3/11 [00:03<00:11,  1.47s/it]\n",
      " 36%|###6      | 4/11 [00:03<00:08,  1.16s/it]\n",
      " 45%|####5     | 5/11 [00:04<00:05,  1.07it/s]\n",
      " 55%|#####4    | 6/11 [00:04<00:03,  1.28it/s]\n",
      " 64%|######3   | 7/11 [00:05<00:02,  1.49it/s]\n",
      " 73%|#######2  | 8/11 [00:05<00:01,  1.66it/s]\n",
      " 82%|########1 | 9/11 [00:05<00:01,  1.81it/s]\n",
      " 91%|######### | 10/11 [00:06<00:00,  1.95it/s]\n",
      "100%|##########| 11/11 [00:06<00:00,  2.45it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:06<00:00,  2.45it/s][INFO|trainer.py:1349] 2021-07-01 11:30:49,177 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:06<00:00,  2.45it/s]\n",
      "100%|##########| 11/11 [00:06<00:00,  1.68it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:30:49,180 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:30:49,181 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:30:49,887 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:30:49,889 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:30:49,889 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:30:50,005 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:30:50,008 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:30:50,008 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:30:50,008 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.01it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  5.29it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:30:50,359 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:30:50,361 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:30:50,361 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:30:50,361 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.90it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:30:51,006 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:30:51,007 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-01 11:30:51,331 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:30:51,657 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:30:51,657 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:53,603 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:53,604 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:53,604 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:53,604 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:53,604 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:30:53,604 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-07-01 11:30:53,982 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-07-01 11:30:54,763 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-07-01 11:30:54,763 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.77ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.77ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 15.62ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.42ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:05<00:00,  5.40s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:30:55,921 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:30:55,921 >>   Num examples = 325\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:30:55,921 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:30:55,921 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:30:55,921 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:30:55,921 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:30:55,921 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.27it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:03,  2.31it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.33it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:02,  2.34it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.35it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.35it/s]\n",
      " 64%|######3   | 7/11 [00:02<00:01,  2.35it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.32it/s]\n",
      " 82%|########1 | 9/11 [00:03<00:00,  2.20it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.19it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.83it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.83it/s][INFO|trainer.py:1349] 2021-07-01 11:31:00,422 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.83it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.45it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:31:00,423 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:31:00,425 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:31:00,998 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:31:01,000 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:31:01,001 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:01,080 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:01,082 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:01,082 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:01,082 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.63it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.62it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:01,395 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:01,397 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:01,397 >>   Num examples = 35\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:01,397 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.99it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:31:02,070 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:02,071 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_samples  =      32\n",
      "  test_exact_match =     0.0\n",
      "  test_f1          = 12.9032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-01 11:31:02,396 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:31:02,715 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:02,716 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:04,652 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:04,652 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:04,653 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:04,653 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:04,653 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:04,653 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-07-01 11:31:05,034 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-07-01 11:31:05,809 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-07-01 11:31:05,809 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.55ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.55ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.61ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.61ba/s]\n",
      "\n",
      "100%|##########| 2/2 [00:05<00:00,  2.67s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:31:06,905 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:31:06,905 >>   Num examples = 329\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:31:06,905 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:31:06,905 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:31:06,905 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:31:06,905 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:31:06,905 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.29it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:03,  2.31it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.33it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.33it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.35it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.35it/s]\n",
      " 64%|######3   | 7/11 [00:02<00:01,  2.34it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.34it/s]\n",
      " 82%|########1 | 9/11 [00:03<00:00,  2.33it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.34it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.90it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.90it/s][INFO|trainer.py:1349] 2021-07-01 11:31:11,332 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.90it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.49it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:31:11,334 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:31:11,335 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:31:11,898 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:31:11,899 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:31:11,900 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:11,978 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:11,979 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:11,979 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:11,979 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.20it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.94it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:12,277 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:12,278 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:12,278 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:12,278 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.77it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:31:13,107 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:13,107 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-01 11:31:13,424 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:31:13,745 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:13,745 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:15,681 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:15,682 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:15,682 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:15,682 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:15,682 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:15,682 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-07-01 11:31:16,065 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-07-01 11:31:16,823 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-07-01 11:31:16,823 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.90ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.90ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.50ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.23ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:05<00:00,  5.48s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:31:17,923 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:31:17,923 >>   Num examples = 328\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:31:17,923 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:31:17,923 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:31:17,923 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:31:17,923 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:31:17,924 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.24it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:03,  2.27it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.31it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.32it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.34it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.35it/s]\n",
      " 64%|######3   | 7/11 [00:02<00:01,  2.35it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.36it/s]\n",
      " 82%|########1 | 9/11 [00:03<00:00,  2.36it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.37it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.97it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.97it/s][INFO|trainer.py:1349] 2021-07-01 11:31:22,306 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.97it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.51it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:31:22,308 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:31:22,309 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:31:22,868 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:31:22,870 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:31:22,870 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:22,948 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:22,949 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:22,950 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:22,950 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.90it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.85it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:23,251 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:23,252 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:23,252 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:23,252 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.90it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:31:23,913 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:23,914 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-01 11:31:24,239 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 11:31:24,578 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:24,579 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:26,520 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:26,520 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:26,520 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:26,520 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:26,520 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 11:31:26,520 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-07-01 11:31:26,898 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-07-01 11:31:27,653 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-07-01 11:31:27,653 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.01ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.01ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.61ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.61ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:05<00:00,  5.30s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:31:28,711 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:31:28,712 >>   Num examples = 327\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:31:28,712 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:31:28,712 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:31:28,712 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:31:28,712 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:31:28,712 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.28it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:03,  2.31it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.32it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:02,  2.34it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.35it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.36it/s]\n",
      " 64%|######3   | 7/11 [00:02<00:01,  2.36it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.37it/s]\n",
      " 82%|########1 | 9/11 [00:03<00:00,  2.37it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.36it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.99it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.99it/s][INFO|trainer.py:1349] 2021-07-01 11:31:33,078 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.99it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.52it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:31:33,080 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:31:33,081 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:31:33,636 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:31:33,637 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:31:33,638 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-1//Split-5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:33,716 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:33,718 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:33,718 >>   Num examples = 33\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:33,718 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.49it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.76it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:34,045 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:34,047 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:34,047 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:34,047 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.90it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 11:31:37 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 11:31:37 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_method_one/checkpoint-2/runs\\Jul01_11-31-37_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_method_one/checkpoint-2/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint-2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_method_one/checkpoint-2/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    bio: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n",
      "////////////////////////////////////////////////////////////////////\n",
      "////////////////////////////////////////////////////////////////////\n",
      "             Using the default covid_qa values\n",
      "////////////////////////////////////////////////////////////////////\n",
      "////////////////////////////////////////////////////////////////////\n",
      "\n",
      "07/01/2021 11:31:37 - WARNING - datasets.builder -   Using custom data configuration default-a396986d6b3a2375\n",
      "07/01/2021 11:31:37 - WARNING - datasets.builder -   Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 73\n",
      "})}\n",
      "{'loss': 5.165, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 5.1931, 'train_samples_per_second': 62.391, 'train_steps_per_second': 2.118, 'train_loss': 5.165038368918679, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      5.165\n",
      "  train_runtime            = 0:00:05.19\n",
      "  train_samples            =        324\n",
      "  train_samples_per_second =     62.391\n",
      "  train_steps_per_second   =      2.118\n",
      "07/01/2021 11:31:46 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:31:46 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "07/01/2021 11:31:47 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\\eval_predictions.json.\n",
      "07/01/2021 11:31:47 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  31\n",
      "07/01/2021 11:31:47 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:31:47 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:31:47 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\\predict_predictions.json.\n",
      "07/01/2021 11:31:47 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  2\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 73\n",
      "})}\n",
      "{'loss': 5.2102, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.254, 'train_samples_per_second': 75.223, 'train_steps_per_second': 2.351, 'train_loss': 5.2102195739746096, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2102\n",
      "  train_runtime            = 0:00:04.25\n",
      "  train_samples            =        320\n",
      "  train_samples_per_second =     75.223\n",
      "  train_steps_per_second   =      2.351\n",
      "07/01/2021 11:31:54 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:31:54 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:31:55 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\\eval_predictions.json.\n",
      "07/01/2021 11:31:55 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  32\n",
      "07/01/2021 11:31:55 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:31:55 - INFO - utils_qa -   Post-processing 1 example predictions split into 35 features.\n",
      "07/01/2021 11:31:55 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\\predict_predictions.json.\n",
      "07/01/2021 11:31:55 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  35\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  3\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 73\n",
      "})}\n",
      "{'loss': 5.2098, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.402, 'train_samples_per_second': 73.603, 'train_steps_per_second': 2.499, 'train_loss': 5.2097601457075635, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2098\n",
      "  train_runtime            = 0:00:04.40\n",
      "  train_samples            =        324\n",
      "  train_samples_per_second =     73.603\n",
      "  train_steps_per_second   =      2.499\n",
      "07/01/2021 11:32:02 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:32:02 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "07/01/2021 11:32:02 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\\eval_predictions.json.\n",
      "07/01/2021 11:32:02 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  31\n",
      "07/01/2021 11:32:02 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:32:03 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:03 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\\predict_predictions.json.\n",
      "07/01/2021 11:32:03 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  4\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 73\n",
      "})}\n",
      "{'loss': 5.2476, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.3657, 'train_samples_per_second': 73.986, 'train_steps_per_second': 2.52, 'train_loss': 5.2476137334650215, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2476\n",
      "  train_runtime            = 0:00:04.36\n",
      "  train_samples            =        323\n",
      "  train_samples_per_second =     73.986\n",
      "  train_steps_per_second   =       2.52\n",
      "07/01/2021 11:32:10 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:32:10 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:10 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\\eval_predictions.json.\n",
      "07/01/2021 11:32:10 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  32\n",
      "07/01/2021 11:32:10 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:32:10 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:10 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\\predict_predictions.json.\n",
      "07/01/2021 11:32:10 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  5\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 73\n",
      "})}\n",
      "{'loss': 5.2087, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.4254, 'train_samples_per_second': 72.763, 'train_steps_per_second': 2.486, 'train_loss': 5.208669489080256, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2087\n",
      "  train_runtime            = 0:00:04.42\n",
      "  train_samples            =        322\n",
      "  train_samples_per_second =     72.763\n",
      "  train_steps_per_second   =      2.486\n",
      "07/01/2021 11:32:18 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:32:18 - INFO - utils_qa -   Post-processing 1 example predictions split into 33 features.\n",
      "07/01/2021 11:32:18 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\\eval_predictions.json.\n",
      "07/01/2021 11:32:18 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  33\n",
      "07/01/2021 11:32:18 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:32:18 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:18 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\\predict_predictions.json.\n",
      "07/01/2021 11:32:18 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "\n",
      "\n",
      "**************************************************\n",
      "==================================================\n",
      "          At Gradual Fine Tuning Step:  3\n",
      "==================================================\n",
      "**************************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 11:31:37,786 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:37,786 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:31:37,789 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:37,790 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:37,790 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:37,790 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:37,790 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:37,790 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:37,790 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:31:37,863 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:31:38,668 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:31:38,668 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.37ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 17.54ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 18.18ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:31:40,940 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:31:40,940 >>   Num examples = 324\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:31:40,940 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:31:40,940 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:31:40,940 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:31:40,940 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:31:40,940 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:01<00:12,  1.29s/it]\n",
      " 18%|#8        | 2/11 [00:01<00:09,  1.03s/it]\n",
      " 27%|##7       | 3/11 [00:02<00:06,  1.18it/s]\n",
      " 36%|###6      | 4/11 [00:02<00:05,  1.39it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:03,  1.58it/s]\n",
      " 55%|#####4    | 6/11 [00:03<00:02,  1.76it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:02,  1.90it/s]\n",
      " 73%|#######2  | 8/11 [00:04<00:01,  2.02it/s]\n",
      " 82%|########1 | 9/11 [00:04<00:00,  2.12it/s]\n",
      " 91%|######### | 10/11 [00:05<00:00,  2.18it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:05<00:00,  2.18it/s][INFO|trainer.py:1349] 2021-07-01 11:31:46,133 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:05<00:00,  2.18it/s]\n",
      "100%|##########| 11/11 [00:05<00:00,  2.12it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:31:46,135 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:31:46,136 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:31:46,700 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:31:46,701 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:31:46,702 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:46,779 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:46,780 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:46,781 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:46,781 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.05it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.90it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:47,077 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:47,079 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:47,079 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:47,079 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.91it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:31:47,380 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:47,381 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:31:47,381 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:47,382 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:47,382 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:47,382 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:47,382 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:47,382 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:47,382 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:31:47,441 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:31:48,326 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:31:48,326 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.52ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.43ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.66ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 18.87ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.43s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:31:49,666 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:31:49,666 >>   Num examples = 320\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:31:49,666 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:31:49,666 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:31:49,666 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:31:49,666 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:31:49,666 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:03,  2.29it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.32it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.33it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.34it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.35it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.35it/s]\n",
      " 70%|#######   | 7/10 [00:02<00:01,  2.36it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.36it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.35it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.34it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.34it/s][INFO|trainer.py:1349] 2021-07-01 11:31:53,920 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.34it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.35it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:31:53,922 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:31:53,923 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:31:54,635 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:31:54,636 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:31:54,637 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:54,763 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:54,765 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:54,765 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:54,765 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.94it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  7.94it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  4.33it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:31:55,172 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:31:55,173 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:31:55,174 >>   Num examples = 35\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:31:55,174 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.99it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:31:55,517 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:31:55,517 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:31:55,520 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:55,521 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:55,521 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:55,521 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:55,521 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:55,521 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:31:55,521 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:31:55,601 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:31:56,453 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:31:56,453 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.23ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 17.86ba/s]\n",
      "\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:31:57,529 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:31:57,529 >>   Num examples = 324\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:31:57,529 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:31:57,529 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:31:57,529 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:31:57,529 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:31:57,529 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.22it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:04,  2.24it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.25it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.27it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.29it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.31it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:01,  2.32it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.32it/s]\n",
      " 82%|########1 | 9/11 [00:03<00:00,  2.34it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.35it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.35it/s][INFO|trainer.py:1349] 2021-07-01 11:32:01,931 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.35it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.50it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:32:01,933 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:32:01,934 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:32:02,513 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:32:02,514 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:32:02,515 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:02,601 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:02,603 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:02,603 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:02,603 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.05it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.99it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:02,897 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:02,899 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:02,899 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:02,899 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.49it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:32:03,217 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:32:03,218 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:32:03,220 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:03,221 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:03,221 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:03,221 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:03,221 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:03,221 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:03,221 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:32:03,298 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:32:04,162 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:32:04,162 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.94ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  6.94ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 18.18ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 18.52ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.26s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:32:05,321 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:32:05,321 >>   Num examples = 323\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:32:05,321 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:32:05,321 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:32:05,321 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:32:05,321 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:32:05,321 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.25it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:03,  2.25it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.27it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.29it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.31it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.33it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:01,  2.34it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.35it/s]\n",
      " 82%|########1 | 9/11 [00:03<00:00,  2.35it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.36it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.36it/s][INFO|trainer.py:1349] 2021-07-01 11:32:09,686 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.36it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.52it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:32:09,688 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:32:09,690 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:32:10,255 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:32:10,256 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:32:10,257 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:10,346 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:10,348 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:10,348 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:10,348 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.91it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.76it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:10,652 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:10,654 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:10,654 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:10,654 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.90it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:32:10,960 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:32:10,960 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:32:10,963 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:10,964 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:10,964 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:10,964 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:10,964 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:10,964 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:10,964 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:32:11,045 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:32:11,924 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:32:11,924 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.48ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.48ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 16.95ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 16.13ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.22s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:32:13,028 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:32:13,029 >>   Num examples = 322\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:32:13,029 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:32:13,029 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:32:13,029 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:32:13,029 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:32:13,029 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.28it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:03,  2.29it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.29it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.30it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.29it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.30it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:01,  2.30it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.30it/s]\n",
      " 82%|########1 | 9/11 [00:03<00:00,  2.31it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.31it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.31it/s][INFO|trainer.py:1349] 2021-07-01 11:32:17,454 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.31it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.49it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:32:17,456 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:32:17,457 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:32:18,062 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:32:18,063 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:32:18,063 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-2//Split-5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:18,153 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:18,155 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:18,155 >>   Num examples = 33\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:18,155 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.87it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.30it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:18,495 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:18,497 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:18,497 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:18,497 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.49it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 11:32:21 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 11:32:21 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_method_one/checkpoint-3/runs\\Jul01_11-32-21_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_method_one/checkpoint-3/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint-3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_method_one/checkpoint-3/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 38\n",
      "    })\n",
      "    bio: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n",
      "////////////////////////////////////////////////////////////////////\n",
      "////////////////////////////////////////////////////////////////////\n",
      "             Using the default covid_qa values\n",
      "////////////////////////////////////////////////////////////////////\n",
      "////////////////////////////////////////////////////////////////////\n",
      "\n",
      "07/01/2021 11:32:22 - WARNING - datasets.builder -   Using custom data configuration default-a396986d6b3a2375\n",
      "07/01/2021 11:32:22 - WARNING - datasets.builder -   Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 61\n",
      "})}\n",
      "{'loss': 4.0886, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 5.0618, 'train_samples_per_second': 61.638, 'train_steps_per_second': 1.976, 'train_loss': 4.088579940795898, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     4.0886\n",
      "  train_runtime            = 0:00:05.06\n",
      "  train_samples            =        312\n",
      "  train_samples_per_second =     61.638\n",
      "  train_steps_per_second   =      1.976\n",
      "07/01/2021 11:32:31 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:32:31 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "07/01/2021 11:32:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\\eval_predictions.json.\n",
      "07/01/2021 11:32:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  31\n",
      "07/01/2021 11:32:31 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:32:31 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\\predict_predictions.json.\n",
      "07/01/2021 11:32:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  2\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 61\n",
      "})}\n",
      "{'loss': 4.0364, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.1574, 'train_samples_per_second': 74.085, 'train_steps_per_second': 2.405, 'train_loss': 4.036429214477539, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     4.0364\n",
      "  train_runtime            = 0:00:04.15\n",
      "  train_samples            =        308\n",
      "  train_samples_per_second =     74.085\n",
      "  train_steps_per_second   =      2.405\n",
      "07/01/2021 11:32:38 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:32:39 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\\eval_predictions.json.\n",
      "07/01/2021 11:32:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  32\n",
      "07/01/2021 11:32:39 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:32:39 - INFO - utils_qa -   Post-processing 1 example predictions split into 35 features.\n",
      "07/01/2021 11:32:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\\predict_predictions.json.\n",
      "07/01/2021 11:32:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  35\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  3\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 61\n",
      "})}\n",
      "{'loss': 3.9839, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.245, 'train_samples_per_second': 73.499, 'train_steps_per_second': 2.356, 'train_loss': 3.9839252471923827, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.9839\n",
      "  train_runtime            = 0:00:04.24\n",
      "  train_samples            =        312\n",
      "  train_samples_per_second =     73.499\n",
      "  train_steps_per_second   =      2.356\n",
      "07/01/2021 11:32:46 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:32:46 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "07/01/2021 11:32:46 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\\eval_predictions.json.\n",
      "07/01/2021 11:32:46 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  31\n",
      "07/01/2021 11:32:46 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:32:47 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:47 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\\predict_predictions.json.\n",
      "07/01/2021 11:32:47 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  4\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 61\n",
      "})}\n",
      "{'loss': 4.0969, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.239, 'train_samples_per_second': 73.366, 'train_steps_per_second': 2.359, 'train_loss': 4.096864700317383, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     4.0969\n",
      "  train_runtime            = 0:00:04.23\n",
      "  train_samples            =        311\n",
      "  train_samples_per_second =     73.366\n",
      "  train_steps_per_second   =      2.359\n",
      "07/01/2021 11:32:54 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:32:54 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:54 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\\eval_predictions.json.\n",
      "07/01/2021 11:32:54 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  32\n",
      "07/01/2021 11:32:54 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:32:54 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:32:54 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\\predict_predictions.json.\n",
      "07/01/2021 11:32:54 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  5\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 61\n",
      "})}\n",
      "{'loss': 4.1099, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.2452, 'train_samples_per_second': 73.023, 'train_steps_per_second': 2.356, 'train_loss': 4.109887313842774, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     4.1099\n",
      "  train_runtime            = 0:00:04.24\n",
      "  train_samples            =        310\n",
      "  train_samples_per_second =     73.023\n",
      "  train_steps_per_second   =      2.356\n",
      "07/01/2021 11:33:02 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:33:02 - INFO - utils_qa -   Post-processing 1 example predictions split into 33 features.\n",
      "07/01/2021 11:33:02 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\\eval_predictions.json.\n",
      "07/01/2021 11:33:02 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  33\n",
      "07/01/2021 11:33:02 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:33:02 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:33:02 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\\predict_predictions.json.\n",
      "07/01/2021 11:33:02 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "\n",
      "\n",
      "**************************************************\n",
      "==================================================\n",
      "          At Gradual Fine Tuning Step:  4\n",
      "==================================================\n",
      "**************************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 11:32:22,384 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:32:22,384 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-1/Split-4/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:32:22,397 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:22,397 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:22,397 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:22,397 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:22,397 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:22,397 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:22,397 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:32:22,473 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:32:23,176 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:32:23,177 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.49ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.49ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 18.87ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 17.24ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:32:25,576 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:32:25,577 >>   Num examples = 312\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:32:25,577 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:32:25,577 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:32:25,577 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:32:25,577 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:32:25,577 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:01<00:11,  1.32s/it]\n",
      " 20%|##        | 2/10 [00:01<00:08,  1.05s/it]\n",
      " 30%|###       | 3/10 [00:02<00:06,  1.16it/s]\n",
      " 40%|####      | 4/10 [00:02<00:04,  1.36it/s]\n",
      " 50%|#####     | 5/10 [00:03<00:03,  1.56it/s]\n",
      " 60%|######    | 6/10 [00:03<00:02,  1.74it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  1.88it/s]\n",
      " 80%|########  | 8/10 [00:04<00:00,  2.01it/s]\n",
      " 90%|######### | 9/10 [00:04<00:00,  2.10it/s]\n",
      "100%|##########| 10/10 [00:05<00:00,  2.31it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:05<00:00,  2.31it/s][INFO|trainer.py:1349] 2021-07-01 11:32:30,639 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:05<00:00,  2.31it/s]\n",
      "100%|##########| 10/10 [00:05<00:00,  1.98it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:32:30,641 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:32:30,642 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:32:31,232 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:32:31,234 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:32:31,234 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:31,316 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:31,318 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:31,318 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:31,318 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.24it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.25it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:31,608 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:31,610 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:31,610 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:31,610 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.76it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:32:31,917 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:32:31,918 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-1/Split-5/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:32:31,931 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:31,931 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:31,931 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:31,931 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:31,931 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:31,931 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:31,931 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:32:32,029 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:32:32,879 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:32:32,879 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.62ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.62ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 18.18ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.99ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.25s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:32:34,015 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:32:34,015 >>   Num examples = 308\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:32:34,015 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:32:34,015 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:32:34,015 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:32:34,015 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:32:34,015 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:03,  2.26it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.30it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.30it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.33it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.33it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.34it/s]\n",
      " 70%|#######   | 7/10 [00:02<00:01,  2.35it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.33it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.32it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.56it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.56it/s][INFO|trainer.py:1349] 2021-07-01 11:32:38,173 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.56it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.41it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:32:38,175 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:32:38,176 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:32:38,783 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:32:38,784 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:32:38,784 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:38,884 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:38,886 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:38,886 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:38,886 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.49it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.58it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:39,198 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:39,200 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:39,200 >>   Num examples = 35\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:39,200 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.87it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:32:39,555 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:32:39,556 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-1/Split-1/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:32:39,569 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:39,569 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:39,569 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:39,570 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:39,570 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:39,570 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:39,570 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:32:39,655 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:32:40,526 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:32:40,526 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.70ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.62ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 16.39ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 15.63ba/s]\n",
      "\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:32:41,684 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:32:41,684 >>   Num examples = 312\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:32:41,684 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:32:41,685 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:32:41,685 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:32:41,685 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:32:41,685 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:04,  2.22it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.24it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.27it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.29it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.30it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.29it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  2.30it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.31it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.30it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.48it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.48it/s][INFO|trainer.py:1349] 2021-07-01 11:32:45,930 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.48it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.36it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:32:45,932 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:32:45,934 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:32:46,563 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:32:46,564 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:32:46,565 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:46,661 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:46,662 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:46,662 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:46,662 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.31it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.17it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:46,983 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:46,985 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:46,985 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:46,985 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.49it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:32:47,303 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:32:47,304 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-1/Split-2/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:32:47,318 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:47,319 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:47,319 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:47,319 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:47,319 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:47,319 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:47,319 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:32:47,397 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:32:48,200 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:32:48,200 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 18.18ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 18.52ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.23s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:32:49,378 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:32:49,378 >>   Num examples = 311\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:32:49,379 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:32:49,379 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:32:49,379 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:32:49,379 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:32:49,379 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:03,  2.31it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.33it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.32it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.32it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.30it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.30it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  2.29it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.30it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.31it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.47it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.47it/s][INFO|trainer.py:1349] 2021-07-01 11:32:53,618 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.47it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.36it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:32:53,620 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:32:53,622 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:32:54,245 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:32:54,247 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:32:54,247 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:54,342 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:54,344 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:54,344 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:54,344 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.75it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.62it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:32:54,663 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:32:54,664 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:32:54,664 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:32:54,664 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.63it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:32:55,009 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:32:55,010 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-1/Split-3/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:32:55,023 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:55,024 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:55,024 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:55,024 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:55,024 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:55,024 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:32:55,024 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:32:55,106 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:32:55,923 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:32:55,923 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.26ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.26ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 14.29ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 16.67ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.46s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:32:57,280 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:32:57,280 >>   Num examples = 310\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:32:57,280 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:32:57,280 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:32:57,280 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:32:57,280 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:32:57,280 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:04,  2.18it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.23it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.25it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.27it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.25it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.27it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  2.29it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.30it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.31it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.51it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.51it/s][INFO|trainer.py:1349] 2021-07-01 11:33:01,526 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.51it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.36it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:33:01,528 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:33:01,529 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:33:02,130 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:33:02,131 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:33:02,132 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-3//Split-5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:02,224 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:02,225 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:02,226 >>   Num examples = 33\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:02,226 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.75it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.27it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:02,554 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:02,555 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:02,556 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:02,556 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 11:33:06 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 11:33:06 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_method_one/checkpoint-4/runs\\Jul01_11-33-06_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_method_one/checkpoint-4/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint-4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_method_one/checkpoint-4/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 38\n",
      "    })\n",
      "    bio: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "////////////////////////////////////////////////////////////////////\n",
      "////////////////////////////////////////////////////////////////////\n",
      "             Using the default covid_qa values\n",
      "////////////////////////////////////////////////////////////////////\n",
      "////////////////////////////////////////////////////////////////////\n",
      "\n",
      "07/01/2021 11:33:06 - WARNING - datasets.builder -   Using custom data configuration default-a396986d6b3a2375\n",
      "07/01/2021 11:33:06 - WARNING - datasets.builder -   Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 56\n",
      "})}\n",
      "{'loss': 2.9947, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 5.1176, 'train_samples_per_second': 59.989, 'train_steps_per_second': 1.954, 'train_loss': 2.994716453552246, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.9947\n",
      "  train_runtime            = 0:00:05.11\n",
      "  train_samples            =        307\n",
      "  train_samples_per_second =     59.989\n",
      "  train_steps_per_second   =      1.954\n",
      "07/01/2021 11:33:16 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:33:16 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "07/01/2021 11:33:16 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\\eval_predictions.json.\n",
      "07/01/2021 11:33:16 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  31\n",
      "07/01/2021 11:33:16 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:33:16 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:33:16 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\\predict_predictions.json.\n",
      "07/01/2021 11:33:16 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  2\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 56\n",
      "})}\n",
      "{'loss': 3.0367, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.1362, 'train_samples_per_second': 73.256, 'train_steps_per_second': 2.418, 'train_loss': 3.0367218017578126, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.0367\n",
      "  train_runtime            = 0:00:04.13\n",
      "  train_samples            =        303\n",
      "  train_samples_per_second =     73.256\n",
      "  train_steps_per_second   =      2.418\n",
      "07/01/2021 11:33:23 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:33:23 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:33:23 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\\eval_predictions.json.\n",
      "07/01/2021 11:33:23 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  32\n",
      "07/01/2021 11:33:23 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:33:24 - INFO - utils_qa -   Post-processing 1 example predictions split into 35 features.\n",
      "07/01/2021 11:33:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\\predict_predictions.json.\n",
      "07/01/2021 11:33:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  35\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  3\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 56\n",
      "})}\n",
      "{'loss': 2.9422, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.1384, 'train_samples_per_second': 74.183, 'train_steps_per_second': 2.416, 'train_loss': 2.9422256469726564, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.9422\n",
      "  train_runtime            = 0:00:04.13\n",
      "  train_samples            =        307\n",
      "  train_samples_per_second =     74.183\n",
      "  train_steps_per_second   =      2.416\n",
      "07/01/2021 11:33:30 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:33:31 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "07/01/2021 11:33:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\\eval_predictions.json.\n",
      "07/01/2021 11:33:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  31\n",
      "07/01/2021 11:33:31 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:33:31 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:33:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\\predict_predictions.json.\n",
      "07/01/2021 11:33:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  4\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 56\n",
      "})}\n",
      "{'loss': 2.9603, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.1519, 'train_samples_per_second': 73.701, 'train_steps_per_second': 2.409, 'train_loss': 2.9603052139282227, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.9603\n",
      "  train_runtime            = 0:00:04.15\n",
      "  train_samples            =        306\n",
      "  train_samples_per_second =     73.701\n",
      "  train_steps_per_second   =      2.409\n",
      "07/01/2021 11:33:38 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:33:38 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:33:38 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\\eval_predictions.json.\n",
      "07/01/2021 11:33:38 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  32\n",
      "07/01/2021 11:33:38 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:33:39 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:33:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\\predict_predictions.json.\n",
      "07/01/2021 11:33:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "***************************\n",
      "Split  5\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 56\n",
      "})}\n",
      "{'loss': 2.9688, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.115, 'train_samples_per_second': 74.119, 'train_steps_per_second': 2.43, 'train_loss': 2.9687549591064455, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.9688\n",
      "  train_runtime            = 0:00:04.11\n",
      "  train_samples            =        305\n",
      "  train_samples_per_second =     74.119\n",
      "  train_steps_per_second   =       2.43\n",
      "07/01/2021 11:33:45 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 11:33:46 - INFO - utils_qa -   Post-processing 1 example predictions split into 33 features.\n",
      "07/01/2021 11:33:46 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\\eval_predictions.json.\n",
      "07/01/2021 11:33:46 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  33\n",
      "07/01/2021 11:33:46 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 11:33:46 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "07/01/2021 11:33:46 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\\predict_predictions.json.\n",
      "07/01/2021 11:33:46 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  32\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "Finished process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 11:33:06,740 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:33:06,740 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-2/Split-4/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:33:06,752 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:06,753 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:06,753 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:06,753 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:06,753 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:06,753 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:06,753 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:33:06,832 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:33:07,552 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:33:07,552 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.67ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.67ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 18.52ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 19.23ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:33:10,242 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:33:10,243 >>   Num examples = 307\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:33:10,243 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:33:10,243 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:33:10,243 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:33:10,243 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:33:10,243 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:01<00:12,  1.36s/it]\n",
      " 20%|##        | 2/10 [00:01<00:08,  1.08s/it]\n",
      " 30%|###       | 3/10 [00:02<00:06,  1.13it/s]\n",
      " 40%|####      | 4/10 [00:02<00:04,  1.33it/s]\n",
      " 50%|#####     | 5/10 [00:03<00:03,  1.53it/s]\n",
      " 60%|######    | 6/10 [00:03<00:02,  1.71it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  1.86it/s]\n",
      " 80%|########  | 8/10 [00:04<00:01,  1.95it/s]\n",
      " 90%|######### | 9/10 [00:04<00:00,  2.05it/s]\n",
      "100%|##########| 10/10 [00:05<00:00,  2.34it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:05<00:00,  2.34it/s][INFO|trainer.py:1349] 2021-07-01 11:33:15,361 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:05<00:00,  2.34it/s]\n",
      "100%|##########| 10/10 [00:05<00:00,  1.96it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:33:15,363 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:33:15,365 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:33:15,964 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:33:15,965 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:33:15,966 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:16,049 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:16,051 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:16,051 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:16,051 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.90it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:16,339 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:16,341 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:16,341 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:16,341 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.63it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:33:16,649 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:33:16,649 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-2/Split-5/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:33:16,662 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:16,662 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:16,662 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:16,662 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:16,662 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:16,662 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:16,662 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:33:16,751 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:33:17,563 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:33:17,563 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.33ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.33ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 16.67ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.99ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.23s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:33:18,731 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:33:18,732 >>   Num examples = 303\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:33:18,732 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:33:18,732 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:33:18,732 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:33:18,732 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:33:18,732 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:04,  2.24it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.26it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.28it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.28it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.30it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.31it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  2.31it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.31it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.31it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.68it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.68it/s][INFO|trainer.py:1349] 2021-07-01 11:33:22,868 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.68it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.42it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:33:22,870 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:33:22,872 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:33:23,447 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:33:23,449 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:33:23,449 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:23,529 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:23,530 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:23,530 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:23,531 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.76it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:23,826 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:23,828 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:23,828 >>   Num examples = 35\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:23,828 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.11it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:33:24,159 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:33:24,160 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-2/Split-1/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:33:24,172 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:24,173 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:24,173 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:24,173 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:24,173 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:24,173 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:24,173 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:33:24,249 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:33:25,031 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:33:25,032 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 18.52ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.23ba/s]\n",
      "\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:33:26,120 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:33:26,120 >>   Num examples = 307\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:33:26,120 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:33:26,120 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:33:26,120 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:33:26,120 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:33:26,120 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:03,  2.26it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.29it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.30it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.31it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.32it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.32it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  2.34it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.33it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.34it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.62it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.62it/s][INFO|trainer.py:1349] 2021-07-01 11:33:30,258 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.62it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.42it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:33:30,260 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:33:30,261 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:33:30,842 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:33:30,843 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:33:30,843 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:30,923 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:30,925 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:30,925 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:30,925 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.91it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:31,215 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:31,217 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:31,217 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:31,217 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.87it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:33:31,535 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:33:31,536 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-2/Split-2/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:33:31,549 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:31,550 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:31,550 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:31,550 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:31,550 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:31,550 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:31,550 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:33:31,633 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:33:32,506 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:33:32,507 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-3/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.54ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  6.54ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 16.95ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 16.67ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.35s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:33:33,726 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:33:33,726 >>   Num examples = 306\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:33:33,726 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:33:33,726 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:33:33,726 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:33:33,726 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:33:33,726 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:03,  2.26it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.27it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.29it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.31it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.30it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.31it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  2.31it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.32it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.32it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.63it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.63it/s][INFO|trainer.py:1349] 2021-07-01 11:33:37,879 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.63it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.41it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:33:37,881 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:33:37,882 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:33:38,445 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:33:38,447 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:33:38,447 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:38,526 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:38,528 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:38,528 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:38,528 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.63it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.30it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:38,828 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:38,830 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:38,830 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:38,830 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.63it/s]\n",
      "[INFO|configuration_utils.py:528] 2021-07-01 11:33:39,136 >> loading configuration file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 11:33:39,136 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_method_one/checkpoint-2/Split-3/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 11:33:39,149 >> Didn't find file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:39,150 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:39,150 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:39,150 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:39,150 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:39,150 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 11:33:39,150 >> loading file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 11:33:39,226 >> loading weights file ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 11:33:39,997 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 11:33:39,997 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_method_one/checkpoint-3/Split-4/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00, 10.00ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  9.90ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.23ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.50ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:02<00:00,  2.09s/it]\n",
      "[INFO|trainer.py:1153] 2021-07-01 11:33:41,077 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 11:33:41,077 >>   Num examples = 305\n",
      "[INFO|trainer.py:1155] 2021-07-01 11:33:41,077 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 11:33:41,077 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 11:33:41,077 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 11:33:41,077 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 11:33:41,077 >>   Total optimization steps = 10\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|#         | 1/10 [00:00<00:04,  2.24it/s]\n",
      " 20%|##        | 2/10 [00:00<00:03,  2.27it/s]\n",
      " 30%|###       | 3/10 [00:01<00:03,  2.30it/s]\n",
      " 40%|####      | 4/10 [00:01<00:02,  2.31it/s]\n",
      " 50%|#####     | 5/10 [00:02<00:02,  2.33it/s]\n",
      " 60%|######    | 6/10 [00:02<00:01,  2.32it/s]\n",
      " 70%|#######   | 7/10 [00:03<00:01,  2.32it/s]\n",
      " 80%|########  | 8/10 [00:03<00:00,  2.33it/s]\n",
      " 90%|######### | 9/10 [00:03<00:00,  2.33it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.66it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.66it/s][INFO|trainer.py:1349] 2021-07-01 11:33:45,192 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 10/10 [00:04<00:00,  2.66it/s]\n",
      "100%|##########| 10/10 [00:04<00:00,  2.43it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 11:33:45,194 >> Saving model checkpoint to ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 11:33:45,195 >> Configuration saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 11:33:45,761 >> Model weights saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 11:33:45,762 >> tokenizer config file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 11:33:45,763 >> Special tokens file saved in ../models/gradual_ft_baseline_method_one/checkpoint-4//Split-5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:45,844 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:45,845 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:45,845 >>   Num examples = 33\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:45,845 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.49it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.90it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 11:33:46,160 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 11:33:46,161 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 11:33:46,161 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-07-01 11:33:46,161 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.76it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "for i in range(K):\n",
    "    directory = '../models/gradual_ft_baseline_method_one/checkpoint-' + str(i+1) + '/'\n",
    "    output_dir = '../models/gradual_ft_baseline_method_one/checkpoint-' + str(i) + '/' + 'Split-'\n",
    "    print('\\n\\n**************************************************')\n",
    "    print('==================================================')\n",
    "    print('          At Gradual Fine Tuning Step: ',i+1)\n",
    "    print('==================================================')\n",
    "    print('**************************************************\\n\\n')\n",
    "    if i < 1:\n",
    "        run_gradual_ft(directory,'roberta-base', k_fold, covid_bio_squad_dataset_path)\n",
    "    else:\n",
    "        run_gradual_ft(directory, output_dir, k_fold, covid_bio_squad_dataset_path)\n",
    "        squad_qa.shuffle()\n",
    "        bio_qa.shuffle()\n",
    "        covid_qa = datasets.Dataset.from_dict(covid_qa[:])\n",
    "\n",
    "    if i%2:\n",
    "        if to_remove_per_step > squad_qa.num_rows:\n",
    "            print('not enough data')\n",
    "            break\n",
    "        squad_qa = datasets.Dataset.from_dict(squad_qa[:-to_remove_per_step])\n",
    "        make_and_save_full_dataset(squad=squad_qa, bioASQ= bio_qa, path=covid_bio_squad_dataset_path)\n",
    "    else:\n",
    "        if to_remove_per_step > bio_qa.num_rows:\n",
    "            print('not enough data')\n",
    "            break\n",
    "        bio_qa = datasets.Dataset.from_dict(bio_qa[:-bio_remove_per_step])\n",
    "        make_and_save_full_dataset(squad=squad_qa, bioASQ=bio_qa,path=covid_bio_squad_dataset_path)\n",
    "\n",
    "print('Finished process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}