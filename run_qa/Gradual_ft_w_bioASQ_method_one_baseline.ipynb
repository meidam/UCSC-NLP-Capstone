{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "bio_file = '../bioASQ/bioASQ.json'\n",
    "\n",
    "def make_and_save_full_dataset(train, valid, test, path):\n",
    "    full_data = datasets.dataset_dict.DatasetDict({'train':train, 'validation':valid, 'test': test})\n",
    "    full_data.save_to_disk(path)\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return datasets.load_dataset('custom_squad.py', data_files= {'train':filename})['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import run_qa\n",
    "\n",
    "def run_gradual_ft(output_dir, checkpoint, covid_val):\n",
    "    !python run_qa.py \\\n",
    "      --model_name_or_path {checkpoint} \\\n",
    "      --dataset_name ../data/full_squad_covidQA/ \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --do_predict \\\n",
    "      --per_device_train_batch_size 32\\\n",
    "      --per_device_eval_batch_size 32\\\n",
    "      --evaluation_strategy \"no\" \\\n",
    "      --save_strategy \"no\" \\\n",
    "      --logging_strategy \"epoch\" \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --max_seq_length 384 \\\n",
    "      --doc_stride 128 \\\n",
    "      --output_dir {output_dir} \\\n",
    "      --overwrite_output_dir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a396986d6b3a2375\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "Using custom data configuration default-8fdbe041288a2f4d\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-8fdbe041288a2f4d\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n"
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = covid_file\n",
    "\n",
    "covid_qa = get_dataset(covid_file)\n",
    "bio_qa = get_dataset(bio_file)\n",
    "\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "covid_and_squad_dataset_path = \"../data/full_squad_covidQA\"\n",
    "\n",
    "squad_qa = datasets.Dataset.from_dict(squad_qa[:50])\n",
    "covid_qa = datasets.Dataset.from_dict(covid_qa[:20])\n",
    "bio_qa = datasets.Dataset.from_dict(bio_qa[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:47:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:47:56 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\runs\\Jul01_20-47-56_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:48:02 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-5ed7e8d24c6522e4.arrow\n",
      "07/01/2021 20:48:02 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-77c112c6ad537089.arrow\n",
      "{'loss': 5.3002, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.9541, 'train_samples_per_second': 59.612, 'train_steps_per_second': 1.917, 'train_loss': 5.300233386811756, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3002\n",
      "  train_runtime            = 0:00:10.95\n",
      "  train_samples            =        653\n",
      "  train_samples_per_second =     59.612\n",
      "  train_steps_per_second   =      1.917\n",
      "07/01/2021 20:48:17 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:48:17 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:48:18 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\eval_predictions.json.\n",
      "07/01/2021 20:48:18 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:48:18 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:48:18 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "07/01/2021 20:48:18 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\predict_predictions.json.\n",
      "07/01/2021 20:48:18 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-07-01 20:47:57,884 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:47:57,884 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-01 20:47:58,209 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 20:47:58,539 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:47:58,540 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:48:00,510 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:48:00,510 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:48:00,511 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:48:00,511 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:48:00,511 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:48:00,511 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-07-01 20:48:00,937 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-07-01 20:48:01,742 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-07-01 20:48:01,742 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.13ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.13ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:48:05,684 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:48:05,684 >>   Num examples = 653\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:48:05,684 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:48:05,684 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:48:05,684 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:48:05,684 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:48:05,684 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:02<00:49,  2.48s/it]\n",
      " 10%|9         | 2/21 [00:02<00:35,  1.86s/it]\n",
      " 14%|#4        | 3/21 [00:03<00:25,  1.43s/it]\n",
      " 19%|#9        | 4/21 [00:03<00:19,  1.13s/it]\n",
      " 24%|##3       | 5/21 [00:04<00:14,  1.08it/s]\n",
      " 29%|##8       | 6/21 [00:04<00:11,  1.28it/s]\n",
      " 33%|###3      | 7/21 [00:05<00:09,  1.47it/s]\n",
      " 38%|###8      | 8/21 [00:05<00:07,  1.66it/s]\n",
      " 43%|####2     | 9/21 [00:05<00:06,  1.81it/s]\n",
      " 48%|####7     | 10/21 [00:06<00:05,  1.94it/s]\n",
      " 52%|#####2    | 11/21 [00:06<00:04,  2.04it/s]\n",
      " 57%|#####7    | 12/21 [00:07<00:04,  2.12it/s]\n",
      " 62%|######1   | 13/21 [00:07<00:03,  2.17it/s]\n",
      " 67%|######6   | 14/21 [00:08<00:03,  2.21it/s]\n",
      " 71%|#######1  | 15/21 [00:08<00:02,  2.22it/s]\n",
      " 76%|#######6  | 16/21 [00:08<00:02,  2.24it/s]\n",
      " 81%|########  | 17/21 [00:09<00:01,  2.25it/s]\n",
      " 86%|########5 | 18/21 [00:09<00:01,  2.26it/s]\n",
      " 90%|######### | 19/21 [00:10<00:00,  2.28it/s]\n",
      " 95%|#########5| 20/21 [00:10<00:00,  2.28it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  2.70it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.70it/s][INFO|trainer.py:1349] 2021-07-01 20:48:16,637 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.70it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  1.92it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:48:16,640 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:48:16,641 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:48:17,293 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:48:17,293 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:48:17,294 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:48:17,397 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:48:17,399 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:48:17,399 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:48:17,399 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.51it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.65it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:48:18,096 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:48:18,097 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:48:18,097 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:48:18,098 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 14.49it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 12.05it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:48:21 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:48:21 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\runs\\Jul01_20-48-21_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 86\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 86\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:48:23 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-27ea423c36dafc21.arrow\n",
      "07/01/2021 20:48:23 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-8639512de9a05da5.arrow\n",
      "{'loss': 3.2672, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.0587, 'train_samples_per_second': 67.902, 'train_steps_per_second': 2.187, 'train_loss': 3.267227519642223, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2672\n",
      "  train_runtime            = 0:00:10.05\n",
      "  train_samples            =        683\n",
      "  train_samples_per_second =     67.902\n",
      "  train_steps_per_second   =      2.187\n",
      "07/01/2021 20:48:35 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:48:36 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:48:36 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\eval_predictions.json.\n",
      "07/01/2021 20:48:36 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:48:36 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:48:36 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "07/01/2021 20:48:37 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\predict_predictions.json.\n",
      "07/01/2021 20:48:37 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:48:22,049 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:48:22,050 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:48:22,062 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:22,063 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:22,063 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:22,063 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:22,063 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:22,063 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:22,063 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:48:22,145 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:48:22,875 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:48:22,875 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.03ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.03ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:48:25,196 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:48:25,196 >>   Num examples = 683\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:48:25,196 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:48:25,196 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:48:25,196 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:48:25,196 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:48:25,196 >>   Total optimization steps = 22\n",
      "\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/22 [00:01<00:28,  1.36s/it]\n",
      "  9%|9         | 2/22 [00:01<00:21,  1.08s/it]\n",
      " 14%|#3        | 3/22 [00:02<00:16,  1.13it/s]\n",
      " 18%|#8        | 4/22 [00:02<00:13,  1.34it/s]\n",
      " 23%|##2       | 5/22 [00:03<00:11,  1.54it/s]\n",
      " 27%|##7       | 6/22 [00:03<00:09,  1.72it/s]\n",
      " 32%|###1      | 7/22 [00:03<00:08,  1.87it/s]\n",
      " 36%|###6      | 8/22 [00:04<00:07,  1.99it/s]\n",
      " 41%|####      | 9/22 [00:04<00:06,  2.09it/s]\n",
      " 45%|####5     | 10/22 [00:05<00:05,  2.16it/s]\n",
      " 50%|#####     | 11/22 [00:05<00:04,  2.21it/s]\n",
      " 55%|#####4    | 12/22 [00:06<00:04,  2.25it/s]\n",
      " 59%|#####9    | 13/22 [00:06<00:03,  2.28it/s]\n",
      " 64%|######3   | 14/22 [00:06<00:03,  2.30it/s]\n",
      " 68%|######8   | 15/22 [00:07<00:03,  2.31it/s]\n",
      " 73%|#######2  | 16/22 [00:07<00:02,  2.32it/s]\n",
      " 77%|#######7  | 17/22 [00:08<00:02,  2.33it/s]\n",
      " 82%|########1 | 18/22 [00:08<00:01,  2.34it/s]\n",
      " 86%|########6 | 19/22 [00:09<00:01,  2.34it/s]\n",
      " 91%|######### | 20/22 [00:09<00:00,  2.35it/s]\n",
      " 95%|#########5| 21/22 [00:09<00:00,  2.35it/s]\n",
      "100%|##########| 22/22 [00:10<00:00,  2.83it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 22/22 [00:10<00:00,  2.83it/s][INFO|trainer.py:1349] 2021-07-01 20:48:35,255 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 22/22 [00:10<00:00,  2.83it/s]\n",
      "100%|##########| 22/22 [00:10<00:00,  2.19it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:48:35,256 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:48:35,257 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:48:35,848 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:48:35,849 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:48:35,850 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:48:35,941 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:48:35,943 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:48:35,943 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:48:35,943 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.94it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:48:36,605 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:48:36,607 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:48:36,607 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:48:36,607 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.98it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.90it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:48:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:48:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\runs\\Jul01_20-48-39_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:48:41 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-2b2419cfc1f07237.arrow\n",
      "07/01/2021 20:48:41 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-69dc1a41a42eb1c1.arrow\n",
      "{'loss': 1.8617, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.5592, 'train_samples_per_second': 65.591, 'train_steps_per_second': 2.092, 'train_loss': 1.8617238998413086, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8617\n",
      "  train_runtime            = 0:00:09.55\n",
      "  train_samples            =        627\n",
      "  train_samples_per_second =     65.591\n",
      "  train_steps_per_second   =      2.092\n",
      "07/01/2021 20:48:53 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:48:54 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:48:54 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\eval_predictions.json.\n",
      "07/01/2021 20:48:54 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:48:54 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:48:54 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "07/01/2021 20:48:55 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\predict_predictions.json.\n",
      "07/01/2021 20:48:55 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:48:40,497 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:48:40,498 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:48:40,511 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:40,512 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:40,512 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:40,512 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:40,512 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:40,512 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:40,512 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:48:40,587 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:48:41,312 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:48:41,313 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:48:43,700 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:48:43,700 >>   Num examples = 627\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:48:43,700 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:48:43,700 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:48:43,701 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:48:43,701 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:48:43,701 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:28,  1.48s/it]\n",
      " 10%|#         | 2/20 [00:01<00:21,  1.17s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:16,  1.06it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.26it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.46it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.65it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.79it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.92it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.02it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.10it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.15it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.19it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.23it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.26it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.27it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.30it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.30it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.31it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.31it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.58it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.58it/s][INFO|trainer.py:1349] 2021-07-01 20:48:53,259 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.58it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.09it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:48:53,261 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:48:53,262 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:48:53,844 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:48:53,845 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:48:53,845 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:48:53,937 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:48:53,938 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:48:53,939 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:48:53,939 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.51it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.79it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:48:54,619 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:48:54,621 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:48:54,621 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:48:54,621 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 14.39it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.76it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:48:58 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:48:58 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\runs\\Jul01_20-48-57_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:48:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-decb865b27cb1a0f.arrow\n",
      "07/01/2021 20:48:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-f1201cd02b28fe45.arrow\n",
      "07/01/2021 20:48:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-d259ca3ae54bb9c5.arrow\n",
      "{'loss': 1.1591, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.2723, 'train_samples_per_second': 66.111, 'train_steps_per_second': 2.157, 'train_loss': 1.159121036529541, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.1591\n",
      "  train_runtime            = 0:00:09.27\n",
      "  train_samples            =        613\n",
      "  train_samples_per_second =     66.111\n",
      "  train_steps_per_second   =      2.157\n",
      "07/01/2021 20:49:11 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:49:11 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:49:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\eval_predictions.json.\n",
      "07/01/2021 20:49:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:49:12 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:49:12 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "07/01/2021 20:49:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\predict_predictions.json.\n",
      "07/01/2021 20:49:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:48:58,516 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:48:58,516 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:48:58,528 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:58,528 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:58,529 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:58,529 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:58,529 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:58,529 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:48:58,529 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:48:58,602 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:48:59,323 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:48:59,323 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:49:01,388 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:49:01,388 >>   Num examples = 613\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:49:01,388 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:49:01,388 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:49:01,388 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:49:01,388 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:49:01,388 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:26,  1.39s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.11s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.11it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:09,  1.50it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.69it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:07,  1.84it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.97it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.06it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.13it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.18it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.22it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.25it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.28it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.28it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.30it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.31it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.32it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.31it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.97it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.97it/s][INFO|trainer.py:1349] 2021-07-01 20:49:10,660 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.97it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.16it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:49:10,662 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:49:10,663 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:49:11,272 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:49:11,273 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:49:11,273 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:49:11,364 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:49:11,366 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:49:11,366 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:49:11,366 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.86it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:49:12,038 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:49:12,040 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:49:12,040 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:49:12,040 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.98it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.98it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:49:15 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:49:15 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\runs\\Jul01_20-49-15_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.5905, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.1965, 'train_samples_per_second': 66.656, 'train_steps_per_second': 2.175, 'train_loss': 0.590511417388916, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.5905\n",
      "  train_runtime            = 0:00:09.19\n",
      "  train_samples            =        613\n",
      "  train_samples_per_second =     66.656\n",
      "  train_steps_per_second   =      2.175\n",
      "07/01/2021 20:49:29 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:49:30 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:49:30 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\eval_predictions.json.\n",
      "07/01/2021 20:49:30 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:49:30 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:49:30 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "07/01/2021 20:49:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\predict_predictions.json.\n",
      "07/01/2021 20:49:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:49:16,276 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:49:16,276 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:49:16,289 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:16,290 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:16,290 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:16,290 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:16,290 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:16,290 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:16,290 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:49:16,364 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:49:17,172 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:49:17,172 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.98ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.97ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.55ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.47ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:49:20,028 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:49:20,028 >>   Num examples = 613\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:49:20,028 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:49:20,028 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:49:20,028 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:49:20,028 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:49:20,028 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:26,  1.37s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.09s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.12it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.33it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:09,  1.53it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.70it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:07,  1.86it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.98it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.08it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.14it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.20it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.23it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.25it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.27it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.29it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.31it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.32it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.33it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.34it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  3.01it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  3.01it/s][INFO|trainer.py:1349] 2021-07-01 20:49:29,224 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  3.01it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.18it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:49:29,226 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:49:29,227 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:49:29,810 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:49:29,811 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:49:29,811 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:49:29,904 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:49:29,906 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:49:29,906 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:49:29,906 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.88it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:49:30,576 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:49:30,578 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:49:30,578 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:49:30,578 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 14.70it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.98it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.91it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:49:33 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:49:33 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\runs\\Jul01_20-49-33_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_5,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.428, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.1659, 'train_samples_per_second': 66.878, 'train_steps_per_second': 2.182, 'train_loss': 0.4280045509338379, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      0.428\n",
      "  train_runtime            = 0:00:09.16\n",
      "  train_samples            =        613\n",
      "  train_samples_per_second =     66.878\n",
      "  train_steps_per_second   =      2.182\n",
      "07/01/2021 20:49:47 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:49:48 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:49:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\eval_predictions.json.\n",
      "07/01/2021 20:49:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:49:48 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:49:48 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "07/01/2021 20:49:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\predict_predictions.json.\n",
      "07/01/2021 20:49:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:49:34,476 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:49:34,476 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:49:34,489 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:34,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:34,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:34,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:34,490 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:34,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:49:34,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:49:34,563 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:49:35,288 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:49:35,289 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.39ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.39ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.62ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.62ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:49:37,799 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:49:37,799 >>   Num examples = 613\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:49:37,799 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:49:37,799 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:49:37,799 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:49:37,799 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:49:37,799 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.35s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.08s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.13it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.34it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:09,  1.54it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.72it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.87it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.99it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.08it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.16it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.21it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.24it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.26it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.29it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.30it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.31it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.32it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.32it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.32it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  3.00it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  3.00it/s][INFO|trainer.py:1349] 2021-07-01 20:49:46,964 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  3.00it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.18it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:49:46,966 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:49:46,967 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:49:47,567 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:49:47,568 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:49:47,569 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:49:47,657 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:49:47,659 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:49:47,659 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:49:47,659 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.70it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      " 50%|#####     | 1/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.35it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.01it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.57it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:49:48,360 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:49:48,362 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:49:48,362 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:49:48,362 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 14.49it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      " 50%|#####     | 1/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.58it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:49:51 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:49:51 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\runs\\Jul01_20-49-51_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:49:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-bfb5ef26fcd00904.arrow\n",
      "07/01/2021 20:49:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-01e80e3a8a34f17a.arrow\n",
      "{'loss': 5.2763, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.5263, 'train_samples_per_second': 67.288, 'train_steps_per_second': 2.204, 'train_loss': 5.276311965215774, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2763\n",
      "  train_runtime            = 0:00:09.52\n",
      "  train_samples            =        641\n",
      "  train_samples_per_second =     67.288\n",
      "  train_steps_per_second   =      2.204\n",
      "07/01/2021 20:50:09 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:50:09 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:50:10 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\eval_predictions.json.\n",
      "07/01/2021 20:50:10 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:50:10 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:50:10 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "07/01/2021 20:50:10 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\predict_predictions.json.\n",
      "07/01/2021 20:50:10 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-07-01 20:49:52,748 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:49:52,748 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-01 20:49:53,068 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 20:49:53,403 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:49:53,403 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:49:55,696 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:49:55,696 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:49:55,696 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:49:55,696 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:49:55,696 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:49:55,696 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-07-01 20:49:56,085 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-07-01 20:49:56,808 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-07-01 20:49:56,808 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.27ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.27ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:49:59,157 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:49:59,157 >>   Num examples = 641\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:49:59,157 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:49:59,157 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:49:59,157 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:49:59,157 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:49:59,157 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:26,  1.34s/it]\n",
      " 10%|9         | 2/21 [00:01<00:20,  1.07s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.14it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:12,  1.35it/s]\n",
      " 24%|##3       | 5/21 [00:03<00:10,  1.55it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.72it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.87it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  1.99it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.09it/s]\n",
      " 48%|####7     | 10/21 [00:05<00:05,  2.16it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.21it/s]\n",
      " 57%|#####7    | 12/21 [00:06<00:04,  2.25it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.28it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:03,  2.30it/s]\n",
      " 71%|#######1  | 15/21 [00:07<00:02,  2.31it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.32it/s]\n",
      " 81%|########  | 17/21 [00:08<00:01,  2.33it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.33it/s]\n",
      " 90%|######### | 19/21 [00:09<00:00,  2.34it/s]\n",
      " 95%|#########5| 20/21 [00:09<00:00,  2.34it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.34it/s][INFO|trainer.py:1349] 2021-07-01 20:50:08,682 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.34it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.21it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:50:08,684 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:50:08,685 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:50:09,273 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:50:09,273 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:50:09,274 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:50:09,368 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:50:09,370 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:50:09,370 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:50:09,370 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.71it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:50:10,058 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:50:10,060 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:50:10,060 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:50:10,060 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:50:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:50:13 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\runs\\Jul01_20-50-13_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 86\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 86\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:50:15 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-802baaf5fce410a8.arrow\n",
      "07/01/2021 20:50:15 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-4ff30e05eb1ebaa3.arrow\n",
      "{'loss': 3.3149, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.9201, 'train_samples_per_second': 67.641, 'train_steps_per_second': 2.117, 'train_loss': 3.3149290538969494, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.3149\n",
      "  train_runtime            = 0:00:09.92\n",
      "  train_samples            =        671\n",
      "  train_samples_per_second =     67.641\n",
      "  train_steps_per_second   =      2.117\n",
      "07/01/2021 20:50:27 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:50:28 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:50:28 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\eval_predictions.json.\n",
      "07/01/2021 20:50:28 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:50:28 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:50:29 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "07/01/2021 20:50:29 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\predict_predictions.json.\n",
      "07/01/2021 20:50:29 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:50:14,068 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:50:14,069 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:50:14,072 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:14,072 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:14,072 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:14,072 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:14,072 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:14,072 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:14,072 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:50:14,147 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:50:14,870 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:50:14,870 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.24ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.24ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:50:17,297 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:50:17,297 >>   Num examples = 671\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:50:17,297 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:50:17,297 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:50:17,297 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:50:17,297 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:50:17,297 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:27,  1.37s/it]\n",
      " 10%|9         | 2/21 [00:01<00:20,  1.09s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.13it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:12,  1.33it/s]\n",
      " 24%|##3       | 5/21 [00:03<00:10,  1.53it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.71it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.86it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  1.98it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.08it/s]\n",
      " 48%|####7     | 10/21 [00:05<00:05,  2.15it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.20it/s]\n",
      " 57%|#####7    | 12/21 [00:06<00:04,  2.24it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.27it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:03,  2.28it/s]\n",
      " 71%|#######1  | 15/21 [00:07<00:02,  2.30it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.31it/s]\n",
      " 81%|########  | 17/21 [00:08<00:01,  2.32it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.33it/s]\n",
      " 90%|######### | 19/21 [00:09<00:00,  2.32it/s]\n",
      " 95%|#########5| 20/21 [00:09<00:00,  2.32it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.36it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.36it/s][INFO|trainer.py:1349] 2021-07-01 20:50:27,217 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.36it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.12it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:50:27,219 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:50:27,220 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:50:27,797 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:50:27,798 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:50:27,799 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:50:27,890 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:50:27,892 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:50:27,892 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:50:27,892 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.85it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:50:28,566 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:50:28,568 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:50:28,568 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:50:28,568 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:50:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:50:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\runs\\Jul01_20-50-32_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:50:33 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-bc5dcf634e32134c.arrow\n",
      "07/01/2021 20:50:33 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-19f35eaca0feafa8.arrow\n",
      "{'loss': 1.8597, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.1773, 'train_samples_per_second': 67.013, 'train_steps_per_second': 2.179, 'train_loss': 1.8597063064575194, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8597\n",
      "  train_runtime            = 0:00:09.17\n",
      "  train_samples            =        615\n",
      "  train_samples_per_second =     67.013\n",
      "  train_steps_per_second   =      2.179\n",
      "07/01/2021 20:50:45 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:50:46 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:50:46 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\eval_predictions.json.\n",
      "07/01/2021 20:50:46 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:50:46 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:50:46 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "07/01/2021 20:50:47 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\predict_predictions.json.\n",
      "07/01/2021 20:50:47 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:50:32,564 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:50:32,564 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:50:32,576 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:32,577 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:32,577 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:32,577 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:32,577 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:32,577 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:32,577 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:50:32,651 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:50:33,376 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:50:33,376 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:50:35,981 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:50:35,981 >>   Num examples = 615\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:50:35,981 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:50:35,981 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:50:35,981 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:50:35,982 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:50:35,982 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.34s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.06s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.15it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.35it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:09,  1.55it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.72it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.87it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.99it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.08it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.15it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.21it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.24it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.27it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.29it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.30it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.31it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.32it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.33it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.33it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.93it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.93it/s][INFO|trainer.py:1349] 2021-07-01 20:50:45,159 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.93it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.18it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:50:45,161 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:50:45,162 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:50:45,751 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:50:45,752 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:50:45,753 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:50:45,845 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:50:45,847 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:50:45,847 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:50:45,847 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.85it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:50:46,520 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:50:46,522 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:50:46,522 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:50:46,522 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.61it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:50:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:50:50 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\runs\\Jul01_20-50-50_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:50:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-b346cdf772aafbcc.arrow\n",
      "07/01/2021 20:50:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-ebe55fbb786a6e06.arrow\n",
      "07/01/2021 20:50:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-a22966f7b8cf3c62.arrow\n",
      "{'loss': 1.007, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.0029, 'train_samples_per_second': 66.757, 'train_steps_per_second': 2.11, 'train_loss': 1.0070091046785052, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      1.007\n",
      "  train_runtime            = 0:00:09.00\n",
      "  train_samples            =        601\n",
      "  train_samples_per_second =     66.757\n",
      "  train_steps_per_second   =       2.11\n",
      "07/01/2021 20:51:03 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:51:03 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:51:03 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\eval_predictions.json.\n",
      "07/01/2021 20:51:03 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:51:03 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:51:04 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "07/01/2021 20:51:04 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\predict_predictions.json.\n",
      "07/01/2021 20:51:04 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:50:50,547 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:50:50,548 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:50:50,560 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:50,560 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:50,560 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:50,560 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:50,560 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:50,560 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:50:50,560 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:50:50,633 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:50:51,357 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:50:51,357 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:50:53,427 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:50:53,427 >>   Num examples = 601\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:50:53,427 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:50:53,427 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:50:53,427 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:50:53,427 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:50:53,427 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:24,  1.36s/it]\n",
      " 11%|#         | 2/19 [00:01<00:18,  1.08s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:14,  1.13it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:11,  1.34it/s]\n",
      " 26%|##6       | 5/19 [00:03<00:09,  1.54it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.71it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.86it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  1.98it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.07it/s]\n",
      " 53%|#####2    | 10/19 [00:05<00:04,  2.14it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.20it/s]\n",
      " 63%|######3   | 12/19 [00:06<00:03,  2.24it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.26it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.28it/s]\n",
      " 79%|#######8  | 15/19 [00:07<00:01,  2.30it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.31it/s]\n",
      " 89%|########9 | 17/19 [00:08<00:00,  2.31it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.31it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.46it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.46it/s][INFO|trainer.py:1349] 2021-07-01 20:51:02,430 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.46it/s]\n",
      "100%|##########| 19/19 [00:09<00:00,  2.11it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:51:02,432 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:51:02,433 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:51:03,029 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:51:03,029 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:51:03,030 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:51:03,123 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:51:03,125 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:51:03,125 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:51:03,126 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.51it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.88it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:51:03,795 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:51:03,797 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:51:03,797 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:51:03,797 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.61it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:51:07 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:51:07 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\runs\\Jul01_20-51-07_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.5513, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.0091, 'train_samples_per_second': 66.71, 'train_steps_per_second': 2.109, 'train_loss': 0.5513049175864772, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.5513\n",
      "  train_runtime            = 0:00:09.00\n",
      "  train_samples            =        601\n",
      "  train_samples_per_second =      66.71\n",
      "  train_steps_per_second   =      2.109\n",
      "07/01/2021 20:51:20 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:51:21 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:51:21 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\eval_predictions.json.\n",
      "07/01/2021 20:51:21 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:51:21 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:51:21 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "07/01/2021 20:51:22 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\predict_predictions.json.\n",
      "07/01/2021 20:51:22 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:51:07,785 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:51:07,785 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:51:07,798 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:07,798 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:07,798 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:07,798 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:07,798 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:07,798 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:07,798 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:51:07,872 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:51:08,586 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:51:08,586 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.41ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.35ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:51:11,119 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:51:11,119 >>   Num examples = 601\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:51:11,119 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:51:11,119 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:51:11,119 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:51:11,119 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:51:11,119 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:24,  1.36s/it]\n",
      " 11%|#         | 2/19 [00:01<00:18,  1.08s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:14,  1.13it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:11,  1.34it/s]\n",
      " 26%|##6       | 5/19 [00:03<00:09,  1.54it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.71it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.86it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  1.98it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.07it/s]\n",
      " 53%|#####2    | 10/19 [00:05<00:04,  2.14it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.19it/s]\n",
      " 63%|######3   | 12/19 [00:06<00:03,  2.23it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.27it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.28it/s]\n",
      " 79%|#######8  | 15/19 [00:07<00:01,  2.30it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.30it/s]\n",
      " 89%|########9 | 17/19 [00:08<00:00,  2.31it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.31it/s]\n",
      "100%|##########| 19/19 [00:09<00:00,  2.46it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:09<00:00,  2.46it/s][INFO|trainer.py:1349] 2021-07-01 20:51:20,127 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:09<00:00,  2.46it/s]\n",
      "100%|##########| 19/19 [00:09<00:00,  2.11it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:51:20,129 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:51:20,130 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:51:20,724 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:51:20,725 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:51:20,725 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:51:20,819 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:51:20,821 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:51:20,821 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:51:20,821 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.88it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:51:21,490 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:51:21,492 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:51:21,492 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:51:21,493 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:51:24 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:51:24 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\runs\\Jul01_20-51-24_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_5,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.4142, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.9828, 'train_samples_per_second': 66.906, 'train_steps_per_second': 2.115, 'train_loss': 0.4142366459495143, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.4142\n",
      "  train_runtime            = 0:00:08.98\n",
      "  train_samples            =        601\n",
      "  train_samples_per_second =     66.906\n",
      "  train_steps_per_second   =      2.115\n",
      "07/01/2021 20:51:38 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:51:38 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "07/01/2021 20:51:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\eval_predictions.json.\n",
      "07/01/2021 20:51:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "07/01/2021 20:51:39 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:51:39 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "07/01/2021 20:51:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\predict_predictions.json.\n",
      "07/01/2021 20:51:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:51:25,474 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:51:25,475 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:51:25,488 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:25,488 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:25,488 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:25,488 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:25,489 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:25,489 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:51:25,489 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:51:25,562 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:51:26,273 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:51:26,273 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.31ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.31ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:51:28,805 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:51:28,805 >>   Num examples = 601\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:51:28,805 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:51:28,805 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:51:28,805 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:51:28,805 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:51:28,805 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:24,  1.33s/it]\n",
      " 11%|#         | 2/19 [00:01<00:18,  1.06s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.14it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:11,  1.35it/s]\n",
      " 26%|##6       | 5/19 [00:03<00:09,  1.54it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.72it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.86it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  1.99it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.08it/s]\n",
      " 53%|#####2    | 10/19 [00:05<00:04,  2.15it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.20it/s]\n",
      " 63%|######3   | 12/19 [00:06<00:03,  2.24it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.26it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.29it/s]\n",
      " 79%|#######8  | 15/19 [00:07<00:01,  2.30it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.31it/s]\n",
      " 89%|########9 | 17/19 [00:08<00:00,  2.31it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.32it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.47it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.47it/s][INFO|trainer.py:1349] 2021-07-01 20:51:37,787 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.12it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:51:37,789 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:51:37,790 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:51:38,388 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:51:38,390 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:51:38,390 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:51:38,472 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:51:38,473 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:51:38,474 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:51:38,474 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.81it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:51:39,150 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:51:39,152 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:51:39,152 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:51:39,153 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.79it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:51:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:51:42 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\runs\\Jul01_20-51-42_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:51:47 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-270ae70ff79e9a51.arrow\n",
      "07/01/2021 20:51:48 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-08ea23aa93ac187f.arrow\n",
      "{'loss': 5.3184, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.4618, 'train_samples_per_second': 67.535, 'train_steps_per_second': 2.114, 'train_loss': 5.318376922607422, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3184\n",
      "  train_runtime            = 0:00:09.46\n",
      "  train_samples            =        639\n",
      "  train_samples_per_second =     67.535\n",
      "  train_steps_per_second   =      2.114\n",
      "07/01/2021 20:52:00 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:52:00 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "07/01/2021 20:52:00 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\eval_predictions.json.\n",
      "07/01/2021 20:52:00 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "07/01/2021 20:52:01 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:52:01 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "07/01/2021 20:52:01 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\predict_predictions.json.\n",
      "07/01/2021 20:52:01 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-07-01 20:51:43,643 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:51:43,643 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-01 20:51:43,969 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-07-01 20:51:44,293 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:51:44,294 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:51:46,549 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:51:46,549 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:51:46,549 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:51:46,549 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:51:46,549 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-01 20:51:46,549 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-07-01 20:51:46,934 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-07-01 20:51:47,639 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-07-01 20:51:47,639 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.26ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:51:50,204 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:51:50,205 >>   Num examples = 639\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:51:50,205 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:51:50,205 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:51:50,205 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:51:50,205 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:51:50,205 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.34s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.14it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.35it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:09,  1.54it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.72it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.87it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.99it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.08it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.15it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.21it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.24it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.27it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.29it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.30it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.31it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.32it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.32it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.32it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.36it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.36it/s][INFO|trainer.py:1349] 2021-07-01 20:51:59,667 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.36it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.11it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:51:59,669 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:51:59,670 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:52:00,258 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:52:00,259 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:52:00,259 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:52:00,350 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:52:00,352 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:52:00,352 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:52:00,352 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.80it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:52:01,031 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:52:01,033 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:52:01,033 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:52:01,033 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2021 20:52:04 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/01/2021 20:52:04 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\runs\\Jul01_20-52-04_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 86\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 86\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "07/01/2021 20:52:06 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-af2e9b2946b89313.arrow\n",
      "07/01/2021 20:52:06 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-587a05b1b392ee63.arrow\n",
      "{'loss': 3.3462, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.0545, 'train_samples_per_second': 66.537, 'train_steps_per_second': 2.089, 'train_loss': 3.346188499813988, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.3462\n",
      "  train_runtime            = 0:00:10.05\n",
      "  train_samples            =        669\n",
      "  train_samples_per_second =     66.537\n",
      "  train_steps_per_second   =      2.089\n",
      "07/01/2021 20:52:18 - INFO - __main__ -   *** Evaluate ***\n",
      "07/01/2021 20:52:19 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "07/01/2021 20:52:19 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\eval_predictions.json.\n",
      "07/01/2021 20:52:19 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "07/01/2021 20:52:19 - INFO - __main__ -   *** Predict ***\n",
      "07/01/2021 20:52:20 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "07/01/2021 20:52:20 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\predict_predictions.json.\n",
      "07/01/2021 20:52:20 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-01 20:52:05,065 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-01 20:52:05,066 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-01 20:52:05,069 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:52:05,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:52:05,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:52:05,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:52:05,070 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:52:05,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-01 20:52:05,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-07-01 20:52:05,151 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-01 20:52:05,934 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-07-01 20:52:05,934 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.88ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.88ba/s]\n",
      "[INFO|trainer.py:1153] 2021-07-01 20:52:08,240 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-07-01 20:52:08,240 >>   Num examples = 669\n",
      "[INFO|trainer.py:1155] 2021-07-01 20:52:08,240 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-07-01 20:52:08,240 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-07-01 20:52:08,240 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-07-01 20:52:08,240 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-07-01 20:52:08,240 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:27,  1.38s/it]\n",
      " 10%|9         | 2/21 [00:01<00:20,  1.10s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:16,  1.12it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:12,  1.32it/s]\n",
      " 24%|##3       | 5/21 [00:03<00:10,  1.52it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.70it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.85it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  1.98it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.06it/s]\n",
      " 48%|####7     | 10/21 [00:05<00:05,  2.13it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.19it/s]\n",
      " 57%|#####7    | 12/21 [00:06<00:04,  2.23it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.26it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:03,  2.28it/s]\n",
      " 71%|#######1  | 15/21 [00:07<00:02,  2.28it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.29it/s]\n",
      " 81%|########  | 17/21 [00:08<00:01,  2.25it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.26it/s]\n",
      " 90%|######### | 19/21 [00:09<00:00,  2.25it/s]\n",
      " 95%|#########5| 20/21 [00:09<00:00,  2.21it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  2.29it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.29it/s][INFO|trainer.py:1349] 2021-07-01 20:52:18,294 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.29it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  2.09it/s]\n",
      "[INFO|trainer.py:1900] 2021-07-01 20:52:18,296 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-07-01 20:52:18,297 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-07-01 20:52:18,901 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-01 20:52:18,902 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-01 20:52:18,903 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-07-01 20:52:18,995 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:52:18,996 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:52:18,997 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:52:18,997 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.79it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  8.47it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  8.47it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.36it/s]\n",
      "[INFO|trainer.py:520] 2021-07-01 20:52:19,720 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-07-01 20:52:19,723 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-07-01 20:52:19,723 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-07-01 20:52:19,723 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  8.89it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  8.89it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  4.85it/s]\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "K = 6\n",
    "\n",
    "for i in range(k_fold):\n",
    "    covid_fold = covid_qa.shard(k_fold, i)\n",
    "\n",
    "    covid_test = covid_fold.shard(2, 0)\n",
    "    covid_val = covid_fold.shard(2, 1)\n",
    "    covid_train = concatenate_datasets([covid_qa.shard(k_fold, j) for j in range(k_fold) if j != i])\n",
    "\n",
    "    #make_and_save_full_dataset(covid_train, squad_qa, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "\n",
    "    checkpoint = 'roberta-base'\n",
    "    cur_dir = '../models/gradual_ft_baseline_lr1e-5_2/split_' + str(i)\n",
    "\n",
    "    squad_qa.shuffle()\n",
    "    bio_qa.shuffle()\n",
    "\n",
    "    num_of_shards = int(K/2)\n",
    "    squad_qa_shards = [squad_qa.shard(num_of_shards,i) for i in range(num_of_shards)]\n",
    "    bio_qa_shards = [bio_qa.shard(num_of_shards,i) for i in range(num_of_shards)]\n",
    "    for n in range(K):\n",
    "        output_dir = cur_dir + '/checkpoint_' + str(n)\n",
    "        if n < 1:\n",
    "            squad_qa_cur = concatenate_datasets(squad_qa_shards)\n",
    "            bio_qa_cur = concatenate_datasets(bio_qa_shards)\n",
    "            full_dataset = concatenate_datasets([squad_qa_cur, bio_qa_cur])\n",
    "        try:\n",
    "            if n%2:\n",
    "                squad_qa_cur = concatenate_datasets(squad_qa_shards)\n",
    "                full_dataset = concatenate_datasets([squad_qa_cur, bio_qa_cur,covid_train])\n",
    "                _ = squad_qa_shards.pop()\n",
    "            else:\n",
    "                bio_qa_cur = concatenate_datasets(bio_qa_shards)\n",
    "                full_dataset = concatenate_datasets([bio_qa_cur, bio_qa_cur,covid_train])\n",
    "            _ = bio_qa_shards.pop()\n",
    "        except:\n",
    "            full_dataset = covid_train\n",
    "\n",
    "        make_and_save_full_dataset(full_dataset, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "\n",
    "        run_gradual_ft(output_dir, checkpoint, covid_val)\n",
    "\n",
    "        checkpoint = output_dir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}