{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "bio_file = '../bioASQ/bioASQ.json'\n",
    "\n",
    "def get_data_from_json(filename):\n",
    "    jsonfile = open(covid_file, 'r')\n",
    "    data = jsonfile.read()\n",
    "    jsonfile.close()\n",
    "    return json.loads(data)\n",
    "\n",
    "covid_data = get_data_from_json(covid_file)\n",
    "bio_data = get_data_from_json(bio_file)\n",
    "\n",
    "#datasets.set_caching_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def make_and_save_full_dataset(covid=None, squad = None, bioASQ = None, path = '../data/squad_bioASQ_covidQA/'):\n",
    "    squad = datasets.Dataset.from_dict(squad_qa[:])\n",
    "    bioASQ = datasets.Dataset.from_dict(bioASQ[:])\n",
    "    if covid is not None:\n",
    "        full_data = datasets.dataset_dict.DatasetDict({'squad':squad, 'covid':covid,  'bio':bioASQ})\n",
    "    else:\n",
    "        full_data = datasets.dataset_dict.DatasetDict({'squad':squad,'bio':bioASQ})\n",
    "\n",
    "    full_data.save_to_disk(path)\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return datasets.load_dataset('custom_squad.py', data_files= {'train':filename})['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a396986d6b3a2375\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "Using custom data configuration default-8fdbe041288a2f4d\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-8fdbe041288a2f4d\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n"
     ]
    }
   ],
   "source": [
    "covid_qa = get_dataset(covid_file)\n",
    "bio_qa = get_dataset(bio_file)\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "\n",
    "covid_bio_squad_dataset_path = \"../data/squad_bioASQ_covidQA/\"\n",
    "\n",
    "#this is just for testing purposes, I am going to make both of these files very small only at max 3000 datasets\n",
    "# squad_qa = datasets.Dataset.from_dict(squad_qa[:50])\n",
    "# bio_qa = datasets.Dataset.from_dict(bio_qa[:20])\n",
    "# covid_qa = datasets.Dataset.from_dict(covid_qa[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_and_save_full_dataset(squad=squad_qa,bioASQ=bio_qa,path=covid_bio_squad_dataset_path)\n",
    "\n",
    "K = 4\n",
    "to_remove_per_step = int(squad_qa.num_rows / K)\n",
    "bio_remove_per_step = int(bio_qa.num_rows / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_gradual_ft(output_dir, checkpoint, k_fold):\n",
    "    !python run_qa.py \\\n",
    "      --model_name_or_path {checkpoint} \\\n",
    "      --dataset_name ../data/squad_bioASQ_covidQA/\\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --do_predict \\\n",
    "      --per_device_train_batch_size 32\\\n",
    "      --per_device_eval_batch_size 32\\\n",
    "      --evaluation_strategy \"no\" \\\n",
    "      --save_strategy \"no\" \\\n",
    "      --logging_strategy \"epoch\" \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --max_seq_length 384 \\\n",
    "      --doc_stride 128 \\\n",
    "      --k_fold_cross_valid {k_fold} \\\n",
    "      --output_dir {output_dir} \\\n",
    "      --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**************************************************\n",
      "==================================================\n",
      "          At Gradual Fine Tuning Step:  1\n",
      "**************************************************\n",
      "==================================================\n",
      "\n",
      "\n",
      "../data/squad_bioASQ_covidQA/\n",
      "../data/squad_bioASQ_covidQA/\n",
      "06/29/2021 12:18:34 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 12:18:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline/runs\\Jun29_12-18-34_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gradual_ft_baseline,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline/runs\\Jun29_12-18-34_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=gradual_ft_baseline,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline/,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    bio: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "             Using the default covid_qa values\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "06/29/2021 12:18:35 - WARNING - datasets.builder -   Using custom data configuration default-a396986d6b3a2375\n",
      "06/29/2021 12:18:35 - WARNING - datasets.builder -   Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "06/29/2021 12:18:39 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/bio\\cache-416b97b5c35b0f4f.arrow\n",
      "{'loss': 5.715, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 5.5002, 'train_samples_per_second': 59.816, 'train_steps_per_second': 2.0, 'train_loss': 5.7149810791015625, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      5.715\n",
      "  train_runtime            = 0:00:05.50\n",
      "  train_samples            =        329\n",
      "  train_samples_per_second =     59.816\n",
      "  train_steps_per_second   =        2.0\n",
      "06/29/2021 12:18:48 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 12:18:48 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "06/29/2021 12:18:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-1\\eval_predictions.json.\n",
      "06/29/2021 12:18:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match =     0.0\n",
      "  eval_f1          = 13.3333\n",
      "  eval_samples     =      31\n",
      "06/29/2021 12:18:48 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 12:18:48 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "06/29/2021 12:18:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-1\\predict_predictions.json.\n",
      "06/29/2021 12:18:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =     32\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 9.5238\n",
      "***************************\n",
      "Split  2\n",
      "***************************\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "06/29/2021 12:18:53 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/bio\\cache-dbc0b48fc2cf93c6.arrow\n",
      "{'loss': 5.6748, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.5977, 'train_samples_per_second': 70.687, 'train_steps_per_second': 2.392, 'train_loss': 5.67484560879794, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.6748\n",
      "  train_runtime            = 0:00:04.59\n",
      "  train_samples            =        325\n",
      "  train_samples_per_second =     70.687\n",
      "  train_steps_per_second   =      2.392\n",
      "06/29/2021 12:18:59 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 12:18:59 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "06/29/2021 12:18:59 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-2\\eval_predictions.json.\n",
      "06/29/2021 12:18:59 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match =     0.0\n",
      "  eval_f1          = 26.6667\n",
      "  eval_samples     =      32\n",
      "06/29/2021 12:18:59 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 12:18:59 - INFO - utils_qa -   Post-processing 1 example predictions split into 35 features.\n",
      "06/29/2021 12:19:00 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-2\\predict_predictions.json.\n",
      "06/29/2021 12:19:00 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =     35\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 6.8966\n",
      "***************************\n",
      "Split  3\n",
      "***************************\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "06/29/2021 12:19:04 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/bio\\cache-4aa0df26a27bd28e.arrow\n",
      "{'loss': 5.6745, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.6767, 'train_samples_per_second': 70.349, 'train_steps_per_second': 2.352, 'train_loss': 5.6745473688299, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.6745\n",
      "  train_runtime            = 0:00:04.67\n",
      "  train_samples            =        329\n",
      "  train_samples_per_second =     70.349\n",
      "  train_steps_per_second   =      2.352\n",
      "06/29/2021 12:19:10 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 12:19:10 - INFO - utils_qa -   Post-processing 1 example predictions split into 31 features.\n",
      "06/29/2021 12:19:11 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-3\\eval_predictions.json.\n",
      "06/29/2021 12:19:11 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =  1.0\n",
      "  eval_exact_match =  0.0\n",
      "  eval_f1          = 6.25\n",
      "  eval_samples     =   31\n",
      "06/29/2021 12:19:11 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 12:19:11 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "06/29/2021 12:19:11 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-3\\predict_predictions.json.\n",
      "06/29/2021 12:19:11 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =     32\n",
      "  test_exact_match =    0.0\n",
      "  test_f1          = 9.0909\n",
      "***************************\n",
      "Split  4\n",
      "***************************\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "06/29/2021 12:19:15 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/bio\\cache-204fcc9b60883edd.arrow\n",
      "{'loss': 5.6811, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.665, 'train_samples_per_second': 70.311, 'train_steps_per_second': 2.358, 'train_loss': 5.681053161621094, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.6811\n",
      "  train_runtime            = 0:00:04.66\n",
      "  train_samples            =        328\n",
      "  train_samples_per_second =     70.311\n",
      "  train_steps_per_second   =      2.358\n",
      "06/29/2021 12:19:21 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 12:19:22 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "\n",
      "**************************************************\n",
      "\n",
      "==================================================06/29/2021 12:19:22 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-4\\eval_predictions.json.\n",
      "06/29/2021 12:19:22 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =    1.0\n",
      "\n",
      "  eval_exact_match =    0.0\n",
      "          At Gradual Fine Tuning Step:  2  eval_f1          = 5.2632\n",
      "  eval_samples     =     32\n",
      "06/29/2021 12:19:22 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 12:19:22 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "06/29/2021 12:19:22 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-4\\predict_predictions.json.\n",
      "06/29/2021 12:19:22 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =      32\n",
      "  test_exact_match =     0.0\n",
      "  test_f1          = 15.3846\n",
      "***************************\n",
      "Split  5\n",
      "***************************\n",
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "**************************************************\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-29 12:18:35,556 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:18:35,556 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 12:18:35,915 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:18:36,262 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:18:36,263 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:38,401 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:38,401 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:38,401 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:38,401 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:38,401 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:38,401 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 12:18:38,816 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 12:18:39,508 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 12:18:39,508 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 15.15ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 12.66ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 12:18:42,025 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 12:18:42,025 >>   Num examples = 329\n",
      "[INFO|trainer.py:1155] 2021-06-29 12:18:42,025 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 12:18:42,025 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 12:18:42,025 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 12:18:42,025 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 12:18:42,025 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:01<00:13,  1.31s/it]\n",
      " 18%|#8        | 2/11 [00:01<00:09,  1.05s/it]\n",
      " 27%|##7       | 3/11 [00:02<00:06,  1.15it/s]\n",
      " 36%|###6      | 4/11 [00:02<00:05,  1.34it/s]\n",
      " 45%|####5     | 5/11 [00:03<00:03,  1.53it/s]\n",
      " 55%|#####4    | 6/11 [00:03<00:02,  1.69it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:02,  1.82it/s]\n",
      " 73%|#######2  | 8/11 [00:04<00:01,  1.93it/s]\n",
      " 82%|########1 | 9/11 [00:04<00:00,  2.01it/s]\n",
      " 91%|######### | 10/11 [00:05<00:00,  2.08it/s]\n",
      "100%|##########| 11/11 [00:05<00:00,  2.60it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:05<00:00,  2.60it/s][INFO|trainer.py:1349] 2021-06-29 12:18:47,525 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:05<00:00,  2.60it/s]\n",
      "100%|##########| 11/11 [00:05<00:00,  2.00it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 12:18:47,527 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 12:18:47,527 >> Configuration saved in ../models/gradual_ft_baseline//Split-1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 12:18:48,119 >> Model weights saved in ../models/gradual_ft_baseline//Split-1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 12:18:48,119 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 12:18:48,120 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 12:18:48,204 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:18:48,206 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:18:48,206 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:18:48,206 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.63it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 12:18:48,491 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:18:48,493 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:18:48,493 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:18:48,493 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.05it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:18:49,192 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:18:49,192 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 12:18:49,539 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:18:49,903 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:18:49,903 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:52,066 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:52,067 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:52,067 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:52,067 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:52,067 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:18:52,067 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 12:18:52,476 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 12:18:53,177 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 12:18:53,178 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 14.29ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 18.52ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:05<00:00,  5.47s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 12:18:54,126 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 12:18:54,126 >>   Num examples = 325\n",
      "[INFO|trainer.py:1155] 2021-06-29 12:18:54,126 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 12:18:54,126 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 12:18:54,126 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 12:18:54,127 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 12:18:54,127 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.18it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:04,  2.20it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.22it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.22it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.22it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.23it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:01,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    num_rows: 1\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 1\n",
      "}), 'train': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 78\n",
      "})}\n",
      "06/29/2021 12:19:26 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/squad_bioASQ_covidQA/bio\\cache-186b11770ffd8ff7.arrow\n",
      "{'loss': 5.6681, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 4.6526, 'train_samples_per_second': 70.283, 'train_steps_per_second': 2.364, 'train_loss': 5.668061689897017, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.6681\n",
      "  train_runtime            = 0:00:04.65\n",
      "  train_samples            =        327\n",
      "  train_samples_per_second =     70.283\n",
      "  train_steps_per_second   =      2.364\n",
      "06/29/2021 12:19:33 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 12:19:33 - INFO - utils_qa -   Post-processing 1 example predictions split into 33 features.\n",
      "06/29/2021 12:19:33 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-5\\eval_predictions.json.\n",
      "06/29/2021 12:19:33 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-5\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match =     0.0\n",
      "  eval_f1          = 13.0435\n",
      "  eval_samples     =      33\n",
      "06/29/2021 12:19:33 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 12:19:34 - INFO - utils_qa -   Post-processing 1 example predictions split into 32 features.\n",
      "06/29/2021 12:19:34 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline//Split-5\\predict_predictions.json.\n",
      "06/29/2021 12:19:34 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline//Split-5\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =      32\n",
      "  test_exact_match =     0.0\n",
      "  test_f1          = 12.9032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|#######2  | 8/11 [00:03<00:01,  2.24it/s]\n",
      " 82%|########1 | 9/11 [00:04<00:00,  2.23it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.23it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.87it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.87it/s][INFO|trainer.py:1349] 2021-06-29 12:18:58,725 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.87it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.39it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 12:18:58,726 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 12:18:58,727 >> Configuration saved in ../models/gradual_ft_baseline//Split-2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 12:18:59,332 >> Model weights saved in ../models/gradual_ft_baseline//Split-2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 12:18:59,333 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 12:18:59,334 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 12:18:59,416 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:18:59,418 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:18:59,418 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:18:59,418 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.76it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 12:18:59,712 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:18:59,713 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:18:59,713 >>   Num examples = 35\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:18:59,714 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.24it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:19:00,410 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:19:00,411 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 12:19:00,772 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:19:01,128 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:19:01,129 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:03,249 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:03,249 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:03,249 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:03,249 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:03,250 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:03,250 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 12:19:03,660 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 12:19:04,359 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 12:19:04,359 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 10.64ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.61ba/s]\n",
      "\n",
      "100%|##########| 2/2 [00:05<00:00,  2.75s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 12:19:05,376 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 12:19:05,376 >>   Num examples = 329\n",
      "[INFO|trainer.py:1155] 2021-06-29 12:19:05,376 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 12:19:05,376 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 12:19:05,376 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 12:19:05,376 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 12:19:05,376 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.11it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:04,  2.15it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.18it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.19it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.21it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.21it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:01,  2.21it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.22it/s]\n",
      " 82%|########1 | 9/11 [00:04<00:00,  2.22it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.22it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.75it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.75it/s][INFO|trainer.py:1349] 2021-06-29 12:19:10,053 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.75it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.35it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 12:19:10,055 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 12:19:10,056 >> Configuration saved in ../models/gradual_ft_baseline//Split-3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 12:19:10,657 >> Model weights saved in ../models/gradual_ft_baseline//Split-3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 12:19:10,658 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 12:19:10,658 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 12:19:10,740 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:19:10,742 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:19:10,742 >>   Num examples = 31\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:19:10,742 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.19it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  7.81it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 12:19:11,034 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:19:11,036 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:19:11,036 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:19:11,036 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.63it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:19:11,701 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:19:11,701 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 12:19:12,049 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:19:12,399 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:19:12,399 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:14,511 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:14,511 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:14,511 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:14,511 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:14,511 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:14,511 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 12:19:14,940 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 12:19:15,649 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 12:19:15,650 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 14.29ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 19.61ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:05<00:00,  5.38s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 12:19:16,583 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 12:19:16,583 >>   Num examples = 328\n",
      "[INFO|trainer.py:1155] 2021-06-29 12:19:16,583 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 12:19:16,583 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 12:19:16,583 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 12:19:16,583 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 12:19:16,583 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.13it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:04,  2.15it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.17it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.19it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.19it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.20it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:01,  2.21it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.22it/s]\n",
      " 82%|########1 | 9/11 [00:04<00:00,  2.22it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.22it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.79it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.79it/s][INFO|trainer.py:1349] 2021-06-29 12:19:21,247 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.79it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.36it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 12:19:21,249 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 12:19:21,250 >> Configuration saved in ../models/gradual_ft_baseline//Split-4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 12:19:21,843 >> Model weights saved in ../models/gradual_ft_baseline//Split-4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 12:19:21,844 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 12:19:21,844 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 12:19:21,923 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:19:21,925 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:19:21,925 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:19:21,925 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  8.93it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00,  8.93it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.17it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 12:19:22,249 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:19:22,250 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:19:22,250 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:19:22,251 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 12.05it/s]\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:19:22,924 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:19:22,924 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 12:19:23,270 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 12:19:23,665 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 12:19:23,665 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:25,762 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:25,763 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:25,763 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:25,763 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:25,763 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 12:19:25,763 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 12:19:26,203 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 12:19:26,898 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 12:19:26,898 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 14.71ba/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 20.00ba/s]\n",
      "\n",
      "100%|##########| 1/1 [00:05<00:00,  5.69s/it]\n",
      "[INFO|trainer.py:1153] 2021-06-29 12:19:28,112 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 12:19:28,112 >>   Num examples = 327\n",
      "[INFO|trainer.py:1155] 2021-06-29 12:19:28,112 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 12:19:28,112 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 12:19:28,112 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 12:19:28,112 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 12:19:28,112 >>   Total optimization steps = 11\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  9%|9         | 1/11 [00:00<00:04,  2.10it/s]\n",
      " 18%|#8        | 2/11 [00:00<00:04,  2.14it/s]\n",
      " 27%|##7       | 3/11 [00:01<00:03,  2.17it/s]\n",
      " 36%|###6      | 4/11 [00:01<00:03,  2.19it/s]\n",
      " 45%|####5     | 5/11 [00:02<00:02,  2.20it/s]\n",
      " 55%|#####4    | 6/11 [00:02<00:02,  2.21it/s]\n",
      " 64%|######3   | 7/11 [00:03<00:01,  2.22it/s]\n",
      " 73%|#######2  | 8/11 [00:03<00:01,  2.22it/s]\n",
      " 82%|########1 | 9/11 [00:04<00:00,  2.22it/s]\n",
      " 91%|######### | 10/11 [00:04<00:00,  2.22it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.81it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.81it/s][INFO|trainer.py:1349] 2021-06-29 12:19:32,764 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 11/11 [00:04<00:00,  2.81it/s]\n",
      "100%|##########| 11/11 [00:04<00:00,  2.37it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 12:19:32,765 >> Saving model checkpoint to ../models/gradual_ft_baseline//Split-5\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 12:19:32,766 >> Configuration saved in ../models/gradual_ft_baseline//Split-5\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 12:19:33,418 >> Model weights saved in ../models/gradual_ft_baseline//Split-5\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 12:19:33,419 >> tokenizer config file saved in ../models/gradual_ft_baseline//Split-5\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 12:19:33,419 >> Special tokens file saved in ../models/gradual_ft_baseline//Split-5\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 12:19:33,499 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:19:33,501 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:19:33,501 >>   Num examples = 33\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:19:33,501 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.36it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 13.42it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 12:19:33,814 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 12:19:33,815 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 12:19:33,816 >>   Num examples = 32\n",
      "[INFO|trainer.py:2151] 2021-06-29 12:19:33,816 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "100%|##########| 1/1 [00:00<00:00, 11.76it/s]\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-cffbf0e95e42>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[0msquad_qa\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msquad_qa\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mto_remove_per_step\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0mbio_temp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbio_qa\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m         \u001B[0mmake_and_save_full_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msquad\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msquad_qa\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcovid_bio_squad_dataset_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mto_remove_per_step\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mbio_qa\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_rows\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-2-928c6e9b9181>\u001B[0m in \u001B[0;36mmake_and_save_full_dataset\u001B[1;34m(covid, squad, bioASQ, path)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mmake_and_save_full_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcovid\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msquad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbioASQ\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'../data/squad_bioASQ_covidQA/'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0msquad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msquad_qa\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mbioASQ\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbioASQ\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcovid\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m         \u001B[0mfull_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset_dict\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDatasetDict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'squad'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0msquad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'covid'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mcovid\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;34m'bio'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mbioASQ\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "directory = '../models/gradual_ft_baseline/'\n",
    "output_dir = '../models/gradual_ft_baseline/Split-' + str(k_fold)+'/'\n",
    "\n",
    "for i in range(K):\n",
    "    print('\\n\\n**************************************************')\n",
    "    print('==================================================')\n",
    "    print('          At Gradual Fine Tuning Step: ',i+1)\n",
    "    print('**************************************************')\n",
    "    print('==================================================\\n\\n')\n",
    "    if i < 1:\n",
    "        run_gradual_ft(directory,'roberta-base', k_fold )\n",
    "    else:\n",
    "        run_gradual_ft(directory, output_dir, k_fold)\n",
    "        squad_qa.shuffle()\n",
    "        bio_qa.shuffle()\n",
    "        covid_qa = datasets.Dataset.from_dict(covid_qa[:])\n",
    "\n",
    "    if i%2:\n",
    "        if to_remove_per_step > squad_qa.num_rows:\n",
    "            print('not enough data')\n",
    "            break\n",
    "        squad_qa = datasets.Dataset.from_dict(squad_qa[:-to_remove_per_step])\n",
    "        bio_temp = datasets.Dataset.from_dict(bio_qa[:1])\n",
    "        make_and_save_full_dataset(squad=squad_qa, bioASQ= bio_temp, path=covid_bio_squad_dataset_path)\n",
    "    else:\n",
    "        if to_remove_per_step > bio_qa.num_rows:\n",
    "            print('not enough data')\n",
    "            break\n",
    "        bio_qa = datasets.Dataset.from_dict(bio_qa[:-bio_remove_per_step])\n",
    "        squad_temp = datasets.Dataset.from_dict(squad_qa[:1])\n",
    "        make_and_save_full_dataset(squad=squad_temp, bioASQ=bio_qa,path=covid_bio_squad_dataset_path)\n",
    "\n",
    "print('Finished process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}