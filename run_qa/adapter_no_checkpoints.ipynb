{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/soe/joalara/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "bio_file = '../bioASQ/bioASQ.json'\n",
    "config_file_location = '../data/config.json'\n",
    "\n",
    "def make_and_save_full_dataset(train, valid, test, path):\n",
    "    full_data = datasets.dataset_dict.DatasetDict({'train':train, 'validation':valid, 'test': test})\n",
    "    full_data.save_to_disk(path)\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return datasets.load_dataset('custom_squad.py', data_files= {'train':filename})['train']\n",
    "\n",
    "def update_config(checkpoint, file_location = '../data/config.json'):\n",
    "    config_file = open(file_location, 'r')\n",
    "    config = json.load(config_file)\n",
    "    print(config)\n",
    "    config[\"_name_or_path\"] = checkpoint\n",
    "    config_file = open(file_location, 'w')\n",
    "    json.dump(config, config_file, indent= 2)\n",
    "    config_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import run_qa\n",
    "\n",
    "def run_gradual_ft(output_dir, checkpoint, covid_val, lr):\n",
    "    !CUDA_VISIBLE_DEVICES=1 python run_qa_alt.py \\\n",
    "      --model_name_or_path {checkpoint} \\\n",
    "      --dataset_name ../data/full_squad_covidQA/ \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --do_predict \\\n",
    "      --per_device_train_batch_size 40\\\n",
    "      --per_device_eval_batch_size 40\\\n",
    "      --evaluation_strategy \"no\" \\\n",
    "      --save_strategy \"no\" \\\n",
    "      --logging_strategy \"epoch\" \\\n",
    "      --learning_rate {lr} \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --max_seq_length 384 \\\n",
    "      --doc_stride 128 \\\n",
    "      --output_dir {output_dir} \\\n",
    "      --overwrite_output_dir \\\n",
    "      --train_adapter \\\n",
    "      --adapter_config houlsby \\\n",
    "      --load_adapter @ukp/roberta-base_qa_squad1_houlsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-22b6c9f63817822a\n",
      "Reusing dataset squad (/soe/joalara/.cache/huggingface/datasets/squad/default-22b6c9f63817822a/0.0.0/cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "Using custom data configuration default-c26cd1994268e9f2\n",
      "Reusing dataset squad (/soe/joalara/.cache/huggingface/datasets/squad/default-c26cd1994268e9f2/0.0.0/cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n"
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = covid_file\n",
    "\n",
    "covid_qa = get_dataset(covid_file)\n",
    "bio_qa = get_dataset(bio_file)\n",
    "\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "covid_and_squad_dataset_path = \"../data/full_squad_covidQA\"\n",
    "\n",
    "# squad_qa = datasets.Dataset.from_dict(squad_qa[:50])\n",
    "# covid_qa = datasets.Dataset.from_dict(covid_qa[:20])\n",
    "# bio_qa = datasets.Dataset.from_dict(bio_qa[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_adapter(lr):\n",
    "    k_fold = 5\n",
    "    for i in range(k_fold):\n",
    "        covid_fold = covid_qa.shard(k_fold, i)\n",
    "\n",
    "        covid_test = covid_fold.shard(2, 0)\n",
    "        covid_val = covid_fold.shard(2, 1)\n",
    "        covid_train = concatenate_datasets([covid_qa.shard(k_fold, j) for j in range(k_fold) if j != i])\n",
    "\n",
    "        #make_and_save_full_dataset(covid_train, squad_qa, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "\n",
    "        checkpoint = 'roberta-base'\n",
    "        cur_dir = '../models/adapter_baseline/split_' + str(i)\n",
    "\n",
    "        #log_file = open(cur_dir + \"/log_file.txt\",\"w+\")\n",
    "        #sys.stdout = log_file\n",
    "\n",
    "        squad_qa.shuffle()\n",
    "        bio_qa.shuffle()\n",
    "        full_dataset = datasets.concatenate_datasets([bio_qa, covid_train])\n",
    "        make_and_save_full_dataset(full_dataset, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "        run_gradual_ft(cur_dir, checkpoint, covid_val, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /soe/joalara/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-0818905460d158f7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/26/2021 16:47:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 16:47:35 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/adapter_baseline/split_0/runs/Jul26_16-47-35_nlp-gpu-01.soe.ucsc.edu,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/adapter_baseline/split_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=40,\n",
      "per_device_train_batch_size=40,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=split_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/adapter_baseline/split_0,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "===============================================================\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 14507\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 16:47:36,659 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/joalara/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 16:47:36,661 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-26 16:47:36,926 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 16:47:37,197 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/joalara/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 16:47:37,198 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 16:47:38,838 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /soe/joalara/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 16:47:38,839 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /soe/joalara/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 16:47:38,839 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /soe/joalara/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 16:47:38,839 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 16:47:38,840 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 16:47:38,840 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 16:47:39,291 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /soe/joalara/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1340] 2021-07-26 16:47:41,081 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1351] 2021-07-26 16:47:41,081 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "==============================================\n",
      "----------------------------------------------\n",
      "task_name not in model.config.adapters:  True\n",
      "adapter_config:  houlsby\n",
      "----------------------------------------------\n",
      "==============================================\n",
      "[INFO|utils.py:384] 2021-07-26 16:47:41,342 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_task/squad1/roberta-base/houlsby/roberta-base_qa_squad1_houlsby.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|loading.py:76] 2021-07-26 16:47:43,319 >> Loading module configuration from ~/.cache/torch/adapters/ee464f64e055b7b434f3f133d27c4f12ab29a53e427275020c25232ebcaff680-78424428eab1232aefb47c1358bc02042157efc9cdcff01a0da0d066dea810cd-extracted/adapter_config.json\n",
      "[INFO|configuration.py:260] 2021-07-26 16:47:43,320 >> Adding adapter 'full_squad_covidQA'.\n",
      "[INFO|loading.py:135] 2021-07-26 16:47:43,453 >> Loading module weights from ~/.cache/torch/adapters/ee464f64e055b7b434f3f133d27c4f12ab29a53e427275020c25232ebcaff680-78424428eab1232aefb47c1358bc02042157efc9cdcff01a0da0d066dea810cd-extracted/pytorch_adapter.bin\n",
      "[INFO|loading.py:76] 2021-07-26 16:47:43,464 >> Loading module configuration from ~/.cache/torch/adapters/ee464f64e055b7b434f3f133d27c4f12ab29a53e427275020c25232ebcaff680-78424428eab1232aefb47c1358bc02042157efc9cdcff01a0da0d066dea810cd-extracted/head_config.json\n",
      "[WARNING|loading.py:655] 2021-07-26 16:47:43,465 >> Model class 'RobertaModelWithHeads' of found prediction head does not match current model class.\n",
      "07/26/2021 16:47:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train/cache-44b8ef3a07998a4e.arrow\n",
      "07/26/2021 16:47:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation/cache-5f215fedc5de5870.arrow\n",
      "07/26/2021 16:47:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test/cache-fc2709b906e5037a.arrow\n",
      "[INFO|trainer.py:1199] 2021-07-26 16:48:11,221 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 16:48:11,221 >>   Num examples = 64948\n",
      "[INFO|trainer.py:1201] 2021-07-26 16:48:11,221 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1202] 2021-07-26 16:48:11,221 >>   Instantaneous batch size per device = 40\n",
      "[INFO|trainer.py:1203] 2021-07-26 16:48:11,222 >>   Total train batch size (w. parallel, distributed & accumulation) = 40\n",
      "[INFO|trainer.py:1204] 2021-07-26 16:48:11,222 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 16:48:11,222 >>   Total optimization steps = 1624\n",
      " 79%|██████████████████████████████▉        | 1288/1624 [08:23<02:11,  2.55it/s]"
     ]
    }
   ],
   "source": [
    "train_adapter(1e-5)\n",
    "train_adapter(2e-5)\n",
    "train_adapter(3e-5)\n",
    "train_adapter(5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adapter_base(lr):\n",
    "    k_fold = 5\n",
    "    for i in range(k_fold):\n",
    "        covid_fold = covid_qa.shard(k_fold, i)\n",
    "\n",
    "        covid_test = covid_fold.shard(2, 0)\n",
    "        covid_val = covid_fold.shard(2, 1)\n",
    "        covid_train = concatenate_datasets([covid_qa.shard(k_fold, j) for j in range(k_fold) if j != i])\n",
    "\n",
    "        #make_and_save_full_dataset(covid_train, squad_qa, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "\n",
    "        checkpoint = 'roberta-base'\n",
    "        cur_dir = '../models/adapter_COVID_baseline/split_' + str(i)\n",
    "\n",
    "        #log_file = open(cur_dir + \"/log_file.txt\",\"w+\")\n",
    "        #sys.stdout = log_file\n",
    "\n",
    "        squad_qa.shuffle()\n",
    "        bio_qa.shuffle()\n",
    "        make_and_save_full_dataset(covid_train, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "        run_gradual_ft(cur_dir, checkpoint, covid_val, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adapter_base(1e-5)\n",
    "train_adapter_base(2e-5)\n",
    "train_adapter_base(3e-5)\n",
    "train_adapter_base(5e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}