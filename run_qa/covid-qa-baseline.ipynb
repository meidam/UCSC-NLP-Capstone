{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04af5df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/19/2021 17:26:15 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 6distributed training: False, 16-bits training: False\n",
      "05/19/2021 17:26:15 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=models/covid-qa-baseline, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May19_17-26-15_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=models/covid-qa-baseline, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=6, mp_parameters=)\n",
      "05/19/2021 17:26:16 - WARNING - datasets.builder -   Using custom data configuration default-741ac7dcfb8a81cd\n",
      "Downloading and preparing dataset squad/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /soe/meidam/.cache/huggingface/datasets/squad/default-741ac7dcfb8a81cd/0.0.0/ddccfb2f36d6e6e3f09c4a028a2e2dc6afbe3f8b1db026351824be8ff9be500b...\n",
      "LOADING DATAFILES\n",
      "{'train': '../data/COVID-QA.json', 'validation': '../data/dev-v1.1.json'}\n",
      "Dataset squad downloaded and prepared to /soe/meidam/.cache/huggingface/datasets/squad/default-741ac7dcfb8a81cd/0.0.0/ddccfb2f36d6e6e3f09c4a028a2e2dc6afbe3f8b1db026351824be8ff9be500b. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:517] 2021-05-19 17:26:19,499 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-05-19 17:26:19,501 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-05-19 17:26:19,717 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-05-19 17:26:19,719 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-19 17:26:20,999 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /soe/meidam/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-19 17:26:21,000 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /soe/meidam/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-19 17:26:21,000 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /soe/meidam/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-19 17:26:21,000 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-19 17:26:21,000 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-19 17:26:21,000 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1155] 2021-05-19 17:26:21,344 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /soe/meidam/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1330] 2021-05-19 17:26:22,728 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1341] 2021-05-19 17:26:22,728 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                     | 0/3 [00:00<?, ?ba/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path roberta-base \\\n",
    "  --train_file ../data/COVID-QA.json \\\n",
    "  --validation_file ../data/dev-v1.1.json \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 4 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir models/covid-qa-baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
