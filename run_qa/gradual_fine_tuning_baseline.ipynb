{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/soe/meidam/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "jsonfile = open(covid_file, 'r')\n",
    "\n",
    "covidQA = jsonfile.read()\n",
    "jsonfile.close()\n",
    "\n",
    "covid_data = json.loads(covidQA)\n",
    "def convert_to_squad_format(data):\n",
    "    covid_qa_squad_format = []\n",
    "    for rows in data['data']:\n",
    "      for context in rows['paragraphs']:\n",
    "        for qa_pairs in context['qas']:\n",
    "          features = {'id':str(context['document_id']),\n",
    "                      'title': 'COVID_19',\n",
    "                      'context':str(context['context']),\n",
    "                      'question':qa_pairs['question'],\n",
    "                      'answers':{'answer_start': np.array([qa_pairs['answers'][0]['answer_start']], dtype=np.int32),\n",
    "                                 'text':[qa_pairs['answers'][0]['text']]}}\n",
    "          covid_qa_squad_format.append(features)\n",
    "    covid_df = pd.DataFrame(covid_qa_squad_format)\n",
    "    return (datasets.Dataset.from_dict(covid_df))#.train_test_split(test_size=0.2)\n",
    "\n",
    "def make_and_save_full_dataset(train, valid, test, path):\n",
    "    full_data = datasets.dataset_dict.DatasetDict({'train':train, 'validation':valid, 'test': test})\n",
    "    full_data.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import run_qa\n",
    "\n",
    "def run_gradual_ft(output_dir, checkpoint, covid_val):\n",
    "    !CUDA_VISIBLE_DEVICES=0 python run_qa.py \\\n",
    "      --model_name_or_path {checkpoint} \\\n",
    "      --dataset_name ../data/full_squad_covidQA/ \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --do_predict \\\n",
    "      --per_device_train_batch_size 32\\\n",
    "      --per_device_eval_batch_size 32\\\n",
    "      --evaluation_strategy \"no\" \\\n",
    "      --save_strategy \"no\" \\\n",
    "      --logging_strategy \"epoch\" \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --max_seq_length 384 \\\n",
    "      --doc_stride 128 \\\n",
    "      --output_dir {output_dir} \\\n",
    "      --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7edf67a6e660d46e\n",
      "Reusing dataset squad (/soe/meidam/.cache/huggingface/datasets/squad/default-7edf67a6e660d46e/0.0.0/cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n"
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = covid_file\n",
    "\n",
    "covid_qa = load_dataset('custom_squad.py', data_files=data_files)['train']\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "covid_and_squad_dataset_path = \"../data/full_squad_covidQA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/28/2021 09:53:44 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 09:53:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_09-53-44_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 80151\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:517] 2021-06-28 09:53:45,745 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 09:53:45,746 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-06-28 09:53:46,021 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 09:53:46,022 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 09:53:47,637 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /soe/meidam/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 09:53:47,638 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /soe/meidam/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 09:53:47,638 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /soe/meidam/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 09:53:47,638 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 09:53:47,638 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 09:53:47,638 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1155] 2021-06-28 09:53:48,069 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /soe/meidam/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1330] 2021-06-28 09:53:49,753 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1341] 2021-06-28 09:53:49,753 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/28/2021 09:53:49 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train/cache-2aabd9dbe9a9e650.arrow\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.01s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.54s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 09:54:18,390 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 09:54:18,390 >>   Num examples = 129315\n",
      "[INFO|trainer.py:1147] 2021-06-28 09:54:18,390 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 09:54:18,390 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 09:54:18,391 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 09:54:18,391 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 09:54:18,391 >>   Total optimization steps = 4042\n",
      "{'loss': 0.872, 'learning_rate': 0.0, 'epoch': 1.0}                             \n",
      "100%|███████████████████████████████████████| 4042/4042 [37:17<00:00,  2.13it/s][INFO|trainer.py:1341] 2021-06-28 10:31:35,985 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2237.5948, 'train_samples_per_second': 1.806, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 4042/4042 [37:17<00:00,  1.81it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 10:31:36,336 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 10:31:36,338 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 10:31:37,217 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 10:31:37,218 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 10:31:37,218 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 10:31:37,318 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   init_mem_cpu_alloc_delta   =     1441MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   init_mem_cpu_peaked_delta  =      355MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   train_mem_cpu_alloc_delta  =      896MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   train_mem_cpu_peaked_delta =        3MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   train_runtime              = 0:37:17.59\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   train_samples              =     129315\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:31:37,318 >>   train_samples_per_second   =      1.806\n",
      "06/28/2021 10:31:37 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 10:31:37,319 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 10:31:37,321 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 10:31:37,321 >>   Num examples = 6102\n",
      "[INFO|trainer.py:2120] 2021-06-28 10:31:37,321 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 190/191 [00:25<00:00,  7.41it/s]06/28/2021 10:32:09 - INFO - utils_qa -   Post-processing 202 example predictions split into 6102 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:17, 11.75it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:13, 14.65it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:13, 14.87it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:11, 16.44it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 17.54it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:09, 18.95it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 20.53it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:08, 21.39it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 21.80it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 23.52it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:07, 22.98it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:07, 21.43it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:07, 21.40it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:07, 21.61it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 19.74it/s]\u001b[A\n",
      " 23%|█████████▊                                | 47/202 [00:02<00:07, 21.09it/s]\u001b[A\n",
      " 25%|██████████▍                               | 50/202 [00:02<00:07, 19.47it/s]\u001b[A\n",
      " 26%|███████████                               | 53/202 [00:02<00:07, 20.41it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:08, 17.46it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:02<00:08, 17.10it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:02<00:06, 20.45it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.51it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.13it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 15.35it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:08, 14.98it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:10, 12.17it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:04<00:10, 12.07it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:12, 10.18it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:15,  7.94it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:04<00:12,  9.22it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:05<00:13,  8.61it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:12,  8.90it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:12,  9.18it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:12,  9.31it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:12,  9.28it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 93/202 [00:05<00:09, 11.70it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:05<00:09, 11.83it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:06<00:08, 12.52it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:06<00:07, 13.05it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:08, 11.81it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:07, 12.69it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:07, 12.99it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:07<00:09, 10.27it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:07<00:08, 10.34it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:07<00:08, 10.48it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:08, 10.68it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:07<00:07, 10.89it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:07<00:08, 10.03it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:08,  9.49it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:08<00:07, 11.11it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:08<00:06, 12.62it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:08<00:05, 13.39it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:05, 13.25it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 11.55it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:09<00:05, 12.27it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:05, 11.85it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:06,  9.66it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.38it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:07,  8.01it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:08,  7.65it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:08,  7.51it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:08,  7.30it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:08,  7.09it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  6.96it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:10<00:06,  8.50it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:05, 10.24it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:11<00:03, 13.00it/s]\u001b[A\n",
      " 76%|███████████████████████████████▎         | 154/202 [00:11<00:03, 13.66it/s]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 157/202 [00:11<00:03, 14.73it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 159/202 [00:11<00:02, 14.43it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 161/202 [00:11<00:03, 13.61it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 163/202 [00:11<00:02, 13.62it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 165/202 [00:12<00:02, 13.59it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 167/202 [00:12<00:02, 13.61it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 169/202 [00:12<00:03, 10.15it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 171/202 [00:12<00:03,  8.58it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:13<00:03,  7.90it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:03,  7.38it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:13<00:02,  9.62it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:13<00:01, 12.30it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:13<00:01, 14.84it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:13<00:00, 13.10it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:14<00:00, 15.58it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:14<00:00, 17.92it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:14<00:00, 14.08it/s]\u001b[A\n",
      "06/28/2021 10:32:23 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/eval_predictions.json.\n",
      "06/28/2021 10:32:23 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 191/191 [00:46<00:00,  4.11it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 10:32:23,972 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:32:23,973 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:32:23,973 >>   eval_exact_match = 34.1584\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:32:23,973 >>   eval_f1          = 61.5321\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:32:23,973 >>   eval_samples     =    6102\n",
      "06/28/2021 10:32:24 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 10:32:24,005 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 10:32:24,009 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 10:32:24,009 >>   Num examples = 6259\n",
      "[INFO|trainer.py:2120] 2021-06-28 10:32:24,009 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:26<00:00,  7.44it/s]06/28/2021 10:32:57 - INFO - utils_qa -   Post-processing 202 example predictions split into 6259 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:14, 13.80it/s]\u001b[A\n",
      "  2%|▊                                          | 4/202 [00:00<00:13, 14.83it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:11, 17.24it/s]\u001b[A\n",
      "  5%|██                                        | 10/202 [00:00<00:11, 16.03it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:10, 17.54it/s]\u001b[A\n",
      "  8%|███▎                                      | 16/202 [00:00<00:10, 18.51it/s]\u001b[A\n",
      "  9%|███▉                                      | 19/202 [00:00<00:08, 20.91it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:07, 22.73it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:08, 21.16it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/202 [00:01<00:08, 21.25it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:08, 20.09it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 18.81it/s]\u001b[A\n",
      " 18%|███████▍                                  | 36/202 [00:01<00:08, 19.08it/s]\u001b[A\n",
      " 19%|████████                                  | 39/202 [00:01<00:08, 19.30it/s]\u001b[A\n",
      " 21%|████████▋                                 | 42/202 [00:02<00:08, 19.68it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 19.60it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:09, 15.90it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 17.38it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 17.83it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 17.87it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 17.77it/s]\u001b[A\n",
      " 29%|████████████▎                             | 59/202 [00:03<00:07, 17.96it/s]\u001b[A\n",
      " 31%|█████████████                             | 63/202 [00:03<00:06, 21.21it/s]\u001b[A\n",
      " 33%|█████████████▋                            | 66/202 [00:03<00:09, 15.11it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:13, 10.30it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:04<00:14,  8.90it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:04<00:14,  8.98it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:12,  9.91it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:11, 11.32it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:09, 13.58it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:09, 12.40it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:05<00:10, 10.86it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:05<00:10, 10.94it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:10, 10.56it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:11, 10.27it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:05<00:10, 10.11it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:06<00:08, 12.07it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:06<00:08, 12.61it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:06<00:08, 12.74it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:08, 11.41it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:07, 12.40it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:07, 12.64it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:07<00:07, 11.98it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:07<00:07, 11.78it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:07<00:07, 11.59it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:08, 10.98it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:07<00:07, 10.89it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:08<00:08, 10.47it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:08,  9.70it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:08<00:08,  9.56it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:08<00:07, 11.19it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:08<00:06, 12.84it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:08<00:05, 14.35it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:05, 12.58it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:09<00:06, 10.98it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:05, 11.88it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:07,  9.57it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.63it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:07,  8.16it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:08,  7.83it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:08,  7.53it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:08,  7.21it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:10,  5.81it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:09,  6.06it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:11<00:09,  6.31it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:11<00:07,  7.75it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 149/202 [00:11<00:05,  9.66it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:11<00:04, 11.48it/s]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 157/202 [00:11<00:03, 13.38it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 159/202 [00:11<00:03, 13.40it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 161/202 [00:12<00:03, 13.51it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 163/202 [00:12<00:02, 13.61it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 165/202 [00:12<00:02, 13.12it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 167/202 [00:12<00:02, 13.35it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 169/202 [00:12<00:02, 13.21it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 171/202 [00:12<00:03, 10.06it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:13<00:03,  8.57it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:03,  7.77it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 175/202 [00:13<00:03,  7.25it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 179/202 [00:13<00:02,  9.47it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▎   | 184/202 [00:13<00:01, 12.36it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:13<00:01, 14.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:14<00:00, 13.02it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:14<00:00, 15.51it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:14<00:00, 13.90it/s]\u001b[A\n",
      "06/28/2021 10:33:11 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/predict_predictions.json.\n",
      "06/28/2021 10:33:11 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 10:33:11,829 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:33:11,829 >>   predict_samples  =    6259\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:33:11,829 >>   test_exact_match = 35.1485\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 10:33:11,829 >>   test_f1          = 59.8116\n",
      "100%|█████████████████████████████████████████| 196/196 [00:47<00:00,  4.10it/s]\n",
      "06/28/2021 10:33:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 10:33:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_10-33-16_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 60517\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 10:33:17,217 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 10:33:17,218 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 10:33:17,219 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 10:33:17,219 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 10:33:17,219 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 10:33:17,219 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 10:33:17,219 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 10:33:17,219 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 10:33:17,219 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 10:33:17,220 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 10:33:17,220 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 10:33:17,330 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 10:33:18,726 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 10:33:18,727 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 61/61 [00:53<00:00,  1.14ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.84s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.40s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 10:34:46,027 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 10:34:46,027 >>   Num examples = 109448\n",
      "[INFO|trainer.py:1147] 2021-06-28 10:34:46,027 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 10:34:46,027 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 10:34:46,027 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 10:34:46,027 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 10:34:46,027 >>   Total optimization steps = 3421\n",
      "{'loss': 0.5767, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 3421/3421 [27:16<00:00,  2.43it/s][INFO|trainer.py:1341] 2021-06-28 11:02:02,514 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1636.4869, 'train_samples_per_second': 2.09, 'epoch': 1.0}    \n",
      "100%|███████████████████████████████████████| 3421/3421 [27:16<00:00,  2.09it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 11:02:02,826 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 11:02:02,827 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 11:02:03,986 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 11:02:03,987 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 11:02:03,988 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:02:04,112 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,112 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,112 >>   init_mem_cpu_alloc_delta   =      342MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,112 >>   init_mem_cpu_peaked_delta  =      357MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,112 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,112 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,112 >>   train_mem_cpu_alloc_delta  =      849MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,112 >>   train_mem_cpu_peaked_delta =        2MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,113 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,113 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,113 >>   train_runtime              = 0:27:16.48\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,113 >>   train_samples              =     109448\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:04,113 >>   train_samples_per_second   =       2.09\n",
      "06/28/2021 11:02:04 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 11:02:04,114 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 11:02:04,115 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 11:02:04,115 >>   Num examples = 6102\n",
      "[INFO|trainer.py:2120] 2021-06-28 11:02:04,116 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 190/191 [00:25<00:00,  7.44it/s]06/28/2021 11:02:35 - INFO - utils_qa -   Post-processing 202 example predictions split into 6102 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:15, 12.95it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:12, 16.13it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:11, 16.23it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:10, 17.90it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:09, 19.49it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:09, 20.27it/s]\u001b[A\n",
      " 10%|████▎                                     | 21/202 [00:00<00:08, 22.20it/s]\u001b[A\n",
      " 12%|████▉                                     | 24/202 [00:01<00:08, 21.65it/s]\u001b[A\n",
      " 13%|█████▌                                    | 27/202 [00:01<00:07, 23.57it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:06, 25.23it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:07, 23.61it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:01<00:07, 21.15it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:06, 23.15it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:01<00:07, 21.02it/s]\u001b[A\n",
      " 23%|█████████▊                                | 47/202 [00:02<00:06, 22.18it/s]\u001b[A\n",
      " 25%|██████████▍                               | 50/202 [00:02<00:07, 20.11it/s]\u001b[A\n",
      " 26%|███████████                               | 53/202 [00:02<00:07, 21.10it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:07, 18.28it/s]\u001b[A\n",
      " 29%|████████████▎                             | 59/202 [00:02<00:07, 19.82it/s]\u001b[A\n",
      " 32%|█████████████▎                            | 64/202 [00:02<00:05, 24.18it/s]\u001b[A\n",
      " 33%|█████████████▉                            | 67/202 [00:03<00:07, 19.06it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:08, 16.44it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:08, 15.40it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:08, 15.87it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 78/202 [00:03<00:06, 17.93it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:03<00:08, 15.03it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 16.04it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 14.00it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:08, 12.79it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:04<00:09, 11.84it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:04<00:07, 14.01it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 14.14it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 14.24it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:07, 13.51it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:05<00:07, 13.34it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:05<00:06, 14.55it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:05<00:07, 13.68it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:05<00:07, 13.00it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 12.36it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.98it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:06<00:07, 11.86it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:06<00:07, 11.41it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:06<00:07, 10.59it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:07, 11.28it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:07<00:06, 13.05it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:07<00:05, 14.43it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:07<00:05, 14.02it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:07<00:06, 11.94it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:07<00:06, 11.05it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:06, 11.23it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:06,  9.79it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:08<00:07,  8.87it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:08<00:07,  8.30it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:08<00:08,  7.86it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:08<00:08,  7.67it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.61it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:07,  7.53it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:09<00:08,  7.14it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:09<00:06,  8.78it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 149/202 [00:09<00:04, 10.95it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:09<00:03, 13.17it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:09<00:02, 15.41it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 159/202 [00:10<00:02, 15.10it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 161/202 [00:10<00:02, 14.88it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 163/202 [00:10<00:02, 14.83it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 165/202 [00:10<00:02, 14.82it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 167/202 [00:10<00:02, 14.79it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 169/202 [00:10<00:02, 12.59it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 171/202 [00:11<00:03,  9.80it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:11<00:03,  8.62it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 175/202 [00:11<00:02,  9.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 179/202 [00:11<00:01, 12.00it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▎   | 184/202 [00:11<00:01, 15.39it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:12<00:00, 17.38it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:12<00:00, 14.99it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:12<00:00, 17.62it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:12<00:00, 19.86it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:12<00:00, 15.92it/s]\u001b[A\n",
      "06/28/2021 11:02:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/eval_predictions.json.\n",
      "06/28/2021 11:02:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 191/191 [00:44<00:00,  4.28it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:02:48,889 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:48,889 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:48,889 >>   eval_exact_match = 35.1485\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:48,889 >>   eval_f1          = 62.3795\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:02:48,889 >>   eval_samples     =    6102\n",
      "06/28/2021 11:02:48 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 11:02:48,943 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 11:02:48,947 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 11:02:48,947 >>   Num examples = 6259\n",
      "[INFO|trainer.py:2120] 2021-06-28 11:02:48,947 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:26<00:00,  7.33it/s]06/28/2021 11:03:21 - INFO - utils_qa -   Post-processing 202 example predictions split into 6259 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:13, 14.87it/s]\u001b[A\n",
      "  1%|▋                                          | 3/202 [00:00<00:16, 12.21it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:12, 15.18it/s]\u001b[A\n",
      "  5%|██                                        | 10/202 [00:00<00:12, 15.46it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:10, 17.21it/s]\u001b[A\n",
      "  8%|███▎                                      | 16/202 [00:00<00:09, 18.75it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 21.60it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:07, 22.92it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 20.36it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 22.39it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:08, 20.20it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:08, 20.02it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 20.24it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:07, 21.15it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 19.73it/s]\u001b[A\n",
      " 23%|█████████▊                                | 47/202 [00:02<00:08, 19.04it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:10, 15.16it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:09, 16.27it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 16.97it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 17.54it/s]\u001b[A\n",
      " 30%|████████████▍                             | 60/202 [00:02<00:07, 19.03it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 20.82it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.63it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:08, 15.66it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:03<00:08, 14.52it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:03<00:08, 14.31it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:03<00:08, 15.03it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 78/202 [00:04<00:07, 16.05it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:07, 15.85it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 14.00it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 14.95it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 13.31it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 12.27it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:09, 11.71it/s]\u001b[A\n",
      " 46%|███████████████████▏                      | 92/202 [00:05<00:08, 12.64it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:05<00:07, 14.61it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:05<00:07, 14.41it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:05<00:07, 14.20it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:05<00:08, 12.51it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:05<00:07, 13.74it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:06, 13.99it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:06<00:07, 13.28it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:06<00:07, 12.79it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:06<00:07, 12.23it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:06<00:07, 11.98it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:06<00:07, 11.75it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:07<00:07, 11.35it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:07<00:07, 10.57it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:07<00:07, 11.23it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:07<00:06, 12.93it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:07<00:05, 14.44it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:07<00:05, 14.55it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:05, 12.35it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:08<00:05, 12.97it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:05, 12.76it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:06, 10.52it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:08<00:06,  9.36it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:07,  8.41it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:07,  8.08it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:07,  7.68it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:08,  7.48it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:09<00:07,  7.43it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:09<00:09,  6.36it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:10<00:07,  7.77it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 149/202 [00:10<00:05,  9.78it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:10<00:04, 11.73it/s]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 157/202 [00:10<00:03, 14.22it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 159/202 [00:10<00:03, 14.23it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 161/202 [00:10<00:02, 14.24it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 163/202 [00:10<00:02, 14.39it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 165/202 [00:11<00:02, 14.62it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 167/202 [00:11<00:02, 14.84it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 169/202 [00:11<00:02, 14.40it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 171/202 [00:11<00:02, 10.95it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:11<00:03,  9.24it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 175/202 [00:12<00:03,  8.31it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 179/202 [00:12<00:02, 10.81it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▎   | 184/202 [00:12<00:01, 14.00it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:12<00:00, 15.94it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:12<00:00, 14.22it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:12<00:00, 16.93it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 15.35it/s]\u001b[A\n",
      "06/28/2021 11:03:34 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/predict_predictions.json.\n",
      "06/28/2021 11:03:34 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:03:35,011 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:03:35,011 >>   predict_samples  =    6259\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:03:35,011 >>   test_exact_match = 39.1089\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:03:35,011 >>   test_f1          =  64.515\n",
      "100%|█████████████████████████████████████████| 196/196 [00:46<00:00,  4.25it/s]\n",
      "06/28/2021 11:03:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 11:03:40 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_11-03-40_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 40883\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 11:03:41,161 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 11:03:41,163 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 11:03:41,163 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 11:03:41,164 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 11:03:41,165 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:03:41,165 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:03:41,165 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:03:41,166 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:03:41,166 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:03:41,166 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:03:41,166 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 11:03:41,305 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 11:03:42,951 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 11:03:42,951 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 41/41 [00:53<00:00,  1.30s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.18s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.60s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 11:05:05,956 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 11:05:05,956 >>   Num examples = 89586\n",
      "[INFO|trainer.py:1147] 2021-06-28 11:05:05,956 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 11:05:05,956 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 11:05:05,956 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 11:05:05,956 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 11:05:05,956 >>   Total optimization steps = 2800\n",
      "{'loss': 0.4439, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 2800/2800 [21:52<00:00,  2.39it/s][INFO|trainer.py:1341] 2021-06-28 11:26:58,830 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1312.8744, 'train_samples_per_second': 2.133, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 2800/2800 [21:52<00:00,  2.13it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 11:26:59,314 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 11:26:59,315 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 11:27:00,258 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 11:27:00,259 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 11:27:00,259 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:27:00,356 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   init_mem_cpu_alloc_delta   =      496MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   train_mem_cpu_alloc_delta  =      818MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   train_mem_cpu_peaked_delta =        2MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   train_runtime              = 0:21:52.87\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   train_samples              =      89586\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:00,356 >>   train_samples_per_second   =      2.133\n",
      "06/28/2021 11:27:00 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 11:27:00,357 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 11:27:00,358 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 11:27:00,359 >>   Num examples = 6102\n",
      "[INFO|trainer.py:2120] 2021-06-28 11:27:00,359 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 190/191 [00:25<00:00,  7.39it/s]06/28/2021 11:27:33 - INFO - utils_qa -   Post-processing 202 example predictions split into 6102 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:19, 10.48it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:14, 13.16it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:14, 13.43it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:12, 14.84it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:11, 16.02it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:10, 17.38it/s]\u001b[A\n",
      "  9%|███▉                                      | 19/202 [00:01<00:10, 17.99it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:09, 19.79it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:09, 19.12it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/202 [00:01<00:08, 21.12it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:07, 22.16it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 19.95it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:01<00:09, 17.72it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:02<00:08, 19.25it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:02<00:08, 19.22it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 17.41it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 18.31it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 17.35it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 17.42it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:03<00:09, 14.67it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:09, 15.41it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:07, 18.48it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:07, 19.33it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:08, 15.44it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:09, 13.53it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:04<00:10, 12.53it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:09, 13.20it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:09, 13.67it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:08, 14.33it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:09, 12.33it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:08, 13.29it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:05<00:10, 11.53it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:10, 10.48it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:11,  9.84it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 93/202 [00:05<00:08, 12.14it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:05<00:08, 12.24it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:06<00:08, 12.21it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:06<00:08, 12.45it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:09, 10.90it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:08, 11.97it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:08, 11.78it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:06<00:08, 11.15it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:07<00:08, 10.57it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:07<00:08, 10.33it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:09,  9.89it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:07<00:08,  9.84it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:08<00:09,  9.08it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:08<00:10,  8.37it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:09,  8.31it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:08<00:08,  9.72it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:08<00:07, 11.18it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:08<00:07, 10.81it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:06, 11.04it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:09<00:07,  9.81it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:09<00:06, 10.57it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:06, 10.24it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:07,  8.52it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:10<00:08,  7.76it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:10<00:09,  7.15it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:09,  6.96it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:09,  6.63it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:09,  6.47it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:09,  6.31it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:11<00:09,  6.25it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:11<00:09,  6.14it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:11<00:07,  7.58it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:05,  9.20it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:11<00:04, 11.47it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:11<00:03, 12.58it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:11<00:03, 14.54it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 13.72it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 13.29it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 12.83it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:02, 12.75it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:12<00:02, 12.65it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:12<00:02, 12.80it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:13<00:03,  9.49it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  7.82it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:13<00:04,  7.01it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:04,  6.61it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:13<00:02,  8.62it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▋    | 181/202 [00:14<00:01, 10.96it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:14<00:01, 13.47it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:14<00:01, 11.84it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:14<00:00, 13.89it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:14<00:00, 15.89it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 13.41it/s]\u001b[A\n",
      "06/28/2021 11:27:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/eval_predictions.json.\n",
      "06/28/2021 11:27:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 191/191 [00:47<00:00,  4.00it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:27:48,301 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:48,301 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:48,301 >>   eval_exact_match = 37.6238\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:48,302 >>   eval_f1          = 65.3642\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:27:48,302 >>   eval_samples     =    6102\n",
      "06/28/2021 11:27:48 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 11:27:48,338 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 11:27:48,341 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 11:27:48,341 >>   Num examples = 6259\n",
      "[INFO|trainer.py:2120] 2021-06-28 11:27:48,341 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:25<00:00,  7.32it/s]06/28/2021 11:28:20 - INFO - utils_qa -   Post-processing 202 example predictions split into 6259 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:14, 13.94it/s]\u001b[A\n",
      "  2%|▊                                          | 4/202 [00:00<00:13, 15.05it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:11, 17.49it/s]\u001b[A\n",
      "  5%|██                                        | 10/202 [00:00<00:11, 16.77it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:10, 18.12it/s]\u001b[A\n",
      "  8%|███▎                                      | 16/202 [00:00<00:09, 19.05it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 21.70it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:07, 22.66it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 19.96it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 21.89it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:08, 19.71it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:08, 19.60it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 19.71it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:07, 20.64it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 19.21it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:09, 16.68it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:09, 15.78it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:09, 16.68it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 17.12it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 17.38it/s]\u001b[A\n",
      " 29%|████████████▎                             | 59/202 [00:03<00:07, 17.90it/s]\u001b[A\n",
      " 31%|█████████████                             | 63/202 [00:03<00:06, 21.26it/s]\u001b[A\n",
      " 33%|█████████████▋                            | 66/202 [00:03<00:07, 18.84it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 69/202 [00:03<00:08, 15.96it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:09, 14.52it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:09, 13.41it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:04<00:08, 14.32it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:04<00:08, 15.10it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:07, 15.83it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 13.98it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 14.95it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 13.33it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:09, 12.10it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:09, 11.21it/s]\u001b[A\n",
      " 46%|███████████████████▏                      | 92/202 [00:05<00:09, 12.08it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:05<00:07, 13.90it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:05<00:07, 13.67it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:05<00:07, 13.74it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:08, 12.07it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:07, 13.22it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:07, 13.26it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:06<00:07, 12.51it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:06<00:07, 11.90it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:06<00:07, 11.60it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:07, 11.29it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:07<00:07, 11.33it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:07<00:07, 10.91it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:07<00:08, 10.06it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:07<00:07, 10.68it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:07<00:06, 12.22it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:08<00:05, 13.54it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:05, 14.00it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 11.78it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:08<00:05, 12.49it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:05, 12.27it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:06, 10.07it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.83it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:07,  8.09it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.79it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.57it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.32it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:08,  7.30it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  7.19it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:10<00:08,  7.15it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:10<00:06,  8.49it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:10<00:05,  9.32it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:10<00:04, 11.47it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:10<00:03, 12.68it/s]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 157/202 [00:10<00:02, 15.11it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 159/202 [00:11<00:02, 14.56it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 161/202 [00:11<00:02, 14.53it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 163/202 [00:11<00:02, 14.41it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 165/202 [00:11<00:02, 14.44it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 167/202 [00:11<00:02, 14.45it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 169/202 [00:11<00:02, 13.74it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 171/202 [00:12<00:03, 10.12it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:12<00:03,  8.72it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 175/202 [00:12<00:03,  8.03it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 179/202 [00:12<00:02, 10.39it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▎   | 184/202 [00:12<00:01, 13.44it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:13<00:00, 15.26it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:13<00:00, 13.35it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:13<00:00, 15.83it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.73it/s]\u001b[A\n",
      "06/28/2021 11:28:34 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/predict_predictions.json.\n",
      "06/28/2021 11:28:34 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:28:34,767 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:28:34,767 >>   predict_samples  =    6259\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:28:34,767 >>   test_exact_match =  40.099\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:28:34,768 >>   test_f1          = 64.7505\n",
      "100%|█████████████████████████████████████████| 196/196 [00:46<00:00,  4.22it/s]\n",
      "06/28/2021 11:28:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 11:28:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_11-28-39_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 21249\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 11:28:40,365 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 11:28:40,367 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 11:28:40,367 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 11:28:40,368 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 11:28:40,369 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:28:40,369 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:28:40,369 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:28:40,369 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:28:40,369 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:28:40,369 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:28:40,370 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 11:28:40,511 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 11:28:42,139 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 11:28:42,139 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 22/22 [00:35<00:00,  1.63s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.29s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.75s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 11:29:46,253 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 11:29:46,253 >>   Num examples = 69710\n",
      "[INFO|trainer.py:1147] 2021-06-28 11:29:46,253 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 11:29:46,253 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 11:29:46,253 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 11:29:46,253 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 11:29:46,253 >>   Total optimization steps = 2179\n",
      "{'loss': 0.3016, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 2179/2179 [17:05<00:00,  2.45it/s][INFO|trainer.py:1341] 2021-06-28 11:46:51,323 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1025.0702, 'train_samples_per_second': 2.126, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 2179/2179 [17:05<00:00,  2.13it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 11:46:51,756 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 11:46:51,757 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 11:46:52,702 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 11:46:52,702 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 11:46:52,703 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:46:52,800 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   init_mem_cpu_alloc_delta   =      441MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   train_mem_cpu_alloc_delta  =      782MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   train_mem_cpu_peaked_delta =        3MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   train_runtime              = 0:17:05.07\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   train_samples              =      69710\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:46:52,801 >>   train_samples_per_second   =      2.126\n",
      "06/28/2021 11:46:52 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 11:46:52,802 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 11:46:52,804 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 11:46:52,804 >>   Num examples = 6102\n",
      "[INFO|trainer.py:2120] 2021-06-28 11:46:52,804 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 190/191 [00:25<00:00,  7.44it/s]06/28/2021 11:47:24 - INFO - utils_qa -   Post-processing 202 example predictions split into 6102 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:17, 11.44it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:13, 14.50it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:13, 14.92it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:11, 16.38it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 17.63it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:09, 19.31it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 20.73it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:08, 21.75it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:07, 22.29it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 23.65it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:07, 23.10it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:07, 21.24it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:07, 21.57it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:07, 21.82it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:07, 19.76it/s]\u001b[A\n",
      " 23%|█████████▊                                | 47/202 [00:02<00:07, 21.00it/s]\u001b[A\n",
      " 25%|██████████▍                               | 50/202 [00:02<00:07, 19.21it/s]\u001b[A\n",
      " 26%|███████████                               | 53/202 [00:02<00:07, 20.45it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:08, 17.55it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:02<00:07, 18.14it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:02<00:06, 21.59it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 22.18it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.55it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 15.46it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:08, 14.81it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:08, 15.50it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 78/202 [00:03<00:07, 17.57it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:08, 14.18it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 14.42it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:08, 14.52it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:09, 12.73it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 11.62it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:10, 10.92it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 13.09it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 13.38it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.31it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:08, 12.60it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:05<00:08, 12.27it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:05<00:07, 13.43it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.51it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 12.09it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 11.64it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.48it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:06<00:07, 11.20it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08, 10.52it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08,  9.80it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:07, 10.33it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:07<00:06, 12.08it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:07<00:06, 11.65it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:07<00:05, 13.30it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:06, 11.33it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:08<00:06, 10.36it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:06, 11.23it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:07,  9.53it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.70it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:07,  8.01it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.60it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.26it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.17it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:08,  6.98it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:09<00:08,  6.93it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:10<00:06,  8.54it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:10<00:05, 10.28it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:10<00:03, 13.14it/s]\u001b[A\n",
      " 77%|███████████████████████████████▍         | 155/202 [00:10<00:03, 14.57it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:10<00:03, 14.38it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:10<00:02, 14.03it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:02, 13.99it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 14.00it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 13.78it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 13.88it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:11<00:03, 10.32it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  8.65it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  7.80it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:12<00:02, 10.16it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:12<00:01, 12.87it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:12<00:00, 15.55it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:13<00:00, 13.55it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:13<00:00, 16.06it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:13<00:00, 17.97it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.97it/s]\u001b[A\n",
      "06/28/2021 11:47:38 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/eval_predictions.json.\n",
      "06/28/2021 11:47:38 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 191/191 [00:45<00:00,  4.21it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:47:38,301 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:47:38,301 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:47:38,301 >>   eval_exact_match = 37.6238\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:47:38,301 >>   eval_f1          = 65.3636\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:47:38,301 >>   eval_samples     =    6102\n",
      "06/28/2021 11:47:38 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 11:47:38,369 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 11:47:38,372 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 11:47:38,372 >>   Num examples = 6259\n",
      "[INFO|trainer.py:2120] 2021-06-28 11:47:38,373 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:25<00:00,  7.47it/s]06/28/2021 11:48:10 - INFO - utils_qa -   Post-processing 202 example predictions split into 6259 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:14, 14.17it/s]\u001b[A\n",
      "  2%|▊                                          | 4/202 [00:00<00:13, 15.21it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:12, 15.96it/s]\u001b[A\n",
      "  5%|██                                        | 10/202 [00:00<00:12, 15.71it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:11, 17.12it/s]\u001b[A\n",
      "  8%|███▎                                      | 16/202 [00:00<00:10, 18.16it/s]\u001b[A\n",
      "  9%|███▉                                      | 19/202 [00:00<00:08, 20.58it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:07, 22.66it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:08, 21.12it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/202 [00:01<00:08, 21.56it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:08, 20.40it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 19.26it/s]\u001b[A\n",
      " 18%|███████▍                                  | 36/202 [00:01<00:08, 19.19it/s]\u001b[A\n",
      " 19%|████████                                  | 39/202 [00:01<00:08, 19.28it/s]\u001b[A\n",
      " 21%|████████▋                                 | 42/202 [00:02<00:08, 19.41it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 19.02it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:09, 16.53it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 17.74it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 18.25it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 18.32it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:07, 18.29it/s]\u001b[A\n",
      " 29%|████████████▎                             | 59/202 [00:03<00:07, 18.61it/s]\u001b[A\n",
      " 31%|█████████████                             | 63/202 [00:03<00:06, 21.91it/s]\u001b[A\n",
      " 33%|█████████████▋                            | 66/202 [00:03<00:07, 19.17it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 69/202 [00:03<00:08, 16.21it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 14.83it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:09, 13.70it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:04<00:08, 14.56it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:04<00:08, 15.58it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:07, 16.12it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 13.92it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:08, 14.63it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 12.94it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:09, 11.81it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:10, 10.99it/s]\u001b[A\n",
      " 46%|███████████████████▏                      | 92/202 [00:05<00:09, 11.94it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:05<00:07, 13.83it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:05<00:07, 13.61it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:05<00:07, 13.54it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:08, 11.84it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:07, 12.97it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:07, 13.08it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:06<00:07, 12.52it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:06<00:07, 11.91it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:06<00:07, 11.66it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:07, 11.25it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:07<00:07, 11.15it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:07<00:07, 10.65it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:07<00:08,  9.77it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:07<00:07, 10.53it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:07<00:06, 12.16it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:08<00:05, 13.47it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:05, 13.94it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 11.72it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:08<00:05, 12.40it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:06, 11.04it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:07,  9.38it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.49it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:07,  8.01it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.73it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.40it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.16it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:08,  7.02it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  6.97it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:10<00:08,  6.99it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:10<00:06,  8.41it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 149/202 [00:10<00:05, 10.40it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:10<00:03, 12.93it/s]\u001b[A\n",
      " 76%|███████████████████████████████▎         | 154/202 [00:10<00:03, 13.75it/s]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 157/202 [00:11<00:02, 15.90it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:02, 15.12it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:02, 15.03it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 14.70it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 14.77it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 14.42it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:02, 11.86it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  9.41it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  8.29it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 175/202 [00:12<00:03,  7.66it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 179/202 [00:12<00:02,  9.92it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▎   | 184/202 [00:13<00:01, 12.91it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:13<00:01, 14.77it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:13<00:00, 13.19it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:13<00:00, 15.61it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.65it/s]\u001b[A\n",
      "06/28/2021 11:48:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/predict_predictions.json.\n",
      "06/28/2021 11:48:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 11:48:24,878 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:48:24,878 >>   predict_samples  =    6259\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:48:24,878 >>   test_exact_match = 41.0891\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 11:48:24,878 >>   test_f1          = 65.6192\n",
      "100%|█████████████████████████████████████████| 196/196 [00:46<00:00,  4.22it/s]\n",
      "06/28/2021 11:48:30 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 11:48:30 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_11-48-30_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 21249\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 11:48:30,681 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 11:48:30,682 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 11:48:30,682 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 11:48:30,683 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 11:48:30,683 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:48:30,683 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:48:30,683 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:48:30,683 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:48:30,683 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:48:30,683 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 11:48:30,684 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 11:48:30,831 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 11:48:32,391 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 11:48:32,392 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 22/22 [00:42<00:00,  1.95s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.04s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.40s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 11:49:44,820 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 11:49:44,820 >>   Num examples = 69710\n",
      "[INFO|trainer.py:1147] 2021-06-28 11:49:44,820 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 11:49:44,820 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 11:49:44,820 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 11:49:44,820 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 11:49:44,820 >>   Total optimization steps = 2179\n",
      "{'loss': 0.2082, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 2179/2179 [17:07<00:00,  2.41it/s][INFO|trainer.py:1341] 2021-06-28 12:06:52,560 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1027.7406, 'train_samples_per_second': 2.12, 'epoch': 1.0}    \n",
      "100%|███████████████████████████████████████| 2179/2179 [17:07<00:00,  2.12it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 12:06:53,102 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 12:06:53,103 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 12:06:54,120 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 12:06:54,121 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 12:06:54,121 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 12:06:54,218 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   init_mem_cpu_alloc_delta   =      491MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   train_mem_cpu_alloc_delta  =      780MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   train_mem_cpu_peaked_delta =        3MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   train_runtime              = 0:17:07.74\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   train_samples              =      69710\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:06:54,218 >>   train_samples_per_second   =       2.12\n",
      "06/28/2021 12:06:54 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 12:06:54,219 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 12:06:54,221 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 12:06:54,221 >>   Num examples = 6102\n",
      "[INFO|trainer.py:2120] 2021-06-28 12:06:54,221 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 190/191 [00:25<00:00,  7.41it/s]06/28/2021 12:07:27 - INFO - utils_qa -   Post-processing 202 example predictions split into 6102 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:19, 10.51it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:14, 13.20it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:15, 12.83it/s]\u001b[A\n",
      "  5%|██                                        | 10/202 [00:00<00:13, 14.27it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:12, 15.35it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:11, 16.21it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:10, 17.20it/s]\u001b[A\n",
      "  9%|███▉                                      | 19/202 [00:01<00:10, 17.76it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:09, 19.43it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:09, 18.73it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/202 [00:01<00:08, 20.71it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:08, 21.35it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 19.41it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:01<00:09, 17.27it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:02<00:08, 19.10it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:02<00:08, 19.14it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:09, 17.07it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 17.55it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 17.08it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 17.13it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:03<00:10, 14.36it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:09, 14.94it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:07, 17.97it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:07, 18.94it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:08, 15.26it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:09, 13.25it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:04<00:10, 12.09it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:10, 12.67it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:09, 13.46it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:08, 14.01it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:09, 12.16it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:05<00:09, 12.97it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:05<00:10, 11.34it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:10, 10.39it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:11,  9.58it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 93/202 [00:05<00:09, 11.89it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:06<00:09, 11.84it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:06<00:08, 12.00it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:06<00:08, 11.97it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:09, 10.53it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:08, 11.44it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:08, 11.52it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:07<00:08, 10.78it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:07<00:09, 10.29it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:07<00:08, 10.21it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:08, 10.04it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:07<00:08,  9.98it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:08<00:09,  8.98it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:08<00:10,  8.11it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:12,  6.47it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:08<00:10,  7.88it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:08<00:08,  9.41it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:09<00:07, 10.47it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:09<00:07, 10.56it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:09<00:07,  9.32it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:09<00:07, 10.04it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:07,  9.84it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:10<00:08,  8.33it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:10<00:08,  7.48it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:10<00:09,  6.94it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:09,  6.68it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:09,  6.31it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:11<00:09,  6.22it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:11<00:09,  6.10it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:11<00:09,  6.05it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:11<00:09,  5.95it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:11<00:07,  7.33it/s]\u001b[A\n",
      " 73%|█████████████████████████████▊           | 147/202 [00:11<00:06,  9.02it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 149/202 [00:11<00:05, 10.48it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:12<00:03, 12.28it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:12<00:03, 13.85it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:12<00:03, 13.29it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 12.76it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 12.62it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:03, 12.40it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:13<00:02, 12.29it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:13<00:02, 12.14it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:13<00:03,  8.93it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:04,  7.48it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:14<00:04,  6.90it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:14<00:04,  6.45it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:14<00:02,  8.45it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▋    | 181/202 [00:14<00:01, 10.73it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 186/202 [00:14<00:01, 13.89it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▎  | 189/202 [00:14<00:01, 11.81it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▉  | 192/202 [00:15<00:00, 12.80it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▌ | 195/202 [00:15<00:00, 14.79it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▏| 198/202 [00:15<00:00, 16.71it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 12.99it/s]\u001b[A\n",
      "06/28/2021 12:07:42 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4/eval_predictions.json.\n",
      "06/28/2021 12:07:42 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 191/191 [00:48<00:00,  3.94it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 12:07:42,883 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:07:42,883 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:07:42,884 >>   eval_exact_match = 36.1386\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:07:42,884 >>   eval_f1          = 65.1809\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:07:42,884 >>   eval_samples     =    6102\n",
      "06/28/2021 12:07:42 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 12:07:42,932 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 12:07:42,936 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 12:07:42,936 >>   Num examples = 6259\n",
      "[INFO|trainer.py:2120] 2021-06-28 12:07:42,936 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:25<00:00,  7.47it/s]06/28/2021 12:08:16 - INFO - utils_qa -   Post-processing 202 example predictions split into 6259 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:15, 13.11it/s]\u001b[A\n",
      "  1%|▋                                          | 3/202 [00:00<00:17, 11.22it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:13, 14.11it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:13, 13.85it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:16, 11.55it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:14, 13.19it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:13, 14.35it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:01<00:11, 15.62it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:09, 18.37it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:10, 17.65it/s]\u001b[A\n",
      " 13%|█████▌                                    | 27/202 [00:01<00:10, 17.43it/s]\u001b[A\n",
      " 15%|██████▏                                   | 30/202 [00:01<00:09, 17.74it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:10, 16.95it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:10, 16.61it/s]\u001b[A\n",
      " 18%|███████▍                                  | 36/202 [00:02<00:09, 16.93it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:02<00:09, 17.52it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:02<00:09, 17.68it/s]\u001b[A\n",
      " 21%|████████▋                                 | 42/202 [00:02<00:09, 17.19it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:09, 17.01it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:10, 14.30it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:09, 15.58it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:03<00:09, 15.71it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:03<00:09, 15.86it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:03<00:08, 16.80it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:09, 15.09it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:07, 17.99it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:07, 18.32it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:04<00:09, 14.86it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:04<00:10, 13.08it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:04<00:10, 12.05it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:10, 12.19it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:09, 12.70it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:08, 14.56it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:05<00:09, 12.38it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:05<00:09, 12.64it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:05<00:10, 11.56it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:10, 10.62it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:11,  9.91it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:06<00:11,  9.47it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:06<00:09, 11.79it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:06<00:08, 11.82it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:06<00:08, 11.92it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:09, 10.94it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:07<00:09, 10.79it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:07<00:08, 11.59it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:07<00:08, 11.10it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:07<00:08, 10.63it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:08, 10.32it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:07<00:09,  9.99it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:08<00:09,  9.71it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:08<00:08,  9.72it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:08<00:08,  9.72it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:08<00:09,  8.95it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:08<00:11,  7.57it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:11,  7.40it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:08<00:10,  7.67it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:09<00:08,  9.19it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:09<00:07, 10.54it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:09<00:06, 11.86it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:09<00:06, 10.72it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:09<00:07,  9.65it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:10<00:06, 10.51it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:10<00:07,  8.60it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:10<00:08,  7.57it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:10<00:09,  7.01it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:09,  6.74it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:11<00:09,  6.52it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:11<00:09,  6.55it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:11<00:09,  6.30it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:11<00:09,  6.24it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:11<00:09,  6.14it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:11<00:09,  6.11it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:12<00:07,  7.31it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:12<00:06,  8.97it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:12<00:04, 10.89it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:12<00:04, 11.84it/s]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 157/202 [00:12<00:03, 13.99it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 159/202 [00:12<00:03, 13.39it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 161/202 [00:12<00:03, 13.18it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 163/202 [00:13<00:03, 12.86it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 165/202 [00:13<00:02, 12.73it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 167/202 [00:13<00:02, 12.58it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 169/202 [00:13<00:02, 12.01it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 171/202 [00:13<00:03,  9.02it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:14<00:03,  7.74it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:14<00:03,  7.00it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 175/202 [00:14<00:04,  5.78it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:14<00:03,  7.60it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▋    | 181/202 [00:14<00:02,  9.72it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 186/202 [00:14<00:01, 12.71it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▎  | 189/202 [00:15<00:01, 11.30it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▉  | 192/202 [00:15<00:00, 12.44it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▌ | 195/202 [00:15<00:00, 14.58it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 12.81it/s]\u001b[A\n",
      "06/28/2021 12:08:32 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4/predict_predictions.json.\n",
      "06/28/2021 12:08:32 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 12:08:32,277 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:08:32,277 >>   predict_samples  =    6259\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:08:32,277 >>   test_exact_match = 41.5842\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:08:32,278 >>   test_f1          =  66.053\n",
      "100%|█████████████████████████████████████████| 196/196 [00:49<00:00,  3.97it/s]\n",
      "06/28/2021 12:08:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 12:08:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_12-08-38_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 80151\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:517] 2021-06-28 12:08:39,331 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 12:08:39,332 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-06-28 12:08:39,601 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 12:08:39,602 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 12:08:41,210 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /soe/meidam/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 12:08:41,211 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /soe/meidam/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 12:08:41,211 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /soe/meidam/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 12:08:41,211 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 12:08:41,212 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 12:08:41,212 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1155] 2021-06-28 12:08:41,632 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /soe/meidam/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1330] 2021-06-28 12:08:43,453 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1341] 2021-06-28 12:08:43,453 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/28/2021 12:08:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train/cache-f5c9c0439ce4a9c9.arrow\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.03s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.94s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 12:09:11,034 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 12:09:11,035 >>   Num examples = 129233\n",
      "[INFO|trainer.py:1147] 2021-06-28 12:09:11,035 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 12:09:11,035 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 12:09:11,035 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 12:09:11,035 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 12:09:11,035 >>   Total optimization steps = 4039\n",
      "{'loss': 0.8661, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 4039/4039 [31:31<00:00,  2.35it/s][INFO|trainer.py:1341] 2021-06-28 12:40:42,252 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1891.2181, 'train_samples_per_second': 2.136, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 4039/4039 [31:31<00:00,  2.14it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 12:40:42,661 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 12:40:42,662 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 12:40:43,769 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 12:40:43,770 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 12:40:43,770 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 12:40:43,868 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,868 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   init_mem_cpu_alloc_delta   =     1451MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   init_mem_cpu_peaked_delta  =      355MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   train_mem_cpu_alloc_delta  =      882MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   train_mem_cpu_peaked_delta =        3MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   train_runtime              = 0:31:31.21\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   train_samples              =     129233\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:40:43,869 >>   train_samples_per_second   =      2.136\n",
      "06/28/2021 12:40:43 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 12:40:43,870 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 12:40:43,871 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 12:40:43,871 >>   Num examples = 6175\n",
      "[INFO|trainer.py:2120] 2021-06-28 12:40:43,871 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 193/193 [00:25<00:00,  7.49it/s]06/28/2021 12:41:17 - INFO - utils_qa -   Post-processing 202 example predictions split into 6175 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:19, 10.46it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:14, 13.23it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:14, 13.24it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:13, 14.68it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:12, 15.73it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:11, 16.24it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:10, 17.22it/s]\u001b[A\n",
      " 10%|████▎                                     | 21/202 [00:01<00:10, 17.96it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:10, 17.43it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:10, 17.31it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/202 [00:01<00:08, 19.35it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:08, 20.86it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 19.30it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:02<00:09, 17.04it/s]\u001b[A\n",
      " 19%|████████                                  | 39/202 [00:02<00:10, 16.16it/s]\u001b[A\n",
      " 21%|████████▋                                 | 42/202 [00:02<00:09, 17.01it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:10, 15.78it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:10, 15.51it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:09, 16.75it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:09, 16.41it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:03<00:08, 16.78it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:03<00:09, 14.71it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:09, 15.26it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:07, 18.25it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:07, 18.99it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:08, 14.99it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:04<00:09, 13.25it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:04<00:10, 12.19it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:09, 12.90it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:09, 13.40it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:08, 13.97it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:10, 11.89it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:05<00:09, 12.77it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:05<00:10, 11.10it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:11, 10.20it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:11,  9.68it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 93/202 [00:05<00:09, 11.89it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:06<00:09, 11.77it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:06<00:08, 11.70it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:06<00:08, 11.84it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:09, 10.33it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:08, 11.42it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:07<00:08, 11.49it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:07<00:08, 10.92it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:07<00:09, 10.33it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:07<00:09,  9.94it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:09,  9.87it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:08<00:08,  9.83it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:08<00:10,  8.21it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:08<00:10,  7.84it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:10,  7.64it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:08<00:08,  9.11it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:08<00:07, 10.54it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:09<00:06, 11.62it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:09<00:06, 11.54it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:09<00:07, 10.05it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:09<00:06, 10.73it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:10<00:08,  8.59it/s]\u001b[A\n",
      " 66%|███████████████████████████▏             | 134/202 [00:10<00:09,  7.51it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:10<00:09,  6.99it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:10<00:09,  6.62it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:10<00:10,  6.45it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:10,  6.32it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:11<00:10,  6.29it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:11<00:10,  6.11it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:11<00:10,  6.07it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:11<00:10,  5.97it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:11<00:09,  5.97it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:11<00:07,  7.35it/s]\u001b[A\n",
      " 73%|█████████████████████████████▊           | 147/202 [00:11<00:06,  8.97it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 149/202 [00:12<00:04, 10.60it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:12<00:03, 12.36it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:12<00:03, 14.18it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:12<00:03, 13.34it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 12.83it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 12.38it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:13<00:03, 12.34it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:13<00:02, 12.15it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:13<00:02, 11.95it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:13<00:03,  9.05it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:14<00:04,  7.10it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:14<00:04,  6.58it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:14<00:04,  6.33it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 177/202 [00:14<00:03,  8.28it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:14<00:02, 10.57it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:14<00:01, 13.58it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:15<00:01, 13.20it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:15<00:00, 13.90it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:15<00:00, 15.78it/s]\u001b[A\n",
      " 98%|███████████████████████████████████████▉ | 197/202 [00:15<00:00, 17.47it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 12.87it/s]\u001b[A\n",
      "06/28/2021 12:41:32 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/eval_predictions.json.\n",
      "06/28/2021 12:41:32 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 193/193 [00:48<00:00,  3.94it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 12:41:33,011 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:41:33,011 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:41:33,012 >>   eval_exact_match = 36.1386\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:41:33,012 >>   eval_f1          = 61.1232\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:41:33,012 >>   eval_samples     =    6175\n",
      "06/28/2021 12:41:33 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 12:41:33,061 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 12:41:33,064 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 12:41:33,064 >>   Num examples = 6268\n",
      "[INFO|trainer.py:2120] 2021-06-28 12:41:33,064 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 196/196 [00:26<00:00,  7.70it/s]06/28/2021 12:42:06 - INFO - utils_qa -   Post-processing 202 example predictions split into 6268 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:19, 10.13it/s]\u001b[A\n",
      "  1%|▋                                          | 3/202 [00:00<00:20,  9.72it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:16, 12.09it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:15, 12.26it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:13, 13.89it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:12, 15.09it/s]\u001b[A\n",
      "  8%|███▎                                      | 16/202 [00:00<00:11, 15.72it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:01<00:11, 16.56it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:09, 19.26it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:09, 18.04it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/202 [00:01<00:08, 20.03it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:09, 18.32it/s]\u001b[A\n",
      " 16%|██████▊                                   | 33/202 [00:01<00:10, 16.40it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:10, 16.16it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:02<00:09, 17.12it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:02<00:09, 17.66it/s]\u001b[A\n",
      " 21%|████████▋                                 | 42/202 [00:02<00:09, 16.78it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:09, 15.99it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:11, 13.70it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:10, 15.05it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:09, 15.17it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:03<00:09, 15.33it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:03<00:08, 16.24it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:09, 14.75it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:07, 17.63it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:07, 18.56it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:04<00:09, 14.75it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:04<00:10, 13.11it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:04<00:10, 11.87it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:10, 11.98it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:10, 12.50it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:08, 14.52it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:05<00:09, 12.28it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:05<00:09, 12.51it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:05<00:10, 11.32it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:11, 10.33it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:11,  9.77it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:06<00:10, 10.69it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:06<00:08, 12.29it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:06<00:08, 12.32it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:06<00:08, 12.09it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:09, 11.10it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:09, 10.67it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:07<00:08, 11.54it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:07<00:09,  9.70it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:07<00:09,  9.66it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:09,  9.57it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:08<00:09,  9.49it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:08<00:09,  9.45it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:08<00:09,  9.26it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:08<00:09,  9.27it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:08<00:09,  8.76it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:08<00:10,  8.42it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:08<00:10,  8.34it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:10,  7.85it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:09<00:10,  7.99it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:09<00:08,  9.47it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:09<00:07, 10.73it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:09<00:06, 11.96it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:09<00:06, 10.72it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:09<00:07,  9.57it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:10<00:06, 10.37it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:10<00:07,  8.54it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:10<00:08,  7.52it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:10<00:09,  6.93it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:09,  6.59it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:11<00:09,  6.41it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:11<00:09,  6.29it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:11<00:10,  6.10it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:11<00:09,  6.12it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:11<00:09,  5.97it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:11<00:09,  5.98it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:12<00:07,  7.15it/s]\u001b[A\n",
      " 74%|██████████████████████████████▍          | 150/202 [00:12<00:05,  9.22it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:12<00:04, 10.63it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:12<00:03, 12.60it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:12<00:03, 12.42it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 12.46it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:13<00:03, 12.34it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:13<00:03, 12.35it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:13<00:03, 11.05it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:13<00:02, 11.47it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:13<00:03,  9.76it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:14<00:03,  8.04it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:14<00:04,  7.14it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:14<00:04,  6.60it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:14<00:03,  7.51it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 179/202 [00:14<00:02,  9.64it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 183/202 [00:14<00:01, 12.25it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:15<00:01, 14.26it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:15<00:00, 12.19it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:15<00:00, 14.50it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:15<00:00, 16.05it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 12.72it/s]\u001b[A\n",
      "06/28/2021 12:42:22 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/predict_predictions.json.\n",
      "06/28/2021 12:42:22 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 12:42:22,753 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:42:22,753 >>   predict_samples  =    6268\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:42:22,753 >>   test_exact_match = 31.1881\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 12:42:22,753 >>   test_f1          = 61.1655\n",
      "100%|█████████████████████████████████████████| 196/196 [00:49<00:00,  3.95it/s]\n",
      "06/28/2021 12:42:28 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 12:42:28 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_12-42-28_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 60517\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 12:42:28,867 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 12:42:28,868 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 12:42:28,869 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 12:42:28,870 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 12:42:28,870 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 12:42:28,870 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 12:42:28,870 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 12:42:28,870 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 12:42:28,870 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 12:42:28,871 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 12:42:28,871 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 12:42:29,003 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 12:42:30,545 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 12:42:30,546 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 61/61 [00:56<00:00,  1.09ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.94s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.00s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 12:43:54,724 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 12:43:54,724 >>   Num examples = 109366\n",
      "[INFO|trainer.py:1147] 2021-06-28 12:43:54,724 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 12:43:54,724 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 12:43:54,724 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 12:43:54,724 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 12:43:54,724 >>   Total optimization steps = 3418\n",
      "{'loss': 0.573, 'learning_rate': 0.0, 'epoch': 1.0}                             \n",
      "100%|███████████████████████████████████████| 3418/3418 [26:49<00:00,  2.29it/s][INFO|trainer.py:1341] 2021-06-28 13:10:43,958 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1609.234, 'train_samples_per_second': 2.124, 'epoch': 1.0}    \n",
      "100%|███████████████████████████████████████| 3418/3418 [26:49<00:00,  2.12it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 13:10:44,250 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 13:10:44,251 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 13:10:45,194 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 13:10:45,195 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 13:10:45,195 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:10:45,287 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   init_mem_cpu_alloc_delta   =      273MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   train_mem_cpu_alloc_delta  =      856MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   train_mem_cpu_peaked_delta =        2MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   train_runtime              = 0:26:49.23\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   train_samples              =     109366\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:10:45,288 >>   train_samples_per_second   =      2.124\n",
      "06/28/2021 13:10:45 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 13:10:45,289 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 13:10:45,290 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 13:10:45,290 >>   Num examples = 6175\n",
      "[INFO|trainer.py:2120] 2021-06-28 13:10:45,290 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 193/193 [00:25<00:00,  7.37it/s]06/28/2021 13:11:17 - INFO - utils_qa -   Post-processing 202 example predictions split into 6175 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:18, 10.94it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:14, 13.89it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:13, 14.37it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:11, 16.16it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 17.20it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:09, 18.65it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:09, 19.56it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:09, 19.46it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 20.48it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 22.26it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:07, 22.38it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:07, 20.96it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 19.51it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:08, 19.72it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 18.59it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 18.27it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 19.04it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:07, 18.93it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:07, 18.93it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:08, 17.08it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:02<00:08, 17.64it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:06, 20.85it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.99it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.46it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 15.24it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:08, 14.79it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:08, 15.11it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 78/202 [00:04<00:07, 17.18it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:08, 14.26it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 14.56it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 14.91it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:09, 12.82it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 11.57it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:10, 11.15it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 13.16it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 13.37it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.53it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:08, 12.67it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:05<00:08, 11.88it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 13.12it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.46it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 11.89it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:08, 11.49it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:08, 10.02it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:08, 10.12it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08,  9.94it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08,  9.44it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:07<00:09,  9.11it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:07<00:07, 10.80it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:07<00:06, 12.55it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:05, 13.00it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 11.32it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:08<00:05, 12.11it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:07,  9.80it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:07,  8.69it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:09<00:08,  8.13it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:08,  7.75it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:08,  7.53it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.40it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.12it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.05it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:08,  6.88it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  6.87it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:10<00:06,  8.52it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:10<00:05, 10.25it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:10<00:03, 13.04it/s]\u001b[A\n",
      " 77%|███████████████████████████████▍         | 155/202 [00:10<00:03, 14.52it/s]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 157/202 [00:10<00:03, 14.62it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 159/202 [00:11<00:02, 14.56it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 161/202 [00:11<00:02, 14.04it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 163/202 [00:11<00:02, 13.90it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 165/202 [00:11<00:02, 13.72it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 167/202 [00:11<00:02, 13.72it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 169/202 [00:11<00:02, 11.52it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 171/202 [00:12<00:03,  9.24it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:12<00:03,  8.18it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:04,  6.42it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:12<00:02,  8.44it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:12<00:01, 10.99it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:13<00:01, 13.63it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:13<00:00, 13.62it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:13<00:00, 16.23it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:13<00:00, 18.23it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.71it/s]\u001b[A\n",
      "06/28/2021 13:11:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/eval_predictions.json.\n",
      "06/28/2021 13:11:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 193/193 [00:45<00:00,  4.20it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:11:31,426 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:11:31,426 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:11:31,426 >>   eval_exact_match = 40.5941\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:11:31,426 >>   eval_f1          = 65.3499\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:11:31,426 >>   eval_samples     =    6175\n",
      "06/28/2021 13:11:31 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 13:11:31,478 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 13:11:31,480 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 13:11:31,480 >>   Num examples = 6268\n",
      "[INFO|trainer.py:2120] 2021-06-28 13:11:31,480 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 196/196 [00:26<00:00,  7.74it/s]06/28/2021 13:12:03 - INFO - utils_qa -   Post-processing 202 example predictions split into 6268 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:15, 12.52it/s]\u001b[A\n",
      "  2%|▊                                          | 4/202 [00:00<00:14, 13.86it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:11, 16.28it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:12, 15.80it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:11, 17.26it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:10, 18.24it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:09, 18.85it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:08, 22.11it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:08, 20.87it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 22.83it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:08, 20.68it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:08, 19.37it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 20.30it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:07, 20.78it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 19.01it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:09, 16.11it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 17.66it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 18.01it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 17.92it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:07, 18.27it/s]\u001b[A\n",
      " 29%|████████████▎                             | 59/202 [00:02<00:07, 18.57it/s]\u001b[A\n",
      " 32%|█████████████▎                            | 64/202 [00:03<00:06, 22.42it/s]\u001b[A\n",
      " 33%|█████████████▉                            | 67/202 [00:03<00:07, 17.67it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:08, 15.59it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:03<00:09, 13.95it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:03<00:09, 13.89it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:08, 14.46it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:07, 16.64it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:08, 14.07it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:04<00:08, 14.35it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:04<00:08, 13.26it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:04<00:09, 12.00it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:10, 11.24it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:05<00:09, 12.20it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:07, 14.13it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 13.96it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 14.02it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:09, 11.22it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 11.32it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 12.49it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:08, 11.96it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:08, 11.56it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:08, 11.44it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.26it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:07, 11.36it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:07, 10.77it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08, 10.12it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:08,  9.71it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:07<00:07, 11.30it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:07<00:06, 12.96it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:08<00:05, 14.24it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:05, 12.59it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:08<00:06, 11.14it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:05, 11.94it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:06,  9.73it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.63it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:07,  8.15it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.66it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.44it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  6.79it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:08,  6.78it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  6.78it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:10<00:08,  6.74it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:10<00:06,  8.15it/s]\u001b[A\n",
      " 74%|██████████████████████████████▍          | 150/202 [00:10<00:04, 10.47it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:10<00:04, 12.14it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:10<00:03, 14.38it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 14.25it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:03, 12.64it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:03, 12.89it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 13.21it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 13.20it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 13.53it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:02, 11.27it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  9.18it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  8.14it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:12<00:02,  9.19it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:12<00:01, 11.70it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:13<00:01, 14.97it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:13<00:00, 14.88it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:13<00:00, 14.36it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:13<00:00, 16.96it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.64it/s]\u001b[A\n",
      "06/28/2021 13:12:17 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/predict_predictions.json.\n",
      "06/28/2021 13:12:17 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:12:17,733 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:12:17,733 >>   predict_samples  =    6268\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:12:17,734 >>   test_exact_match = 34.1584\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:12:17,734 >>   test_f1          = 64.0796\n",
      "100%|█████████████████████████████████████████| 196/196 [00:46<00:00,  4.23it/s]\n",
      "06/28/2021 13:12:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 13:12:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_13-12-23_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 40883\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 13:12:23,724 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 13:12:23,725 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 13:12:23,725 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 13:12:23,726 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 13:12:23,726 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:12:23,726 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:12:23,726 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:12:23,726 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:12:23,726 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:12:23,726 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:12:23,726 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 13:12:23,845 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 13:12:25,357 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 13:12:25,357 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 41/41 [00:44<00:00,  1.09s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.36s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.69s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 13:13:37,726 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 13:13:37,726 >>   Num examples = 89504\n",
      "[INFO|trainer.py:1147] 2021-06-28 13:13:37,726 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 13:13:37,726 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 13:13:37,726 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 13:13:37,726 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 13:13:37,726 >>   Total optimization steps = 2797\n",
      "{'loss': 0.442, 'learning_rate': 0.0, 'epoch': 1.0}                             \n",
      "100%|███████████████████████████████████████| 2797/2797 [22:02<00:00,  2.10it/s][INFO|trainer.py:1341] 2021-06-28 13:35:40,714 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1322.9883, 'train_samples_per_second': 2.114, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 2797/2797 [22:02<00:00,  2.11it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 13:35:41,081 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 13:35:41,082 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 13:35:42,199 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 13:35:42,200 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 13:35:42,200 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:35:42,299 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,299 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,299 >>   init_mem_cpu_alloc_delta   =      506MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,299 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   train_mem_cpu_alloc_delta  =      802MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   train_mem_cpu_peaked_delta =        2MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   train_runtime              = 0:22:02.98\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   train_samples              =      89504\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:35:42,300 >>   train_samples_per_second   =      2.114\n",
      "06/28/2021 13:35:42 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 13:35:42,301 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 13:35:42,302 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 13:35:42,302 >>   Num examples = 6175\n",
      "[INFO|trainer.py:2120] 2021-06-28 13:35:42,302 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 193/193 [00:25<00:00,  7.55it/s]06/28/2021 13:36:14 - INFO - utils_qa -   Post-processing 202 example predictions split into 6175 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:17, 11.49it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:13, 14.56it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:13, 14.86it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:11, 16.28it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 17.48it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:09, 18.68it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 20.22it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:09, 19.68it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 20.13it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 22.21it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:07, 21.60it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:08, 20.27it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 18.75it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:02<00:08, 19.32it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:02<00:08, 19.21it/s]\u001b[A\n",
      " 22%|█████████▎                                | 45/202 [00:02<00:09, 16.30it/s]\u001b[A\n",
      " 24%|█████████▉                                | 48/202 [00:02<00:08, 18.70it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 18.19it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 18.42it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:08, 16.66it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:02<00:08, 16.99it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:06, 20.51it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.81it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.18it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 15.10it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:08, 14.87it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:08, 15.15it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 78/202 [00:04<00:07, 17.07it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:08, 14.00it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 14.20it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:08, 14.49it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:09, 12.61it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 11.66it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:10, 11.04it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 13.01it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:08, 13.08it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.30it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:08, 12.42it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 12.18it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 13.18it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.59it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 11.94it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 11.64it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:08, 11.18it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:07, 11.09it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08, 10.58it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08,  9.57it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:07<00:08,  9.39it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:07<00:07, 11.15it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:07<00:06, 12.64it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:07<00:05, 14.08it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:05, 13.47it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 11.55it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:08<00:06, 10.64it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:07,  8.82it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:08,  8.22it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:09<00:08,  7.73it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:08,  7.42it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:08,  7.15it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.06it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  6.91it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:08,  6.90it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:08,  6.87it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  6.70it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:10<00:06,  8.31it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:10<00:05,  9.98it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:10<00:03, 12.73it/s]\u001b[A\n",
      " 76%|███████████████████████████████▎         | 154/202 [00:10<00:03, 13.48it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:10<00:03, 14.87it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 14.57it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:02, 14.04it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:02, 13.85it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 13.87it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 13.83it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 13.68it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:03, 10.14it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  8.53it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  7.75it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:12<00:02, 10.07it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:13<00:01, 12.77it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:13<00:00, 15.38it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:13<00:00, 14.49it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:13<00:00, 16.84it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:13<00:00, 18.88it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.58it/s]\u001b[A\n",
      "06/28/2021 13:36:28 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/eval_predictions.json.\n",
      "06/28/2021 13:36:28 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 193/193 [00:46<00:00,  4.17it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:36:28,749 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:36:28,749 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:36:28,749 >>   eval_exact_match = 41.5842\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:36:28,750 >>   eval_f1          = 67.1284\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:36:28,750 >>   eval_samples     =    6175\n",
      "06/28/2021 13:36:28 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 13:36:28,785 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 13:36:28,789 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 13:36:28,789 >>   Num examples = 6268\n",
      "[INFO|trainer.py:2120] 2021-06-28 13:36:28,789 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 196/196 [00:26<00:00,  7.72it/s]06/28/2021 13:37:01 - INFO - utils_qa -   Post-processing 202 example predictions split into 6268 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:16, 12.38it/s]\u001b[A\n",
      "  2%|▊                                          | 4/202 [00:00<00:14, 13.69it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:12, 16.02it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:12, 15.75it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:11, 17.19it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:10, 18.15it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:09, 19.17it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:08, 22.11it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:08, 20.94it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/202 [00:01<00:07, 22.80it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:08, 21.19it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 19.56it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:01<00:08, 19.43it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:01<00:08, 20.04it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:02<00:08, 19.32it/s]\u001b[A\n",
      " 22%|█████████▎                                | 45/202 [00:02<00:09, 17.24it/s]\u001b[A\n",
      " 23%|█████████▊                                | 47/202 [00:02<00:08, 17.98it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 17.56it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 17.98it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 17.68it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 17.93it/s]\u001b[A\n",
      " 29%|████████████▎                             | 59/202 [00:03<00:07, 18.42it/s]\u001b[A\n",
      " 31%|█████████████                             | 63/202 [00:03<00:06, 21.82it/s]\u001b[A\n",
      " 33%|█████████████▋                            | 66/202 [00:03<00:06, 19.78it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 69/202 [00:03<00:08, 16.48it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:09, 14.52it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:09, 13.50it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:04<00:08, 14.29it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:04<00:08, 15.06it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:07, 15.86it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 13.72it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:08, 14.72it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:09, 12.79it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:09, 11.76it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:10, 11.06it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 13.18it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 13.29it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.55it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:08, 12.65it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 12.44it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 13.45it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.62it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 11.98it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 11.65it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.38it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:07, 11.30it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08, 10.74it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08, 10.29it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:08,  9.64it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:07<00:07, 11.31it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:08<00:06, 11.50it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:08<00:05, 13.03it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:06, 11.83it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:08<00:06, 10.67it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:05, 11.62it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:06,  9.64it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.55it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:07,  8.03it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.63it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.38it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.22it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:08,  7.07it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  7.03it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:10<00:08,  6.93it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:10<00:06,  8.35it/s]\u001b[A\n",
      " 74%|██████████████████████████████▍          | 150/202 [00:10<00:04, 10.58it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:10<00:04, 12.18it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:10<00:03, 14.45it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 14.35it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:02, 14.41it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:02, 14.58it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 14.33it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 14.28it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 14.12it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:02, 11.60it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  9.39it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  8.33it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:12<00:02,  9.40it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:12<00:01, 11.97it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:12<00:01, 15.44it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:13<00:00, 15.01it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:13<00:00, 14.33it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:13<00:00, 16.64it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.64it/s]\u001b[A\n",
      "06/28/2021 13:37:15 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/predict_predictions.json.\n",
      "06/28/2021 13:37:15 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:37:15,295 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:37:15,295 >>   predict_samples  =    6268\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:37:15,295 >>   test_exact_match = 37.1287\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:37:15,295 >>   test_f1          = 66.6269\n",
      "100%|█████████████████████████████████████████| 196/196 [00:46<00:00,  4.22it/s]\n",
      "06/28/2021 13:37:21 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 13:37:21 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_13-37-20_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 21249\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 13:37:21,578 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 13:37:21,578 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 13:37:21,579 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 13:37:21,579 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 13:37:21,579 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:37:21,580 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:37:21,580 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:37:21,580 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:37:21,580 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:37:21,580 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:37:21,580 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 13:37:21,697 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 13:37:23,675 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 13:37:23,675 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 22/22 [00:40<00:00,  1.84s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.70s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.04s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 13:38:33,084 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 13:38:33,084 >>   Num examples = 69628\n",
      "[INFO|trainer.py:1147] 2021-06-28 13:38:33,084 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 13:38:33,084 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 13:38:33,084 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 13:38:33,084 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 13:38:33,084 >>   Total optimization steps = 2176\n",
      "{'loss': 0.3026, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 2176/2176 [17:12<00:00,  2.12it/s][INFO|trainer.py:1341] 2021-06-28 13:55:45,661 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1032.5778, 'train_samples_per_second': 2.107, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 2176/2176 [17:12<00:00,  2.11it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 13:55:45,959 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 13:55:45,960 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 13:55:46,963 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 13:55:46,964 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 13:55:46,964 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:55:47,053 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   init_mem_cpu_alloc_delta   =      460MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   train_mem_cpu_alloc_delta  =      773MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   train_mem_cpu_peaked_delta =        1MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   train_runtime              = 0:17:12.57\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   train_samples              =      69628\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:55:47,053 >>   train_samples_per_second   =      2.107\n",
      "06/28/2021 13:55:47 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 13:55:47,054 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 13:55:47,056 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 13:55:47,056 >>   Num examples = 6175\n",
      "[INFO|trainer.py:2120] 2021-06-28 13:55:47,056 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 193/193 [00:25<00:00,  7.54it/s]06/28/2021 13:56:18 - INFO - utils_qa -   Post-processing 202 example predictions split into 6175 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:15, 13.04it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:12, 16.06it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:12, 16.02it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:10, 17.56it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 18.20it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:09, 19.58it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 21.14it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:08, 20.35it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 21.17it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 23.13it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:07, 22.82it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:07, 21.69it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 19.99it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:07, 20.58it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 19.22it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 18.86it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:07, 20.13it/s]\u001b[A\n",
      " 26%|██████████▊                               | 52/202 [00:02<00:07, 20.06it/s]\u001b[A\n",
      " 27%|███████████▍                              | 55/202 [00:02<00:07, 19.14it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 17.25it/s]\u001b[A\n",
      " 30%|████████████▋                             | 61/202 [00:02<00:06, 20.54it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 22.73it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.85it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 15.56it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:08, 15.28it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:08, 15.78it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 78/202 [00:03<00:06, 17.80it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:08, 15.04it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:07, 15.06it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 15.42it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 13.29it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 12.19it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:04<00:09, 11.36it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 13.48it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 13.47it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.92it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:07, 13.05it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:05<00:07, 12.75it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:05<00:07, 13.95it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.98it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 12.48it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 12.04it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.81it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:06<00:07, 11.71it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:06<00:07, 11.15it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08, 10.29it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:07, 10.76it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:07<00:06, 12.49it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:07<00:05, 14.10it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:07<00:05, 13.70it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:07<00:06, 12.10it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:08<00:05, 12.72it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:07,  9.09it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:08,  8.36it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:08<00:08,  7.96it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:08<00:08,  7.61it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:08,  7.43it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.33it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.20it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.16it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:08,  7.04it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:09<00:08,  7.03it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:09<00:06,  8.68it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:10<00:05, 10.43it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:10<00:03, 13.33it/s]\u001b[A\n",
      " 77%|███████████████████████████████▍         | 155/202 [00:10<00:03, 15.01it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:10<00:02, 14.94it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:10<00:02, 14.90it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:10<00:02, 14.52it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:10<00:02, 14.53it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 14.38it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 14.11it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:11<00:03, 10.47it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:11<00:03,  8.92it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  8.13it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:12<00:02, 10.58it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:12<00:01, 13.47it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:12<00:00, 16.00it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:12<00:00, 15.24it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:12<00:00, 17.65it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:12<00:00, 19.97it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 15.34it/s]\u001b[A\n",
      "06/28/2021 13:56:32 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/eval_predictions.json.\n",
      "06/28/2021 13:56:32 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 193/193 [00:45<00:00,  4.27it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:56:32,408 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:56:32,408 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:56:32,408 >>   eval_exact_match =  39.604\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:56:32,408 >>   eval_f1          = 66.3252\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:56:32,408 >>   eval_samples     =    6175\n",
      "06/28/2021 13:56:32 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 13:56:32,410 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 13:56:32,413 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 13:56:32,414 >>   Num examples = 6268\n",
      "[INFO|trainer.py:2120] 2021-06-28 13:56:32,414 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 196/196 [00:26<00:00,  7.72it/s]06/28/2021 13:57:04 - INFO - utils_qa -   Post-processing 202 example predictions split into 6268 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:15, 12.75it/s]\u001b[A\n",
      "  2%|▊                                          | 4/202 [00:00<00:13, 14.24it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:11, 16.50it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:11, 16.11it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:10, 17.72it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:10, 18.51it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:09, 19.72it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:07, 23.08it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:08, 21.55it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:08, 20.39it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:08, 18.95it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:09, 18.37it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 19.50it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:02<00:07, 20.40it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 18.75it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:09, 16.47it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 17.85it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 18.15it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 18.24it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:07, 18.40it/s]\u001b[A\n",
      " 29%|████████████▎                             | 59/202 [00:03<00:07, 18.56it/s]\u001b[A\n",
      " 31%|█████████████                             | 63/202 [00:03<00:06, 22.10it/s]\u001b[A\n",
      " 33%|█████████████▋                            | 66/202 [00:03<00:06, 19.88it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 69/202 [00:03<00:07, 16.73it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 14.89it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:09, 13.85it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:04<00:08, 14.50it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:04<00:08, 15.60it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:07, 16.51it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 14.29it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 15.16it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 13.13it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 12.09it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:09, 11.33it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 13.50it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 13.51it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.75it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:07, 12.86it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:05<00:07, 12.59it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 13.61it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.96it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 12.31it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 12.02it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.70it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:06<00:07, 11.58it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:07, 11.12it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:07, 10.67it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:08, 10.07it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:07<00:07, 11.38it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:07<00:06, 12.92it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:07<00:05, 14.41it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:05, 12.91it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:08<00:06, 11.43it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:05, 12.38it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:06, 10.34it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  9.06it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:07,  8.38it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:07,  7.87it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:09,  6.71it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:08,  6.87it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  6.92it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:10<00:08,  7.01it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:10<00:06,  8.44it/s]\u001b[A\n",
      " 74%|██████████████████████████████▍          | 150/202 [00:10<00:04, 10.88it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:10<00:03, 12.65it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:10<00:03, 14.75it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:10<00:02, 14.73it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:10<00:02, 14.42it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:02, 14.56it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 14.59it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 14.43it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 14.70it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:11<00:02, 12.03it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  9.61it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  8.53it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:12<00:02,  9.69it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:12<00:01, 12.35it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:12<00:01, 15.84it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:12<00:00, 15.56it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:13<00:00, 14.83it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:13<00:00, 17.37it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 15.06it/s]\u001b[A\n",
      "06/28/2021 13:57:18 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/predict_predictions.json.\n",
      "06/28/2021 13:57:18 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 13:57:18,267 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:57:18,268 >>   predict_samples  =    6268\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:57:18,268 >>   test_exact_match = 35.1485\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 13:57:18,268 >>   test_f1          = 66.6788\n",
      "100%|█████████████████████████████████████████| 196/196 [00:45<00:00,  4.27it/s]\n",
      "06/28/2021 13:57:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 13:57:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_13-57-23_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 21249\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 13:57:24,022 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 13:57:24,023 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 13:57:24,024 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 13:57:24,025 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 13:57:24,025 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:57:24,026 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:57:24,026 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:57:24,026 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:57:24,026 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:57:24,026 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 13:57:24,026 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 13:57:24,156 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 13:57:25,707 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 13:57:25,709 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 22/22 [00:37<00:00,  1.70s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.68s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.37s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 13:58:32,099 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 13:58:32,099 >>   Num examples = 69628\n",
      "[INFO|trainer.py:1147] 2021-06-28 13:58:32,099 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 13:58:32,099 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 13:58:32,099 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 13:58:32,099 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 13:58:32,099 >>   Total optimization steps = 2176\n",
      "{'loss': 0.2101, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 2176/2176 [17:08<00:00,  2.17it/s][INFO|trainer.py:1341] 2021-06-28 14:15:40,570 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1028.4707, 'train_samples_per_second': 2.116, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 2176/2176 [17:08<00:00,  2.12it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 14:15:40,876 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 14:15:40,878 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 14:15:42,021 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 14:15:42,022 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 14:15:42,022 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 14:15:42,119 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   init_mem_cpu_alloc_delta   =      461MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   train_mem_cpu_alloc_delta  =      790MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   train_mem_cpu_peaked_delta =        1MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   train_runtime              = 0:17:08.47\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   train_samples              =      69628\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:15:42,120 >>   train_samples_per_second   =      2.116\n",
      "06/28/2021 14:15:42 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 14:15:42,121 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 14:15:42,122 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 14:15:42,122 >>   Num examples = 6175\n",
      "[INFO|trainer.py:2120] 2021-06-28 14:15:42,122 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 193/193 [00:25<00:00,  7.52it/s]06/28/2021 14:16:14 - INFO - utils_qa -   Post-processing 202 example predictions split into 6175 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:16, 12.15it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:12, 15.29it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:12, 15.22it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:11, 16.85it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 17.80it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:09, 19.20it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 20.91it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:08, 20.26it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 20.99it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 23.00it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:07, 22.48it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:07, 21.18it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 19.43it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:08, 20.11it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 18.87it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 18.54it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:07, 19.62it/s]\u001b[A\n",
      " 26%|██████████▊                               | 52/202 [00:02<00:07, 19.57it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:07, 19.27it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:08, 17.01it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:02<00:08, 17.56it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:02<00:06, 21.08it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.91it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.50it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 15.49it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:08, 15.15it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:08, 15.48it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 78/202 [00:03<00:07, 17.50it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:08, 14.30it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 14.60it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 15.08it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 13.10it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 12.04it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:09, 11.46it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:07, 13.55it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:08, 13.10it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.43it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:08, 12.68it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:05<00:08, 12.33it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:05<00:07, 13.54it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.69it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 12.21it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 11.72it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.45it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:06<00:07, 11.08it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08, 10.65it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08,  9.89it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:07, 10.44it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:07<00:06, 12.21it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:07<00:06, 11.73it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:07<00:06, 12.01it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 10.72it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:08<00:06, 11.58it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:07,  9.59it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:07,  8.61it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:09<00:08,  8.12it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:08,  7.67it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:08,  7.47it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.27it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.10it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.03it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:08,  6.93it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  6.97it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:10<00:06,  8.53it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:10<00:05, 10.33it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:10<00:03, 13.17it/s]\u001b[A\n",
      " 77%|███████████████████████████████▍         | 155/202 [00:10<00:03, 14.57it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:10<00:03, 14.45it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:10<00:02, 14.02it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:02, 13.97it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 13.90it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 14.08it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 13.76it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:11<00:03, 10.39it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  8.77it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  7.93it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:12<00:02, 10.28it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:12<00:01, 13.09it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:12<00:00, 15.62it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:13<00:00, 14.85it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:13<00:00, 15.48it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:13<00:00, 17.73it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.86it/s]\u001b[A\n",
      "06/28/2021 14:16:27 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4/eval_predictions.json.\n",
      "06/28/2021 14:16:27 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 193/193 [00:45<00:00,  4.21it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 14:16:28,145 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:16:28,145 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:16:28,145 >>   eval_exact_match = 39.1089\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:16:28,145 >>   eval_f1          = 65.9648\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:16:28,145 >>   eval_samples     =    6175\n",
      "06/28/2021 14:16:28 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 14:16:28,194 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 14:16:28,197 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 14:16:28,197 >>   Num examples = 6268\n",
      "[INFO|trainer.py:2120] 2021-06-28 14:16:28,198 >>   Batch size = 32\n",
      "100%|█████████████████████████████████████████| 196/196 [00:26<00:00,  7.71it/s]06/28/2021 14:17:00 - INFO - utils_qa -   Post-processing 202 example predictions split into 6268 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:15, 12.66it/s]\u001b[A\n",
      "  2%|▊                                          | 4/202 [00:00<00:14, 14.08it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:11, 16.25it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:12, 15.84it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:10, 17.50it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:10, 18.43it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:09, 19.58it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:07, 22.78it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:08, 21.63it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 23.40it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:08, 20.76it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:08, 19.58it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 20.21it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:07, 20.85it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 18.71it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:09, 16.32it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 17.78it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 18.17it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 18.13it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 18.10it/s]\u001b[A\n",
      " 29%|████████████▎                             | 59/202 [00:02<00:07, 18.49it/s]\u001b[A\n",
      " 31%|█████████████                             | 63/202 [00:03<00:06, 21.93it/s]\u001b[A\n",
      " 33%|█████████████▋                            | 66/202 [00:03<00:06, 19.75it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 69/202 [00:03<00:08, 16.62it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 15.05it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:09, 13.89it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:08, 14.60it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:04<00:08, 15.62it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:07, 16.22it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 14.19it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 14.89it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 13.27it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 12.14it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:09, 11.60it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:07, 13.70it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 13.82it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.71it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:07, 12.78it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:05<00:08, 12.45it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 13.63it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.74it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 12.35it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 11.86it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.65it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:06<00:07, 11.38it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08,  9.61it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08,  9.58it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:08,  9.38it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:07<00:07, 11.15it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:07<00:06, 12.79it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:07<00:05, 14.27it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:05, 12.93it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:08<00:06, 11.30it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:05, 12.14it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:06,  9.99it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.83it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:07,  8.23it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:07,  7.77it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.39it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:08,  7.27it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:08,  7.14it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:10<00:08,  7.18it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:10<00:06,  8.67it/s]\u001b[A\n",
      " 74%|██████████████████████████████▍          | 150/202 [00:10<00:04, 11.10it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:10<00:03, 12.77it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:10<00:03, 14.85it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:10<00:02, 14.73it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:10<00:02, 14.60it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:02, 14.52it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 14.72it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 14.66it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 14.78it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:11<00:02, 11.99it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  9.53it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  8.38it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:12<00:02,  9.48it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:12<00:01, 12.09it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:12<00:01, 13.55it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:12<00:00, 16.14it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:13<00:00, 13.87it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:13<00:00, 16.46it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:13<00:00, 18.48it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.93it/s]\u001b[A\n",
      "06/28/2021 14:17:14 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4/predict_predictions.json.\n",
      "06/28/2021 14:17:14 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 14:17:14,330 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:17:14,330 >>   predict_samples  =    6268\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:17:14,330 >>   test_exact_match = 33.6634\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:17:14,330 >>   test_f1          = 66.5441\n",
      "100%|█████████████████████████████████████████| 196/196 [00:46<00:00,  4.25it/s]\n",
      "06/28/2021 14:17:20 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 14:17:20 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_14-17-20_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 80151\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:517] 2021-06-28 14:17:21,228 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 14:17:21,230 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-06-28 14:17:21,503 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 14:17:21,504 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 14:17:23,129 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /soe/meidam/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 14:17:23,129 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /soe/meidam/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 14:17:23,130 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /soe/meidam/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 14:17:23,130 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 14:17:23,130 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 14:17:23,130 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1155] 2021-06-28 14:17:23,550 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /soe/meidam/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1330] 2021-06-28 14:17:25,374 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1341] 2021-06-28 14:17:25,376 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/28/2021 14:17:25 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train/cache-226974c00eed5f0c.arrow\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.14s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.86s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 14:17:53,232 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 14:17:53,232 >>   Num examples = 129276\n",
      "[INFO|trainer.py:1147] 2021-06-28 14:17:53,232 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 14:17:53,232 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 14:17:53,232 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 14:17:53,232 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 14:17:53,232 >>   Total optimization steps = 4040\n",
      "{'loss': 0.8703, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 4040/4040 [31:40<00:00,  2.16it/s][INFO|trainer.py:1341] 2021-06-28 14:49:33,813 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1900.5805, 'train_samples_per_second': 2.126, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 4040/4040 [31:40<00:00,  2.13it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 14:49:34,251 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 14:49:34,252 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 14:49:35,417 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 14:49:35,417 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 14:49:35,418 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 14:49:35,511 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   init_mem_cpu_alloc_delta   =     1442MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   init_mem_cpu_peaked_delta  =      355MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   train_mem_cpu_alloc_delta  =      893MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   train_mem_cpu_peaked_delta =        3MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   train_runtime              = 0:31:40.58\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   train_samples              =     129276\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:49:35,511 >>   train_samples_per_second   =      2.126\n",
      "06/28/2021 14:49:35 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 14:49:35,512 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 14:49:35,514 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 14:49:35,514 >>   Num examples = 6210\n",
      "[INFO|trainer.py:2120] 2021-06-28 14:49:35,514 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 194/195 [00:26<00:00,  7.41it/s]06/28/2021 14:50:08 - INFO - utils_qa -   Post-processing 202 example predictions split into 6210 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:18, 11.06it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:14, 14.00it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:13, 14.02it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:12, 15.64it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:11, 16.49it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:10, 17.94it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 20.32it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:09, 19.13it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 19.76it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:08, 21.53it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:07, 21.29it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:07, 20.90it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 20.03it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:07, 20.16it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 18.53it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 17.85it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 18.85it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 18.02it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:07, 20.36it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 17.18it/s]\u001b[A\n",
      " 30%|████████████▋                             | 61/202 [00:03<00:06, 20.19it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.81it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.15it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:09, 14.46it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:09, 13.87it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:04<00:08, 14.39it/s]\u001b[A\n",
      " 39%|████████████████▏                         | 78/202 [00:04<00:07, 16.40it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:09, 13.36it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 13.61it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:09, 12.30it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:10, 11.08it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:10, 10.43it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:11, 10.05it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 93/202 [00:05<00:08, 12.50it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:05<00:08, 12.52it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:05<00:08, 12.65it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:05<00:08, 12.72it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:08, 11.25it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:08, 12.30it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:07, 12.19it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:06<00:08, 11.55it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:06<00:08, 11.11it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:07<00:08, 10.69it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:08, 10.57it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:07<00:09,  9.18it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:09,  8.85it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:07<00:10,  8.50it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:10,  8.28it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:09,  8.42it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:08<00:08,  9.95it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:08<00:06, 11.49it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:08<00:05, 12.91it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:06, 12.47it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 10.73it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:08<00:06, 11.35it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:07,  9.09it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:08,  8.04it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:09<00:08,  7.57it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:09,  7.10it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:09,  6.91it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:09,  6.63it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:09,  6.64it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:09,  6.49it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:09,  6.44it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:09,  6.34it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:10<00:07,  7.80it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:05,  9.50it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:11<00:04, 11.77it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:11<00:04, 10.97it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:11<00:03, 12.87it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 12.98it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:03, 12.99it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 13.09it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:02, 13.32it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:12<00:02, 13.16it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:12<00:02, 12.80it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:03,  8.77it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  7.70it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:13<00:04,  7.12it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:04,  6.80it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:13<00:02,  8.88it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:13<00:01, 11.41it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 186/202 [00:13<00:01, 13.50it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▎  | 189/202 [00:14<00:01, 12.05it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▉  | 192/202 [00:14<00:00, 14.27it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▌ | 195/202 [00:14<00:00, 16.31it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:14<00:00, 13.74it/s]\u001b[A\n",
      "06/28/2021 14:50:23 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/eval_predictions.json.\n",
      "06/28/2021 14:50:23 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 195/195 [00:47<00:00,  4.07it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 14:50:23,631 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:50:23,631 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:50:23,632 >>   eval_exact_match = 31.1881\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:50:23,632 >>   eval_f1          = 56.0899\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:50:23,632 >>   eval_samples     =    6210\n",
      "06/28/2021 14:50:23 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 14:50:23,675 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 14:50:23,680 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 14:50:23,680 >>   Num examples = 6190\n",
      "[INFO|trainer.py:2120] 2021-06-28 14:50:23,681 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 193/194 [00:25<00:00,  7.45it/s]06/28/2021 14:50:56 - INFO - utils_qa -   Post-processing 202 example predictions split into 6190 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:17, 11.13it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:13, 14.09it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:11, 16.65it/s]\u001b[A\n",
      "  6%|██▍                                       | 12/202 [00:00<00:10, 17.67it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 18.17it/s]\u001b[A\n",
      "  8%|███▎                                      | 16/202 [00:00<00:10, 18.59it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:08, 21.38it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:00<00:08, 21.86it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 20.90it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:07, 21.82it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:07, 21.70it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:08, 20.27it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 19.70it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:01<00:08, 19.87it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 17.89it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:10, 15.03it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:09, 16.59it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 16.98it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 16.83it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:09, 14.93it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:02<00:09, 15.64it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:07, 19.02it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 20.39it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:08, 16.10it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:09, 14.02it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:03<00:09, 13.03it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:09, 12.96it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:08, 14.05it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:07, 16.09it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:09, 13.35it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:04<00:08, 13.59it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:04<00:09, 12.29it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:10, 11.04it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:11,  9.43it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:05<00:10, 10.45it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 12.36it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:08, 12.48it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:08, 12.66it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:08, 11.71it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 11.42it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 12.25it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:08, 11.65it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:08, 11.13it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:08, 10.95it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:07<00:08, 10.62it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:08, 10.45it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08,  9.95it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:09,  9.18it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:09,  8.71it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:08<00:09,  8.61it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:08<00:07, 10.24it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:08<00:06, 11.85it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:08<00:05, 12.93it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:06, 11.67it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:08<00:07, 10.22it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:06, 10.87it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:07,  9.10it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:08,  8.11it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:08,  7.37it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:08,  7.09it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:09,  6.74it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:09,  6.66it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:09,  6.50it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:09,  6.45it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:10<00:10,  5.71it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:11<00:07,  7.02it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:06,  8.57it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:11<00:04, 10.52it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:11<00:04, 11.85it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:11<00:03, 13.71it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 13.56it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:03, 13.39it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 13.30it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:02, 13.41it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:12<00:02, 13.33it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:12<00:02, 13.26it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:02, 10.82it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  8.66it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:03,  7.67it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:13<00:02,  8.72it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 179/202 [00:13<00:02, 11.04it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 183/202 [00:13<00:01, 14.03it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 187/202 [00:13<00:00, 16.10it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 190/202 [00:14<00:00, 13.27it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 193/202 [00:14<00:00, 15.60it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▊ | 196/202 [00:14<00:00, 17.49it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:14<00:00, 13.79it/s]\u001b[A\n",
      "06/28/2021 14:51:10 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/predict_predictions.json.\n",
      "06/28/2021 14:51:10 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 14:51:11,253 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:51:11,253 >>   predict_samples  =    6190\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:51:11,253 >>   test_exact_match = 36.1386\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 14:51:11,253 >>   test_f1          = 60.6174\n",
      "100%|█████████████████████████████████████████| 194/194 [00:47<00:00,  4.08it/s]\n",
      "06/28/2021 14:51:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 14:51:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_14-51-16_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 60517\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 14:51:16,992 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 14:51:16,994 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 14:51:16,995 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 14:51:16,996 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 14:51:16,996 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 14:51:16,996 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 14:51:16,996 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 14:51:16,997 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 14:51:16,997 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 14:51:16,997 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 14:51:16,997 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 14:51:17,139 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 14:51:18,779 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 14:51:18,779 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 61/61 [00:51<00:00,  1.19ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.78s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.66s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 14:52:36,489 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 14:52:36,489 >>   Num examples = 109409\n",
      "[INFO|trainer.py:1147] 2021-06-28 14:52:36,489 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 14:52:36,489 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 14:52:36,489 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 14:52:36,489 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 14:52:36,489 >>   Total optimization steps = 3420\n",
      " 93%|████████████████████████████████████▏  | 3169/3420 [24:51<01:58,  2.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████▏               | 2403/4037 [22:07<14:33,  1.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8613, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 4037/4037 [37:04<00:00,  2.24it/s][INFO|trainer.py:1341] 2021-06-28 17:03:57,096 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2224.1097, 'train_samples_per_second': 1.815, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 4037/4037 [37:04<00:00,  1.82it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 17:03:57,530 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 17:03:57,531 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 17:03:58,681 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 17:03:58,682 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 17:03:58,682 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 17:03:58,780 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   init_mem_cpu_alloc_delta   =     1440MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   init_mem_cpu_peaked_delta  =      355MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   train_mem_cpu_alloc_delta  =      891MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   train_mem_cpu_peaked_delta =        3MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   train_runtime              = 0:37:04.10\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   train_samples              =     129154\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:03:58,780 >>   train_samples_per_second   =      1.815\n",
      "06/28/2021 17:03:58 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 17:03:58,781 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 17:03:58,783 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 17:03:58,783 >>   Num examples = 6274\n",
      "[INFO|trainer.py:2120] 2021-06-28 17:03:58,783 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 196/197 [00:26<00:00,  7.42it/s]06/28/2021 17:04:31 - INFO - utils_qa -   Post-processing 202 example predictions split into 6274 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:18, 10.95it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:14, 13.90it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:13, 14.06it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:12, 15.62it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:11, 16.63it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:10, 17.49it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:09, 19.79it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:09, 19.70it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 20.25it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:08, 20.12it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:08, 20.26it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:08, 20.03it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 19.36it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:02<00:07, 20.35it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 18.41it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 17.70it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 18.51it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 18.05it/s]\u001b[A\n",
      " 26%|███████████                               | 53/202 [00:02<00:08, 17.02it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:08, 17.02it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:08, 17.23it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:06, 20.45it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.35it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:08, 16.48it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:09, 14.24it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:03<00:10, 12.98it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:09, 13.63it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:08, 14.39it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:08, 14.87it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:09, 12.58it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:04<00:08, 13.47it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:04<00:10, 11.65it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:10, 10.74it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:11, 10.17it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:05<00:10, 11.05it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 12.77it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:08, 12.97it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:08, 12.96it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:08, 12.32it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 11.80it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:08, 12.01it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:08, 11.50it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:08, 10.97it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:08, 10.65it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:07<00:09,  9.31it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:09,  9.44it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:09,  9.55it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:09,  9.39it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:07<00:09,  9.06it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:08<00:09,  8.55it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:09,  8.51it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:08<00:08, 10.07it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:08<00:06, 11.55it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:08<00:06, 12.77it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:06, 12.37it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 10.59it/s]\u001b[A\n",
      " 65%|██████████████████████████▊              | 132/202 [00:09<00:06, 11.43it/s]\u001b[A\n",
      " 66%|███████████████████████████▏             | 134/202 [00:09<00:07,  9.26it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:09<00:08,  8.11it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:08,  7.42it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:08,  7.16it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:09,  6.95it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:09,  6.77it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:09,  6.60it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:10,  5.83it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:09,  5.95it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:11<00:07,  7.29it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:06,  8.51it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:11<00:04, 10.80it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:11<00:04, 10.27it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:11<00:03, 12.29it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 12.63it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 12.58it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 12.71it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:02, 12.79it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:12<00:02, 12.93it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:12<00:02, 12.58it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:03,  9.50it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  8.02it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:13<00:03,  7.37it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:04,  6.97it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:13<00:02,  9.11it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:13<00:01, 11.82it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 186/202 [00:14<00:01, 13.88it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▎  | 189/202 [00:14<00:01, 12.08it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▉  | 192/202 [00:14<00:00, 14.48it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▌ | 195/202 [00:14<00:00, 16.59it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▏| 198/202 [00:14<00:00, 16.85it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:14<00:00, 13.57it/s]\u001b[A\n",
      "06/28/2021 17:04:46 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/eval_predictions.json.\n",
      "06/28/2021 17:04:46 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 197/197 [00:48<00:00,  4.09it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 17:04:47,081 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:04:47,081 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:04:47,081 >>   eval_exact_match = 33.1683\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:04:47,081 >>   eval_f1          = 58.2389\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:04:47,082 >>   eval_samples     =    6274\n",
      "06/28/2021 17:04:47 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 17:04:47,133 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 17:04:47,136 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 17:04:47,137 >>   Num examples = 6248\n",
      "[INFO|trainer.py:2120] 2021-06-28 17:04:47,137 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:25<00:00,  7.45it/s]06/28/2021 17:05:19 - INFO - utils_qa -   Post-processing 202 example predictions split into 6248 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:16, 11.89it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:13, 14.89it/s]\u001b[A\n",
      "  5%|██                                        | 10/202 [00:00<00:11, 17.43it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:10, 18.39it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:10, 18.49it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:08, 20.52it/s]\u001b[A\n",
      " 10%|████▎                                     | 21/202 [00:00<00:08, 22.25it/s]\u001b[A\n",
      " 12%|████▉                                     | 24/202 [00:01<00:09, 18.88it/s]\u001b[A\n",
      " 13%|█████▌                                    | 27/202 [00:01<00:08, 20.07it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:07, 22.10it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:07, 21.08it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:01<00:08, 19.37it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:01<00:07, 21.34it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:01<00:07, 20.31it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:09, 17.31it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 18.81it/s]\u001b[A\n",
      " 26%|██████████▊                               | 52/202 [00:02<00:07, 19.30it/s]\u001b[A\n",
      " 27%|███████████▍                              | 55/202 [00:02<00:08, 17.45it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 16.38it/s]\u001b[A\n",
      " 30%|████████████▋                             | 61/202 [00:02<00:07, 19.48it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.77it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 17.31it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:03<00:08, 15.29it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:03<00:09, 14.12it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:03<00:08, 14.86it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:03<00:07, 15.87it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:07, 16.65it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:04<00:08, 14.33it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:04<00:07, 15.28it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:04<00:08, 13.10it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:04<00:09, 12.14it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:09, 11.48it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:07, 13.64it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:07, 13.69it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.82it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:05<00:07, 12.89it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:05<00:07, 12.65it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:05<00:07, 13.64it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:07, 12.60it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:07, 12.03it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:06<00:07, 11.56it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:06<00:07, 11.37it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:06<00:07, 11.16it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08, 10.69it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08, 10.05it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:07<00:08,  9.13it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:07<00:07, 10.91it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:07<00:06, 12.41it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:07<00:05, 13.87it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:06, 11.88it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:08<00:06, 10.73it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:08<00:05, 11.64it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:08<00:06,  9.76it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:07,  8.81it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:07,  8.09it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:09<00:08,  7.76it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:09<00:08,  7.49it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:09<00:08,  7.33it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:09<00:08,  6.91it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:09<00:08,  6.94it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:10<00:08,  6.97it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:10<00:06,  8.36it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:10<00:05, 10.11it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:10<00:04, 11.49it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:10<00:03, 12.87it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:10<00:03, 15.10it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:10<00:02, 14.71it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:02, 14.58it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:11<00:02, 14.55it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:11<00:02, 14.34it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:11<00:02, 14.25it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:11<00:02, 14.12it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:11<00:02, 11.89it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:12<00:03,  9.53it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:12<00:03,  8.46it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:12<00:02,  9.50it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:12<00:01, 12.08it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:12<00:01, 15.54it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:13<00:01, 13.88it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:13<00:00, 13.76it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:13<00:00, 16.08it/s]\u001b[A\n",
      " 98%|███████████████████████████████████████▉ | 197/202 [00:13<00:00, 18.33it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:13<00:00, 14.80it/s]\u001b[A\n",
      "06/28/2021 17:05:32 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/predict_predictions.json.\n",
      "06/28/2021 17:05:32 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 17:05:33,119 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:05:33,119 >>   predict_samples  =    6248\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:05:33,119 >>   test_exact_match = 32.1782\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 17:05:33,119 >>   test_f1          = 62.3797\n",
      "100%|█████████████████████████████████████████| 196/196 [00:45<00:00,  4.26it/s]\n",
      "06/28/2021 17:05:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 17:05:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_17-05-38_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 60517\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 17:05:39,019 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 17:05:39,021 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 17:05:39,021 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 17:05:39,022 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 17:05:39,023 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 17:05:39,023 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 17:05:39,023 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 17:05:39,023 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 17:05:39,023 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 17:05:39,023 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 17:05:39,023 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 17:05:39,157 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 17:05:40,910 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 17:05:40,910 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 61/61 [00:52<00:00,  1.16ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.73s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.79s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 17:07:00,253 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 17:07:00,253 >>   Num examples = 109287\n",
      "[INFO|trainer.py:1147] 2021-06-28 17:07:00,253 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 17:07:00,253 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 17:07:00,253 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 17:07:00,254 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 17:07:00,254 >>   Total optimization steps = 3416\n",
      " 10%|████▏                                   | 358/3416 [03:17<27:08,  1.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2997, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 2174/2174 [17:05<00:00,  2.46it/s][INFO|trainer.py:1341] 2021-06-28 18:23:29,069 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1025.7572, 'train_samples_per_second': 2.119, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 2174/2174 [17:05<00:00,  2.12it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 18:23:29,522 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 18:23:29,523 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 18:23:30,536 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 18:23:30,537 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 18:23:30,537 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 18:23:30,635 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   init_mem_cpu_alloc_delta   =      472MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   train_mem_cpu_alloc_delta  =      787MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   train_mem_cpu_peaked_delta =        1MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,635 >>   train_runtime              = 0:17:05.75\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,636 >>   train_samples              =      69549\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:23:30,636 >>   train_samples_per_second   =      2.119\n",
      "06/28/2021 18:23:30 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 18:23:30,637 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 18:23:30,638 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 18:23:30,638 >>   Num examples = 6274\n",
      "[INFO|trainer.py:2120] 2021-06-28 18:23:30,638 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 196/197 [00:26<00:00,  7.41it/s]06/28/2021 18:24:03 - INFO - utils_qa -   Post-processing 202 example predictions split into 6274 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:17, 11.73it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:13, 14.86it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:12, 15.06it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:11, 16.62it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 17.82it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:09, 18.76it/s]\u001b[A\n",
      " 10%|████▎                                     | 21/202 [00:00<00:08, 21.11it/s]\u001b[A\n",
      " 12%|████▉                                     | 24/202 [00:01<00:08, 20.44it/s]\u001b[A\n",
      " 13%|█████▌                                    | 27/202 [00:01<00:07, 22.30it/s]\u001b[A\n",
      " 15%|██████▏                                   | 30/202 [00:01<00:07, 22.89it/s]\u001b[A\n",
      " 16%|██████▊                                   | 33/202 [00:01<00:08, 20.95it/s]\u001b[A\n",
      " 18%|███████▍                                  | 36/202 [00:01<00:08, 20.67it/s]\u001b[A\n",
      " 19%|████████                                  | 39/202 [00:01<00:07, 20.81it/s]\u001b[A\n",
      " 21%|████████▋                                 | 42/202 [00:01<00:07, 20.78it/s]\u001b[A\n",
      " 22%|█████████▎                                | 45/202 [00:02<00:08, 18.46it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:07, 20.26it/s]\u001b[A\n",
      " 26%|██████████▊                               | 52/202 [00:02<00:07, 20.17it/s]\u001b[A\n",
      " 27%|███████████▍                              | 55/202 [00:02<00:07, 18.77it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:02<00:08, 16.64it/s]\u001b[A\n",
      " 30%|████████████▋                             | 61/202 [00:02<00:07, 19.80it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 22.21it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:10, 13.23it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:14,  9.01it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:04<00:16,  8.06it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:15,  8.35it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:14,  8.86it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:12, 10.12it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:12,  9.93it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:05<00:10, 11.26it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:05<00:11, 10.61it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:11, 10.09it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:11,  9.81it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:05<00:10, 10.65it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:06<00:08, 12.46it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:06<00:08, 12.58it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:06<00:08, 12.73it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:08, 12.09it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 11.74it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:08, 11.88it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:07<00:08, 11.41it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:07<00:08, 11.09it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:08, 10.97it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:07<00:08, 10.72it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:08, 10.46it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:08<00:08,  9.97it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:08<00:09,  9.21it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:09,  9.00it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:08<00:07, 10.57it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:08<00:06, 12.03it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:08<00:05, 13.34it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:09<00:07, 10.58it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:09<00:07,  9.66it/s]\u001b[A\n",
      " 65%|██████████████████████████▊              | 132/202 [00:09<00:06, 10.67it/s]\u001b[A\n",
      " 66%|███████████████████████████▏             | 134/202 [00:09<00:08,  8.09it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:10<00:08,  7.35it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:10<00:09,  6.98it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:09,  6.90it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:09,  6.73it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:09,  6.61it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:11<00:09,  6.49it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:11<00:09,  6.49it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:11<00:09,  6.45it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:11<00:07,  7.76it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:06,  8.43it/s]\u001b[A\n",
      " 74%|██████████████████████████████▍          | 150/202 [00:11<00:05, 10.04it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:12<00:04, 11.29it/s]\u001b[A\n",
      " 76%|███████████████████████████████▎         | 154/202 [00:12<00:03, 12.05it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:12<00:03, 13.57it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:12<00:03, 13.54it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 13.38it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:02, 13.40it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:02, 13.31it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:13<00:02, 13.46it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:13<00:02, 12.95it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:13<00:03,  9.66it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  8.13it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:13<00:03,  7.47it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:14<00:03,  7.01it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:14<00:02,  9.17it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:14<00:01, 11.88it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 186/202 [00:14<00:01, 13.99it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▎  | 189/202 [00:14<00:01, 12.19it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▉  | 192/202 [00:14<00:00, 14.60it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▌ | 195/202 [00:15<00:00, 16.87it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▏| 198/202 [00:15<00:00, 17.05it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 13.14it/s]\u001b[A\n",
      "06/28/2021 18:24:18 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/eval_predictions.json.\n",
      "06/28/2021 18:24:18 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 197/197 [00:48<00:00,  4.08it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 18:24:19,114 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:24:19,114 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:24:19,114 >>   eval_exact_match = 34.1584\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:24:19,114 >>   eval_f1          = 60.7224\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:24:19,114 >>   eval_samples     =    6274\n",
      "06/28/2021 18:24:19 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 18:24:19,148 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 18:24:19,152 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 18:24:19,152 >>   Num examples = 6248\n",
      "[INFO|trainer.py:2120] 2021-06-28 18:24:19,152 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:26<00:00,  7.17it/s]06/28/2021 18:24:53 - INFO - utils_qa -   Post-processing 202 example predictions split into 6248 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:17, 11.20it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:13, 14.19it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:12, 15.48it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:10, 17.77it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:10, 18.36it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:00<00:12, 14.39it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:11, 16.39it/s]\u001b[A\n",
      " 10%|████▎                                     | 21/202 [00:01<00:09, 18.35it/s]\u001b[A\n",
      " 12%|████▉                                     | 24/202 [00:01<00:09, 18.64it/s]\u001b[A\n",
      " 13%|█████▌                                    | 27/202 [00:01<00:09, 19.15it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:08, 20.82it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 19.64it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:01<00:09, 17.80it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:01<00:08, 19.67it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:02<00:08, 18.68it/s]\u001b[A\n",
      " 22%|█████████▎                                | 45/202 [00:02<00:09, 16.08it/s]\u001b[A\n",
      " 24%|█████████▉                                | 48/202 [00:02<00:09, 16.87it/s]\u001b[A\n",
      " 25%|██████████▍                               | 50/202 [00:02<00:09, 16.20it/s]\u001b[A\n",
      " 26%|███████████                               | 53/202 [00:02<00:08, 17.48it/s]\u001b[A\n",
      " 27%|███████████▍                              | 55/202 [00:02<00:09, 14.75it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:03<00:10, 14.01it/s]\u001b[A\n",
      " 30%|████████████▋                             | 61/202 [00:03<00:08, 16.79it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:07, 19.00it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:08, 15.62it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:09, 14.18it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:04<00:10, 12.86it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:09, 12.93it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:09, 13.74it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:07, 15.67it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:09, 12.93it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:04<00:08, 13.24it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:04<00:09, 12.24it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:10, 11.24it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:10, 10.59it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:05<00:09, 11.31it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 13.01it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:08, 13.05it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:06<00:08, 12.88it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:08, 12.00it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 11.78it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 12.81it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:08, 11.45it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:08, 11.08it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:08, 10.69it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:07<00:08, 10.55it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:08, 10.42it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08,  9.95it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:08,  9.39it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:09,  9.03it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:08<00:09,  8.88it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:08<00:07, 10.43it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:08<00:06, 11.77it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:08<00:05, 13.06it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:06, 11.11it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:09<00:07,  9.95it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:06, 10.88it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:07,  9.03it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:08,  8.01it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:08,  7.52it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:08,  7.13it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:08,  6.94it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:10,  5.76it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:10,  5.95it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:09,  6.07it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:11<00:09,  6.16it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:11<00:07,  7.45it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:05,  9.04it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:11<00:04, 10.43it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:11<00:04, 11.73it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:11<00:03, 13.86it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 13.68it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 13.60it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:02, 13.38it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:02, 13.33it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:12<00:02, 13.18it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:12<00:02, 13.07it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:02, 10.97it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  8.78it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:03,  7.72it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:13<00:03,  8.66it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:13<00:01, 11.05it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:14<00:01, 14.17it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:14<00:00, 14.14it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:14<00:00, 13.49it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:14<00:00, 15.46it/s]\u001b[A\n",
      " 98%|███████████████████████████████████████▉ | 197/202 [00:14<00:00, 17.43it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:14<00:00, 13.63it/s]\u001b[A\n",
      "06/28/2021 18:25:08 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/predict_predictions.json.\n",
      "06/28/2021 18:25:08 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 18:25:08,690 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:25:08,690 >>   predict_samples  =    6248\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:25:08,690 >>   test_exact_match =  40.099\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:25:08,690 >>   test_f1          = 68.7053\n",
      "100%|█████████████████████████████████████████| 196/196 [00:49<00:00,  3.96it/s]\n",
      "06/28/2021 18:25:14 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 18:25:14 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_18-25-14_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 21249\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 18:25:14,909 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 18:25:14,909 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 18:25:14,910 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 18:25:14,910 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 18:25:14,910 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 18:25:14,911 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 18:25:14,911 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 18:25:14,911 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 18:25:14,911 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 18:25:14,911 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 18:25:14,911 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 18:25:15,026 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 18:25:16,684 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 18:25:16,685 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 22/22 [00:40<00:00,  1.83s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.94s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.95s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 18:26:26,866 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 18:26:26,866 >>   Num examples = 69549\n",
      "[INFO|trainer.py:1147] 2021-06-28 18:26:26,866 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 18:26:26,866 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 18:26:26,866 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 18:26:26,866 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 18:26:26,866 >>   Total optimization steps = 2174\n",
      "{'loss': 0.2039, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 2174/2174 [17:23<00:00,  2.56it/s][INFO|trainer.py:1341] 2021-06-28 18:43:50,848 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1043.9824, 'train_samples_per_second': 2.082, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 2174/2174 [17:23<00:00,  2.08it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 18:43:51,153 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 18:43:51,154 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 18:43:52,063 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 18:43:52,063 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 18:43:52,064 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 18:43:52,160 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   init_mem_cpu_alloc_delta   =      506MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   init_mem_cpu_peaked_delta  =      358MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   train_mem_cpu_alloc_delta  =      780MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   train_mem_cpu_peaked_delta =        1MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,160 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,161 >>   train_runtime              = 0:17:23.98\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,161 >>   train_samples              =      69549\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:43:52,161 >>   train_samples_per_second   =      2.082\n",
      "06/28/2021 18:43:52 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 18:43:52,162 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 18:43:52,163 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 18:43:52,163 >>   Num examples = 6274\n",
      "[INFO|trainer.py:2120] 2021-06-28 18:43:52,163 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 196/197 [00:26<00:00,  7.41it/s]06/28/2021 18:44:25 - INFO - utils_qa -   Post-processing 202 example predictions split into 6274 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:20,  9.94it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:15, 12.72it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:14, 13.32it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:12, 14.80it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:11, 16.01it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/202 [00:00<00:10, 17.06it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/202 [00:00<00:09, 19.52it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/202 [00:01<00:09, 19.89it/s]\u001b[A\n",
      " 13%|█████▍                                    | 26/202 [00:01<00:08, 20.18it/s]\u001b[A\n",
      " 14%|██████                                    | 29/202 [00:01<00:08, 20.01it/s]\u001b[A\n",
      " 16%|██████▋                                   | 32/202 [00:01<00:08, 20.14it/s]\u001b[A\n",
      " 17%|███████▎                                  | 35/202 [00:01<00:08, 19.77it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/202 [00:01<00:08, 19.41it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/202 [00:02<00:08, 20.06it/s]\u001b[A\n",
      " 22%|█████████▏                                | 44/202 [00:02<00:08, 18.72it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 17.85it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 19.12it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:08, 18.81it/s]\u001b[A\n",
      " 26%|███████████                               | 53/202 [00:02<00:08, 17.29it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:08, 17.39it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:08, 17.18it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:06, 20.49it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.39it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:08, 16.69it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:09, 14.29it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:03<00:09, 13.33it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:09, 13.88it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:08, 14.67it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:08, 15.13it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:09, 12.64it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:04<00:08, 13.68it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:04<00:09, 11.96it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:10, 10.97it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:10, 10.55it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:05<00:09, 11.26it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 12.96it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:08, 12.86it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:08, 12.86it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:08, 12.17it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 11.77it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:08, 11.87it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:08, 11.37it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:08, 10.93it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:08, 10.69it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:07<00:08, 10.51it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:08, 10.51it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08, 10.04it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:09,  9.28it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:09,  9.09it/s]\u001b[A\n",
      " 60%|████████████████████████▌                | 121/202 [00:08<00:07, 10.68it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:08<00:06, 11.93it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:08<00:05, 13.10it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:08<00:05, 12.62it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:08<00:06, 10.87it/s]\u001b[A\n",
      " 65%|██████████████████████████▊              | 132/202 [00:08<00:06, 11.64it/s]\u001b[A\n",
      " 66%|███████████████████████████▏             | 134/202 [00:09<00:08,  8.32it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 136/202 [00:09<00:08,  7.60it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:09,  7.15it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:09,  6.95it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:09,  6.94it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:09,  6.64it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:09,  6.62it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:09,  6.53it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:09,  6.50it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 145/202 [00:10<00:07,  7.83it/s]\u001b[A\n",
      " 73%|█████████████████████████████▊           | 147/202 [00:11<00:05,  9.53it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 149/202 [00:11<00:05, 10.28it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:11<00:04, 11.50it/s]\u001b[A\n",
      " 76%|███████████████████████████████▎         | 154/202 [00:11<00:03, 12.46it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:11<00:03, 14.03it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 13.73it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:03, 13.59it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 11.32it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:03, 10.26it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:12<00:03,  9.99it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:12<00:03, 10.60it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:13<00:04,  6.62it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 171/202 [00:13<00:05,  5.41it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:05,  5.48it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 173/202 [00:13<00:05,  5.67it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:14<00:04,  5.84it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 178/202 [00:14<00:03,  7.74it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 182/202 [00:14<00:01, 10.19it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 186/202 [00:14<00:01, 12.30it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▎  | 189/202 [00:14<00:01, 11.25it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▉  | 192/202 [00:14<00:00, 13.61it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▌ | 195/202 [00:15<00:00, 15.81it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▏| 198/202 [00:15<00:00, 16.19it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 13.18it/s]\u001b[A\n",
      "06/28/2021 18:44:41 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4/eval_predictions.json.\n",
      "06/28/2021 18:44:41 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 197/197 [00:49<00:00,  4.02it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 18:44:41,347 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:44:41,347 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:44:41,347 >>   eval_exact_match = 33.1683\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:44:41,347 >>   eval_f1          = 60.2693\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:44:41,347 >>   eval_samples     =    6274\n",
      "06/28/2021 18:44:41 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 18:44:41,383 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 18:44:41,387 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 18:44:41,387 >>   Num examples = 6248\n",
      "[INFO|trainer.py:2120] 2021-06-28 18:44:41,387 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:26<00:00,  7.44it/s]06/28/2021 18:45:14 - INFO - utils_qa -   Post-processing 202 example predictions split into 6248 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:17, 11.18it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:13, 14.24it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:11, 16.76it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:10, 17.56it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 18.21it/s]\u001b[A\n",
      "  8%|███▎                                      | 16/202 [00:00<00:10, 17.87it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:00<00:11, 15.88it/s]\u001b[A\n",
      " 10%|████▎                                     | 21/202 [00:01<00:10, 17.92it/s]\u001b[A\n",
      " 12%|████▉                                     | 24/202 [00:01<00:09, 18.26it/s]\u001b[A\n",
      " 13%|█████▌                                    | 27/202 [00:01<00:09, 19.41it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:08, 21.03it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 19.97it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:01<00:09, 18.03it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:01<00:08, 19.88it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:02<00:08, 18.84it/s]\u001b[A\n",
      " 22%|█████████▎                                | 45/202 [00:02<00:09, 16.26it/s]\u001b[A\n",
      " 24%|█████████▉                                | 48/202 [00:02<00:09, 16.71it/s]\u001b[A\n",
      " 25%|██████████▍                               | 50/202 [00:02<00:09, 16.08it/s]\u001b[A\n",
      " 26%|███████████                               | 53/202 [00:02<00:08, 17.37it/s]\u001b[A\n",
      " 27%|███████████▍                              | 55/202 [00:02<00:10, 14.69it/s]\u001b[A\n",
      " 28%|███████████▊                              | 57/202 [00:03<00:10, 14.11it/s]\u001b[A\n",
      " 30%|████████████▋                             | 61/202 [00:03<00:08, 16.86it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:07, 19.12it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:08, 15.51it/s]\u001b[A\n",
      " 35%|██████████████▌                           | 70/202 [00:03<00:09, 13.91it/s]\u001b[A\n",
      " 36%|██████████████▉                           | 72/202 [00:03<00:10, 12.82it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/202 [00:04<00:09, 13.05it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 76/202 [00:04<00:09, 13.92it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 79/202 [00:04<00:07, 15.98it/s]\u001b[A\n",
      " 40%|████████████████▊                         | 81/202 [00:04<00:09, 13.28it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/202 [00:04<00:08, 13.52it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 85/202 [00:04<00:09, 12.39it/s]\u001b[A\n",
      " 43%|██████████████████                        | 87/202 [00:05<00:10, 11.35it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/202 [00:05<00:10, 10.69it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 91/202 [00:05<00:09, 11.57it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:05<00:08, 13.20it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:05<00:08, 13.24it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:05<00:07, 13.04it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:08, 12.09it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 11.77it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:06<00:07, 12.70it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:06<00:08, 11.86it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:06<00:08, 11.39it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:08, 10.93it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:07<00:08, 10.77it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:07<00:08, 10.52it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:07<00:08, 10.07it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:07<00:09,  9.30it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:09,  9.07it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:08<00:09,  8.96it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:08<00:07, 10.56it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:08<00:06, 11.89it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:08<00:05, 13.10it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:08<00:06, 10.97it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:08<00:07,  9.85it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:06, 10.71it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:07,  9.01it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:09<00:08,  8.05it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:09<00:08,  7.48it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:08,  7.09it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:10,  6.10it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:10,  6.07it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:09,  6.14it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:10<00:09,  6.17it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:11<00:09,  6.25it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:11<00:07,  7.56it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:05,  9.15it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:11<00:04, 10.51it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:11<00:04, 11.65it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:11<00:03, 13.69it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 13.48it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:11<00:03, 13.55it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:02, 13.69it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:02, 13.52it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:12<00:02, 13.42it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:12<00:02, 13.07it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:12<00:02, 10.95it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  8.87it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:03,  7.94it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:13<00:02,  8.84it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:13<00:01, 11.25it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:13<00:01, 14.44it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:14<00:00, 14.08it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:14<00:00, 13.44it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:14<00:00, 15.51it/s]\u001b[A\n",
      " 98%|███████████████████████████████████████▉ | 197/202 [00:14<00:00, 17.31it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:14<00:00, 13.71it/s]\u001b[A\n",
      "06/28/2021 18:45:29 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4/predict_predictions.json.\n",
      "06/28/2021 18:45:29 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 18:45:29,275 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:45:29,275 >>   predict_samples  =    6248\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:45:29,275 >>   test_exact_match = 36.1386\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 18:45:29,275 >>   test_f1          = 66.0137\n",
      "100%|█████████████████████████████████████████| 196/196 [00:47<00:00,  4.09it/s]\n",
      "06/28/2021 18:45:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 18:45:35 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_18-45-35_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 80152\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 201\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:517] 2021-06-28 18:45:36,711 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 18:45:36,713 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-06-28 18:45:36,981 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 18:45:36,982 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 18:45:38,600 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /soe/meidam/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 18:45:38,601 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /soe/meidam/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 18:45:38,601 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /soe/meidam/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 18:45:38,601 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 18:45:38,601 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-28 18:45:38,601 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1155] 2021-06-28 18:45:39,015 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /soe/meidam/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1330] 2021-06-28 18:45:41,002 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1341] 2021-06-28 18:45:41,002 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/28/2021 18:45:41 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train/cache-348d8fa3882951ad.arrow\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.14s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.45s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 18:46:09,315 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 18:46:09,315 >>   Num examples = 129212\n",
      "[INFO|trainer.py:1147] 2021-06-28 18:46:09,315 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 18:46:09,315 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 18:46:09,316 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 18:46:09,316 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 18:46:09,316 >>   Total optimization steps = 4038\n",
      "{'loss': 0.8688, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 4038/4038 [37:15<00:00,  1.90it/s][INFO|trainer.py:1341] 2021-06-28 19:23:24,374 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2235.0594, 'train_samples_per_second': 1.807, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 4038/4038 [37:15<00:00,  1.81it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 19:23:24,694 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 19:23:24,695 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 19:23:25,644 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 19:23:25,645 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 19:23:25,645 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 19:23:25,780 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   init_mem_cpu_alloc_delta   =     1439MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   init_mem_cpu_peaked_delta  =      356MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   train_mem_cpu_alloc_delta  =      896MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   train_mem_cpu_peaked_delta =        5MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   train_runtime              = 0:37:15.05\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   train_samples              =     129212\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:23:25,781 >>   train_samples_per_second   =      1.807\n",
      "06/28/2021 19:23:25 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 19:23:25,783 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 19:23:25,785 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 19:23:25,785 >>   Num examples = 6218\n",
      "[INFO|trainer.py:2120] 2021-06-28 19:23:25,785 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 194/195 [00:26<00:00,  7.42it/s]06/28/2021 19:23:58 - INFO - utils_qa -   Post-processing 201 example predictions split into 6218 features.\n",
      "\n",
      "  0%|                                                   | 0/201 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                          | 1/201 [00:00<00:25,  7.80it/s]\u001b[A\n",
      "  1%|▍                                          | 2/201 [00:00<00:26,  7.56it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/201 [00:00<00:19,  9.91it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/201 [00:00<00:17, 10.98it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/201 [00:00<00:14, 12.84it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/201 [00:00<00:13, 14.17it/s]\u001b[A\n",
      "  7%|███▏                                      | 15/201 [00:00<00:12, 15.40it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/201 [00:00<00:11, 16.36it/s]\u001b[A\n",
      " 10%|████▏                                     | 20/201 [00:01<00:09, 18.27it/s]\u001b[A\n",
      " 11%|████▊                                     | 23/201 [00:01<00:11, 15.99it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/201 [00:01<00:13, 12.86it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/201 [00:01<00:12, 14.39it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/201 [00:01<00:10, 15.56it/s]\u001b[A\n",
      " 16%|██████▉                                   | 33/201 [00:01<00:10, 16.25it/s]\u001b[A\n",
      " 18%|███████▌                                  | 36/201 [00:02<00:09, 17.23it/s]\u001b[A\n",
      " 19%|███████▉                                  | 38/201 [00:02<00:09, 17.53it/s]\u001b[A\n",
      " 20%|████████▌                                 | 41/201 [00:02<00:08, 18.17it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/201 [00:02<00:08, 18.25it/s]\u001b[A\n",
      " 22%|█████████▍                                | 45/201 [00:02<00:10, 15.59it/s]\u001b[A\n",
      " 24%|██████████                                | 48/201 [00:02<00:09, 16.74it/s]\u001b[A\n",
      " 25%|██████████▍                               | 50/201 [00:02<00:08, 17.35it/s]\u001b[A\n",
      " 26%|███████████                               | 53/201 [00:03<00:08, 17.88it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/201 [00:03<00:07, 18.31it/s]\u001b[A\n",
      " 29%|████████████                              | 58/201 [00:03<00:09, 14.52it/s]\u001b[A\n",
      " 30%|████████████▌                             | 60/201 [00:03<00:09, 15.44it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 63/201 [00:03<00:07, 17.45it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/201 [00:04<00:12, 10.87it/s]\u001b[A\n",
      " 33%|██████████████                            | 67/201 [00:04<00:12, 10.51it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 69/201 [00:04<00:12, 10.88it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/201 [00:04<00:11, 11.34it/s]\u001b[A\n",
      " 36%|███████████████▎                          | 73/201 [00:04<00:10, 11.88it/s]\u001b[A\n",
      " 37%|███████████████▋                          | 75/201 [00:04<00:09, 12.97it/s]\u001b[A\n",
      " 39%|████████████████▎                         | 78/201 [00:04<00:08, 15.11it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/201 [00:05<00:09, 12.73it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 82/201 [00:05<00:08, 13.35it/s]\u001b[A\n",
      " 42%|█████████████████▌                        | 84/201 [00:05<00:09, 12.61it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/201 [00:05<00:09, 11.67it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 88/201 [00:05<00:10, 11.02it/s]\u001b[A\n",
      " 45%|██████████████████▊                       | 90/201 [00:06<00:10, 10.64it/s]\u001b[A\n",
      " 46%|███████████████████▍                      | 93/201 [00:06<00:08, 13.14it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/201 [00:06<00:08, 12.72it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 97/201 [00:06<00:08, 12.57it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 99/201 [00:06<00:09, 10.37it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/201 [00:07<00:13,  7.57it/s]\u001b[A\n",
      " 51%|████████████████████▊                    | 102/201 [00:07<00:12,  8.00it/s]\u001b[A\n",
      " 52%|█████████████████████▏                   | 104/201 [00:07<00:11,  8.65it/s]\u001b[A\n",
      " 53%|█████████████████████▌                   | 106/201 [00:07<00:10,  9.06it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 108/201 [00:07<00:10,  9.09it/s]\u001b[A\n",
      " 55%|██████████████████████▍                  | 110/201 [00:08<00:09,  9.37it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 111/201 [00:08<00:09,  9.26it/s]\u001b[A\n",
      " 56%|██████████████████████▊                  | 112/201 [00:08<00:11,  7.49it/s]\u001b[A\n",
      " 56%|███████████████████████                  | 113/201 [00:08<00:11,  7.88it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 114/201 [00:08<00:10,  8.41it/s]\u001b[A\n",
      " 57%|███████████████████████▍                 | 115/201 [00:08<00:10,  8.36it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 116/201 [00:08<00:10,  8.12it/s]\u001b[A\n",
      " 58%|███████████████████████▊                 | 117/201 [00:09<00:10,  8.03it/s]\u001b[A\n",
      " 59%|████████████████████████                 | 118/201 [00:09<00:10,  7.84it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 119/201 [00:09<00:10,  7.83it/s]\u001b[A\n",
      " 60%|████████████████████████▋                | 121/201 [00:09<00:08,  9.38it/s]\u001b[A\n",
      " 61%|█████████████████████████                | 123/201 [00:09<00:07, 10.85it/s]\u001b[A\n",
      " 62%|█████████████████████████▍               | 125/201 [00:09<00:06, 12.06it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 127/201 [00:09<00:06, 11.57it/s]\u001b[A\n",
      " 64%|██████████████████████████▎              | 129/201 [00:10<00:07,  9.48it/s]\u001b[A\n",
      " 65%|██████████████████████████▋              | 131/201 [00:10<00:06, 11.23it/s]\u001b[A\n",
      " 66%|███████████████████████████▏             | 133/201 [00:10<00:09,  7.15it/s]\u001b[A\n",
      " 67%|███████████████████████████▌             | 135/201 [00:11<00:11,  5.93it/s]\u001b[A\n",
      " 68%|███████████████████████████▋             | 136/201 [00:11<00:12,  5.18it/s]\u001b[A\n",
      " 68%|███████████████████████████▉             | 137/201 [00:11<00:13,  4.92it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 138/201 [00:11<00:12,  5.24it/s]\u001b[A\n",
      " 69%|████████████████████████████▎            | 139/201 [00:12<00:11,  5.63it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 140/201 [00:12<00:10,  5.96it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 141/201 [00:12<00:09,  6.19it/s]\u001b[A\n",
      " 71%|████████████████████████████▉            | 142/201 [00:12<00:09,  6.21it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 143/201 [00:12<00:09,  6.42it/s]\u001b[A\n",
      " 72%|█████████████████████████████▌           | 145/201 [00:12<00:07,  7.72it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 148/201 [00:12<00:05,  9.63it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 151/201 [00:12<00:04, 12.08it/s]\u001b[A\n",
      " 76%|███████████████████████████████▏         | 153/201 [00:13<00:04, 11.31it/s]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 156/201 [00:13<00:03, 13.21it/s]\u001b[A\n",
      " 79%|████████████████████████████████▏        | 158/201 [00:13<00:03, 13.24it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 160/201 [00:13<00:03, 13.49it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 162/201 [00:13<00:02, 13.55it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 164/201 [00:13<00:02, 13.27it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▊       | 166/201 [00:14<00:02, 13.35it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 168/201 [00:14<00:02, 11.09it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 170/201 [00:14<00:03,  9.01it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 172/201 [00:14<00:03,  8.04it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 173/201 [00:15<00:03,  7.47it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▍     | 174/201 [00:15<00:03,  6.96it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 178/201 [00:15<00:02,  9.15it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████    | 182/201 [00:15<00:01, 11.84it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 186/201 [00:15<00:01, 14.13it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 189/201 [00:15<00:00, 12.52it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 192/201 [00:16<00:00, 15.10it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 201/201 [00:16<00:00, 12.36it/s]\u001b[A\n",
      "06/28/2021 19:24:15 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/eval_predictions.json.\n",
      "06/28/2021 19:24:15 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 195/195 [00:49<00:00,  3.96it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 19:24:15,233 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:24:15,233 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:24:15,233 >>   eval_exact_match = 32.3383\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:24:15,233 >>   eval_f1          = 62.8014\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:24:15,233 >>   eval_samples     =    6218\n",
      "06/28/2021 19:24:15 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 19:24:15,280 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2115] 2021-06-28 19:24:15,284 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 19:24:15,284 >>   Num examples = 6246\n",
      "[INFO|trainer.py:2120] 2021-06-28 19:24:15,284 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:26<00:00,  7.49it/s]06/28/2021 19:24:48 - INFO - utils_qa -   Post-processing 202 example predictions split into 6246 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:16, 11.88it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/202 [00:00<00:13, 14.92it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/202 [00:00<00:12, 15.11it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:10, 17.67it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/202 [00:00<00:10, 18.31it/s]\u001b[A\n",
      "  8%|███▎                                      | 16/202 [00:00<00:10, 17.35it/s]\u001b[A\n",
      "  9%|███▉                                      | 19/202 [00:00<00:09, 18.41it/s]\u001b[A\n",
      " 11%|████▌                                     | 22/202 [00:01<00:08, 20.44it/s]\u001b[A\n",
      " 12%|█████▏                                    | 25/202 [00:01<00:08, 20.04it/s]\u001b[A\n",
      " 14%|█████▊                                    | 28/202 [00:01<00:07, 22.04it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:07, 23.21it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:07, 21.80it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:01<00:08, 19.15it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:01<00:07, 21.12it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:02<00:08, 19.28it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 18.07it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:08, 18.99it/s]\u001b[A\n",
      " 25%|██████████▌                               | 51/202 [00:02<00:07, 19.12it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:08, 18.01it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:02<00:12, 11.82it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:12, 11.24it/s]\u001b[A\n",
      " 30%|████████████▍                             | 60/202 [00:03<00:11, 12.76it/s]\u001b[A\n",
      " 31%|█████████████                             | 63/202 [00:03<00:09, 15.28it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:09, 13.82it/s]\u001b[A\n",
      " 33%|█████████████▉                            | 67/202 [00:03<00:13, 10.09it/s]\u001b[A\n",
      " 34%|██████████████▎                           | 69/202 [00:04<00:15,  8.86it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:04<00:14,  9.02it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:04<00:12, 10.02it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:04<00:11, 11.28it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:04<00:09, 12.85it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:08, 13.87it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:05<00:07, 15.06it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:05<00:07, 15.64it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:05<00:08, 13.02it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:09, 11.62it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:05<00:10, 10.96it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 93/202 [00:05<00:08, 13.49it/s]\u001b[A\n",
      " 47%|███████████████████▊                      | 95/202 [00:06<00:10, 10.62it/s]\u001b[A\n",
      " 48%|████████████████████▏                     | 97/202 [00:06<00:09, 11.58it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 99/202 [00:06<00:08, 12.04it/s]\u001b[A\n",
      " 50%|████████████████████▌                    | 101/202 [00:06<00:09, 11.12it/s]\u001b[A\n",
      " 51%|████████████████████▉                    | 103/202 [00:06<00:08, 12.10it/s]\u001b[A\n",
      " 52%|█████████████████████▎                   | 105/202 [00:06<00:07, 12.43it/s]\u001b[A\n",
      " 53%|█████████████████████▋                   | 107/202 [00:07<00:08, 11.78it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 109/202 [00:07<00:08, 11.17it/s]\u001b[A\n",
      " 55%|██████████████████████▌                  | 111/202 [00:07<00:08, 10.87it/s]\u001b[A\n",
      " 56%|██████████████████████▉                  | 113/202 [00:07<00:08, 10.89it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 115/202 [00:07<00:08, 10.85it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 117/202 [00:08<00:08, 10.08it/s]\u001b[A\n",
      " 59%|████████████████████████▏                | 119/202 [00:08<00:08,  9.52it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:08<00:08,  9.26it/s]\u001b[A\n",
      " 60%|████████████████████████▊                | 122/202 [00:08<00:07, 10.60it/s]\u001b[A\n",
      " 61%|█████████████████████████▏               | 124/202 [00:08<00:06, 12.07it/s]\u001b[A\n",
      " 62%|█████████████████████████▌               | 126/202 [00:08<00:05, 13.38it/s]\u001b[A\n",
      " 63%|█████████████████████████▉               | 128/202 [00:09<00:06, 11.63it/s]\u001b[A\n",
      " 64%|██████████████████████████▍              | 130/202 [00:09<00:06, 10.42it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:06, 11.38it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:09<00:07,  9.16it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:10<00:07,  8.28it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:08,  7.54it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:08,  7.28it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:08,  7.26it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:10<00:08,  7.01it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:10<00:08,  7.01it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:11<00:08,  6.83it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:11<00:08,  6.81it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:11<00:07,  7.95it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 149/202 [00:11<00:05,  9.93it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 151/202 [00:11<00:04, 11.16it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 153/202 [00:11<00:03, 12.28it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:11<00:03, 14.44it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:11<00:03, 14.36it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 10.75it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 11.36it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:03, 11.77it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:12<00:02, 12.14it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:12<00:02, 12.65it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:13<00:02, 11.05it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  8.96it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:13<00:03,  7.95it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:13<00:02,  9.00it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:14<00:01, 11.45it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:14<00:01, 14.68it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:14<00:00, 14.18it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:14<00:00, 13.68it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:14<00:00, 16.03it/s]\u001b[A\n",
      " 98%|███████████████████████████████████████▉ | 197/202 [00:14<00:00, 17.89it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 13.46it/s]\u001b[A\n",
      "06/28/2021 19:25:03 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/predict_predictions.json.\n",
      "06/28/2021 19:25:03 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 19:25:03,289 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:25:03,290 >>   predict_samples  =    6246\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:25:03,290 >>   test_exact_match =  39.604\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:25:03,290 >>   test_f1          = 63.2723\n",
      "100%|█████████████████████████████████████████| 196/196 [00:48<00:00,  4.08it/s]\n",
      "06/28/2021 19:25:09 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 19:25:09 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_19-25-08_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 60518\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 201\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 19:25:09,445 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 19:25:09,446 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 19:25:09,447 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 19:25:09,447 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 19:25:09,448 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:25:09,448 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:25:09,448 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:25:09,448 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:25:09,448 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:25:09,448 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:25:09,448 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 19:25:09,555 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 19:25:11,125 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 19:25:11,125 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 61/61 [00:55<00:00,  1.11ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.63s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.06s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 19:26:34,998 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 19:26:34,998 >>   Num examples = 109345\n",
      "[INFO|trainer.py:1147] 2021-06-28 19:26:34,998 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 19:26:34,998 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 19:26:34,998 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 19:26:34,998 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 19:26:34,999 >>   Total optimization steps = 3418\n",
      "{'loss': 0.5706, 'learning_rate': 0.0, 'epoch': 1.0}                            \n",
      "100%|███████████████████████████████████████| 3418/3418 [27:09<00:00,  2.72it/s][INFO|trainer.py:1341] 2021-06-28 19:53:44,341 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1629.3428, 'train_samples_per_second': 2.098, 'epoch': 1.0}   \n",
      "100%|███████████████████████████████████████| 3418/3418 [27:09<00:00,  2.10it/s]\n",
      "[INFO|trainer.py:1885] 2021-06-28 19:53:44,658 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\n",
      "[INFO|configuration_utils.py:351] 2021-06-28 19:53:44,659 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/config.json\n",
      "[INFO|modeling_utils.py:889] 2021-06-28 19:53:45,495 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-06-28 19:53:45,495 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-06-28 19:53:45,496 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 19:53:45,593 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   epoch                      =        1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   init_mem_cpu_alloc_delta   =      301MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   init_mem_cpu_peaked_delta  =      357MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   init_mem_gpu_alloc_delta   =      474MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   train_mem_cpu_alloc_delta  =      848MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   train_mem_cpu_peaked_delta =        2MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   train_mem_gpu_alloc_delta  =     1421MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   train_mem_gpu_peaked_delta =    15779MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   train_runtime              = 0:27:09.34\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   train_samples              =     109345\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:53:45,594 >>   train_samples_per_second   =      2.098\n",
      "06/28/2021 19:53:45 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:515] 2021-06-28 19:53:45,595 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 19:53:45,596 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 19:53:45,596 >>   Num examples = 6218\n",
      "[INFO|trainer.py:2120] 2021-06-28 19:53:45,596 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 194/195 [00:26<00:00,  7.19it/s]06/28/2021 19:54:18 - INFO - utils_qa -   Post-processing 201 example predictions split into 6218 features.\n",
      "\n",
      "  0%|                                                   | 0/201 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/201 [00:00<00:16, 12.22it/s]\u001b[A\n",
      "  3%|█▎                                         | 6/201 [00:00<00:12, 15.36it/s]\u001b[A\n",
      "  4%|█▋                                         | 8/201 [00:00<00:12, 15.49it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/201 [00:00<00:11, 17.11it/s]\u001b[A\n",
      "  7%|██▉                                       | 14/201 [00:00<00:10, 17.85it/s]\u001b[A\n",
      "  8%|███▌                                      | 17/201 [00:00<00:09, 18.73it/s]\u001b[A\n",
      " 10%|████▍                                     | 21/201 [00:00<00:08, 21.91it/s]\u001b[A\n",
      " 12%|█████                                     | 24/201 [00:01<00:08, 20.47it/s]\u001b[A\n",
      " 13%|█████▋                                    | 27/201 [00:01<00:08, 20.95it/s]\u001b[A\n",
      " 15%|██████▎                                   | 30/201 [00:01<00:07, 21.61it/s]\u001b[A\n",
      " 16%|██████▉                                   | 33/201 [00:01<00:08, 20.65it/s]\u001b[A\n",
      " 18%|███████▌                                  | 36/201 [00:01<00:08, 20.33it/s]\u001b[A\n",
      " 19%|████████▏                                 | 39/201 [00:01<00:07, 20.51it/s]\u001b[A\n",
      " 21%|████████▊                                 | 42/201 [00:01<00:07, 20.29it/s]\u001b[A\n",
      " 22%|█████████▍                                | 45/201 [00:02<00:08, 17.35it/s]\u001b[A\n",
      " 24%|██████████                                | 48/201 [00:02<00:08, 18.76it/s]\u001b[A\n",
      " 25%|██████████▍                               | 50/201 [00:02<00:07, 19.00it/s]\u001b[A\n",
      " 26%|███████████                               | 53/201 [00:02<00:07, 18.54it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/201 [00:02<00:07, 18.49it/s]\u001b[A\n",
      " 29%|████████████                              | 58/201 [00:02<00:07, 18.74it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/201 [00:02<00:06, 22.01it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/201 [00:03<00:08, 16.39it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/201 [00:03<00:11, 11.66it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 70/201 [00:03<00:12, 10.33it/s]\u001b[A\n",
      " 36%|███████████████                           | 72/201 [00:04<00:11, 10.78it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 74/201 [00:04<00:10, 12.16it/s]\u001b[A\n",
      " 38%|███████████████▉                          | 76/201 [00:04<00:09, 12.86it/s]\u001b[A\n",
      " 39%|████████████████▌                         | 79/201 [00:04<00:08, 14.06it/s]\u001b[A\n",
      " 40%|████████████████▉                         | 81/201 [00:04<00:09, 12.71it/s]\u001b[A\n",
      " 41%|█████████████████▎                        | 83/201 [00:04<00:08, 13.90it/s]\u001b[A\n",
      " 42%|█████████████████▊                        | 85/201 [00:05<00:09, 12.44it/s]\u001b[A\n",
      " 43%|██████████████████▏                       | 87/201 [00:05<00:09, 11.58it/s]\u001b[A\n",
      " 44%|██████████████████▌                       | 89/201 [00:05<00:10, 11.14it/s]\u001b[A\n",
      " 45%|███████████████████                       | 91/201 [00:05<00:09, 12.08it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 94/201 [00:05<00:07, 13.91it/s]\u001b[A\n",
      " 48%|████████████████████                      | 96/201 [00:05<00:07, 13.87it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/201 [00:05<00:07, 13.81it/s]\u001b[A\n",
      " 50%|████████████████████▍                    | 100/201 [00:06<00:08, 12.47it/s]\u001b[A\n",
      " 51%|████████████████████▊                    | 102/201 [00:06<00:07, 12.44it/s]\u001b[A\n",
      " 52%|█████████████████████▏                   | 104/201 [00:06<00:07, 12.69it/s]\u001b[A\n",
      " 53%|█████████████████████▌                   | 106/201 [00:06<00:07, 12.27it/s]\u001b[A\n",
      " 54%|██████████████████████                   | 108/201 [00:06<00:07, 11.80it/s]\u001b[A\n",
      " 55%|██████████████████████▍                  | 110/201 [00:07<00:07, 11.43it/s]\u001b[A\n",
      " 56%|██████████████████████▊                  | 112/201 [00:07<00:07, 11.25it/s]\u001b[A\n",
      " 57%|███████████████████████▎                 | 114/201 [00:07<00:08, 10.81it/s]\u001b[A\n",
      " 58%|███████████████████████▋                 | 116/201 [00:07<00:08, 10.57it/s]\u001b[A\n",
      " 59%|████████████████████████                 | 118/201 [00:07<00:08,  9.84it/s]\u001b[A\n",
      " 60%|████████████████████████▍                | 120/201 [00:08<00:07, 10.39it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 122/201 [00:08<00:06, 11.97it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 124/201 [00:08<00:05, 13.53it/s]\u001b[A\n",
      " 63%|█████████████████████████▋               | 126/201 [00:08<00:05, 14.97it/s]\u001b[A\n",
      " 64%|██████████████████████████               | 128/201 [00:08<00:06, 12.11it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 130/201 [00:08<00:06, 10.38it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 132/201 [00:09<00:06, 10.67it/s]\u001b[A\n",
      " 67%|███████████████████████████▎             | 134/201 [00:09<00:07,  8.94it/s]\u001b[A\n",
      " 68%|███████████████████████████▋             | 136/201 [00:09<00:07,  8.13it/s]\u001b[A\n",
      " 68%|███████████████████████████▉             | 137/201 [00:09<00:08,  7.62it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 138/201 [00:09<00:08,  7.26it/s]\u001b[A\n",
      " 69%|████████████████████████████▎            | 139/201 [00:10<00:08,  7.22it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 140/201 [00:10<00:09,  6.73it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 141/201 [00:10<00:08,  6.77it/s]\u001b[A\n",
      " 71%|████████████████████████████▉            | 142/201 [00:10<00:08,  6.84it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 143/201 [00:10<00:08,  6.84it/s]\u001b[A\n",
      " 72%|█████████████████████████████▌           | 145/201 [00:10<00:06,  8.27it/s]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 148/201 [00:10<00:05, 10.09it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 152/201 [00:11<00:04, 11.91it/s]\u001b[A\n",
      " 77%|███████████████████████████████▍         | 154/201 [00:11<00:03, 12.69it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 157/201 [00:11<00:03, 13.83it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 159/201 [00:11<00:03, 13.91it/s]\u001b[A\n",
      " 80%|████████████████████████████████▊        | 161/201 [00:11<00:02, 13.81it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▏       | 163/201 [00:11<00:02, 13.88it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 165/201 [00:12<00:02, 13.77it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 167/201 [00:12<00:02, 13.93it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▍      | 169/201 [00:12<00:02, 11.40it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 171/201 [00:12<00:03,  9.13it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 173/201 [00:13<00:03,  7.99it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▍     | 174/201 [00:13<00:03,  7.56it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 178/201 [00:13<00:02,  9.91it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████    | 182/201 [00:13<00:01, 12.74it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████▉   | 186/201 [00:13<00:00, 15.06it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 189/201 [00:14<00:01, 10.73it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 192/201 [00:14<00:00, 13.23it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 201/201 [00:14<00:00, 14.00it/s]\u001b[A\n",
      "06/28/2021 19:54:33 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/eval_predictions.json.\n",
      "06/28/2021 19:54:33 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 195/195 [00:47<00:00,  4.08it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 19:54:33,562 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:54:33,562 >>   epoch            =     1.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:54:33,562 >>   eval_exact_match = 33.3333\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:54:33,563 >>   eval_f1          = 65.3696\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:54:33,563 >>   eval_samples     =    6218\n",
      "06/28/2021 19:54:33 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:515] 2021-06-28 19:54:33,611 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2115] 2021-06-28 19:54:33,615 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-06-28 19:54:33,615 >>   Num examples = 6246\n",
      "[INFO|trainer.py:2120] 2021-06-28 19:54:33,615 >>   Batch size = 32\n",
      " 99%|████████████████████████████████████████▊| 195/196 [00:26<00:00,  7.47it/s]06/28/2021 19:55:06 - INFO - utils_qa -   Post-processing 202 example predictions split into 6246 features.\n",
      "\n",
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                          | 1/202 [00:00<00:24,  8.23it/s]\u001b[A\n",
      "  1%|▍                                          | 2/202 [00:00<00:27,  7.38it/s]\u001b[A\n",
      "  2%|█                                          | 5/202 [00:00<00:20,  9.49it/s]\u001b[A\n",
      "  3%|█▍                                         | 7/202 [00:00<00:18, 10.59it/s]\u001b[A\n",
      "  4%|█▉                                         | 9/202 [00:00<00:17, 10.93it/s]\u001b[A\n",
      "  5%|██▎                                       | 11/202 [00:00<00:16, 11.86it/s]\u001b[A\n",
      "  6%|██▋                                       | 13/202 [00:00<00:14, 12.73it/s]\u001b[A\n",
      "  7%|███                                       | 15/202 [00:01<00:13, 13.94it/s]\u001b[A\n",
      "  9%|███▋                                      | 18/202 [00:01<00:11, 15.74it/s]\u001b[A\n",
      " 10%|████▎                                     | 21/202 [00:01<00:10, 17.95it/s]\u001b[A\n",
      " 12%|████▉                                     | 24/202 [00:01<00:09, 18.89it/s]\u001b[A\n",
      " 13%|█████▌                                    | 27/202 [00:01<00:08, 20.05it/s]\u001b[A\n",
      " 15%|██████▍                                   | 31/202 [00:01<00:07, 22.06it/s]\u001b[A\n",
      " 17%|███████                                   | 34/202 [00:01<00:08, 20.88it/s]\u001b[A\n",
      " 18%|███████▋                                  | 37/202 [00:02<00:08, 18.94it/s]\u001b[A\n",
      " 20%|████████▎                                 | 40/202 [00:02<00:07, 20.91it/s]\u001b[A\n",
      " 21%|████████▉                                 | 43/202 [00:02<00:08, 19.72it/s]\u001b[A\n",
      " 23%|█████████▌                                | 46/202 [00:02<00:08, 18.35it/s]\u001b[A\n",
      " 24%|██████████▏                               | 49/202 [00:02<00:07, 19.34it/s]\u001b[A\n",
      " 26%|██████████▊                               | 52/202 [00:02<00:07, 19.39it/s]\u001b[A\n",
      " 27%|███████████▏                              | 54/202 [00:02<00:07, 18.79it/s]\u001b[A\n",
      " 28%|███████████▋                              | 56/202 [00:03<00:09, 16.16it/s]\u001b[A\n",
      " 29%|████████████                              | 58/202 [00:03<00:08, 17.01it/s]\u001b[A\n",
      " 31%|████████████▉                             | 62/202 [00:03<00:06, 20.30it/s]\u001b[A\n",
      " 32%|█████████████▌                            | 65/202 [00:03<00:06, 21.49it/s]\u001b[A\n",
      " 34%|██████████████▏                           | 68/202 [00:03<00:07, 16.85it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 71/202 [00:04<00:11, 11.81it/s]\u001b[A\n",
      " 36%|███████████████▏                          | 73/202 [00:04<00:12,  9.97it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 75/202 [00:04<00:13,  9.76it/s]\u001b[A\n",
      " 38%|████████████████                          | 77/202 [00:04<00:11, 11.25it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 80/202 [00:04<00:09, 12.45it/s]\u001b[A\n",
      " 41%|█████████████████                         | 82/202 [00:05<00:08, 13.51it/s]\u001b[A\n",
      " 42%|█████████████████▍                        | 84/202 [00:05<00:08, 14.14it/s]\u001b[A\n",
      " 43%|█████████████████▉                        | 86/202 [00:05<00:09, 11.79it/s]\u001b[A\n",
      " 44%|██████████████████▎                       | 88/202 [00:05<00:13,  8.75it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 90/202 [00:06<00:14,  7.62it/s]\u001b[A\n",
      " 46%|███████████████████▏                      | 92/202 [00:06<00:11,  9.33it/s]\u001b[A\n",
      " 47%|███████████████████▌                      | 94/202 [00:06<00:10, 10.75it/s]\u001b[A\n",
      " 48%|███████████████████▉                      | 96/202 [00:06<00:09, 11.48it/s]\u001b[A\n",
      " 49%|████████████████████▍                     | 98/202 [00:06<00:08, 12.07it/s]\u001b[A\n",
      " 50%|████████████████████▎                    | 100/202 [00:06<00:08, 11.66it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 102/202 [00:06<00:08, 11.66it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 104/202 [00:07<00:07, 12.58it/s]\u001b[A\n",
      " 52%|█████████████████████▌                   | 106/202 [00:07<00:07, 12.12it/s]\u001b[A\n",
      " 53%|█████████████████████▉                   | 108/202 [00:07<00:08, 11.25it/s]\u001b[A\n",
      " 54%|██████████████████████▎                  | 110/202 [00:07<00:08, 11.13it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 112/202 [00:07<00:08, 11.23it/s]\u001b[A\n",
      " 56%|███████████████████████▏                 | 114/202 [00:08<00:07, 11.06it/s]\u001b[A\n",
      " 57%|███████████████████████▌                 | 116/202 [00:08<00:07, 10.75it/s]\u001b[A\n",
      " 58%|███████████████████████▉                 | 118/202 [00:08<00:08,  9.83it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 120/202 [00:08<00:10,  7.97it/s]\u001b[A\n",
      " 61%|████████████████████████▉                | 123/202 [00:09<00:08,  9.70it/s]\u001b[A\n",
      " 62%|█████████████████████████▎               | 125/202 [00:09<00:06, 11.21it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 127/202 [00:09<00:06, 11.49it/s]\u001b[A\n",
      " 64%|██████████████████████████▏              | 129/202 [00:09<00:07, 10.36it/s]\u001b[A\n",
      " 65%|██████████████████████████▌              | 131/202 [00:09<00:06, 11.22it/s]\u001b[A\n",
      " 66%|██████████████████████████▉              | 133/202 [00:09<00:06, 11.05it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 135/202 [00:10<00:07,  9.27it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 137/202 [00:10<00:07,  8.36it/s]\u001b[A\n",
      " 68%|████████████████████████████             | 138/202 [00:10<00:08,  7.65it/s]\u001b[A\n",
      " 69%|████████████████████████████▏            | 139/202 [00:10<00:08,  7.29it/s]\u001b[A\n",
      " 69%|████████████████████████████▍            | 140/202 [00:10<00:08,  6.90it/s]\u001b[A\n",
      " 70%|████████████████████████████▌            | 141/202 [00:11<00:08,  6.88it/s]\u001b[A\n",
      " 70%|████████████████████████████▊            | 142/202 [00:11<00:08,  6.90it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 143/202 [00:11<00:08,  6.98it/s]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 144/202 [00:11<00:08,  6.88it/s]\u001b[A\n",
      " 72%|█████████████████████████████▋           | 146/202 [00:11<00:06,  8.29it/s]\u001b[A\n",
      " 73%|██████████████████████████████           | 148/202 [00:11<00:05,  9.46it/s]\u001b[A\n",
      " 74%|██████████████████████████████▍          | 150/202 [00:11<00:04, 10.57it/s]\u001b[A\n",
      " 75%|██████████████████████████████▊          | 152/202 [00:12<00:04, 10.54it/s]\u001b[A\n",
      " 76%|███████████████████████████████▎         | 154/202 [00:12<00:04, 10.23it/s]\u001b[A\n",
      " 77%|███████████████████████████████▋         | 156/202 [00:12<00:03, 11.79it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 158/202 [00:12<00:03, 12.45it/s]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 160/202 [00:12<00:03, 12.97it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 162/202 [00:12<00:03, 13.13it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 164/202 [00:12<00:02, 13.59it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 166/202 [00:13<00:02, 13.59it/s]\u001b[A\n",
      " 83%|██████████████████████████████████       | 168/202 [00:13<00:02, 13.61it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 170/202 [00:13<00:02, 11.44it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 172/202 [00:13<00:03,  9.22it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▎     | 174/202 [00:14<00:03,  7.99it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▋     | 176/202 [00:14<00:02,  9.11it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 180/202 [00:14<00:01, 11.60it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 185/202 [00:14<00:01, 14.99it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▏  | 188/202 [00:14<00:00, 14.97it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 191/202 [00:14<00:00, 14.20it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 194/202 [00:15<00:00, 16.42it/s]\u001b[A\n",
      " 98%|███████████████████████████████████████▉ | 197/202 [00:15<00:00, 18.48it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 202/202 [00:15<00:00, 13.16it/s]\u001b[A\n",
      "06/28/2021 19:55:21 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/predict_predictions.json.\n",
      "06/28/2021 19:55:21 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/predict_nbest_predictions.json.\n",
      "[INFO|trainer_pt_utils.py:907] 2021-06-28 19:55:21,744 >> ***** predict metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:55:21,744 >>   predict_samples  =    6246\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:55:21,744 >>   test_exact_match = 41.0891\n",
      "[INFO|trainer_pt_utils.py:912] 2021-06-28 19:55:21,745 >>   test_f1          = 65.7163\n",
      "100%|█████████████████████████████████████████| 196/196 [00:48<00:00,  4.06it/s]\n",
      "06/28/2021 19:55:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/28/2021 19:55:27 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_19-55-27_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 40884\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 201\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 19:55:27,810 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 19:55:27,811 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:515] 2021-06-28 19:55:27,812 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-28 19:55:27,812 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-28 19:55:27,812 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:55:27,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:55:27,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:55:27,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:55:27,813 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:55:27,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-28 19:55:27,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1153] 2021-06-28 19:55:27,921 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-28 19:55:29,285 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-28 19:55:29,288 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 41/41 [00:46<00:00,  1.14s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.58s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.08s/ba]\n",
      "[INFO|trainer.py:1145] 2021-06-28 19:56:44,598 >> ***** Running training *****\n",
      "[INFO|trainer.py:1146] 2021-06-28 19:56:44,598 >>   Num examples = 89483\n",
      "[INFO|trainer.py:1147] 2021-06-28 19:56:44,598 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1148] 2021-06-28 19:56:44,598 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1149] 2021-06-28 19:56:44,598 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1150] 2021-06-28 19:56:44,598 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1151] 2021-06-28 19:56:44,598 >>   Total optimization steps = 2797\n",
      " 20%|███████▉                                | 555/2797 [04:26<18:26,  2.03it/s]"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "K = 5\n",
    "\n",
    "for i in range(k_fold):\n",
    "    covid_fold = covid_qa.shard(k_fold, i)\n",
    "    \n",
    "    covid_test = covid_fold.shard(2, 0)\n",
    "    covid_val = covid_fold.shard(2, 1)    \n",
    "    covid_train = concatenate_datasets([covid_qa.shard(k_fold, j) for j in range(k_fold) if j != i])   \n",
    "    \n",
    "    #make_and_save_full_dataset(covid_train, squad_qa, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "        \n",
    "    checkpoint = 'roberta-base'\n",
    "    cur_dir = '../models/gradual_ft_baseline_lr1e-5_2/split_' + str(i)\n",
    "    \n",
    "    squad_qa.shuffle()\n",
    "    \n",
    "    for n in range(K):\n",
    "        output_dir = cur_dir + '/checkpoint_' + str(n)\n",
    "        #squad_qa = datasets.Dataset.from_dict(squad_qa[:-to_remove_per_step])\n",
    "        \n",
    "        if n < K-1:\n",
    "            squad_qa_cur = concatenate_datasets([squad_qa.shard(K, j) for j in range(K-n-1)])\n",
    "            full_dataset = concatenate_datasets([squad_qa_cur, covid_train])\n",
    "        elif n < K:\n",
    "            squad_qa_cur = squad_qa.shard(K, 0)\n",
    "            full_dataset = concatenate_datasets([squad_qa_cur, covid_train])\n",
    "        else:\n",
    "            full_dataset = covid_train\n",
    "        \n",
    "        make_and_save_full_dataset(full_dataset, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "        \n",
    "        run_gradual_ft(output_dir, checkpoint, covid_val)\n",
    "        \n",
    "        checkpoint = output_dir\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
