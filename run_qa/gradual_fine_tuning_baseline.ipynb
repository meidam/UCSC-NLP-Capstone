{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "jsonfile = open(covid_file, 'r')\n",
    "\n",
    "covidQA = jsonfile.read()\n",
    "jsonfile.close()\n",
    "\n",
    "covid_data = json.loads(covidQA)\n",
    "def convert_to_squad_format(data):\n",
    "    covid_qa_squad_format = []\n",
    "    for rows in data['data']:\n",
    "      for context in rows['paragraphs']:\n",
    "        for qa_pairs in context['qas']:\n",
    "          features = {'id':str(context['document_id']),\n",
    "                      'title': 'COVID_19',\n",
    "                      'context':str(context['context']),\n",
    "                      'question':qa_pairs['question'],\n",
    "                      'answers':{'answer_start': np.array([qa_pairs['answers'][0]['answer_start']], dtype=np.int32),\n",
    "                                 'text':[qa_pairs['answers'][0]['text']]}}\n",
    "          covid_qa_squad_format.append(features)\n",
    "    covid_df = pd.DataFrame(covid_qa_squad_format)\n",
    "    return (datasets.Dataset.from_dict(covid_df))#.train_test_split(test_size=0.2)\n",
    "\n",
    "def make_and_save_full_dataset(train, valid, test, path):\n",
    "    full_data = datasets.dataset_dict.DatasetDict({'train':train, 'validation':valid, 'test': test})\n",
    "    full_data.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import run_qa\n",
    "\n",
    "def run_gradual_ft(output_dir, checkpoint, covid_val):\n",
    "    !python run_qa.py \\\n",
    "      --model_name_or_path {checkpoint} \\\n",
    "      --dataset_name ../data/full_squad_covidQA/ \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --do_predict \\\n",
    "      --per_device_train_batch_size 32\\\n",
    "      --per_device_eval_batch_size 32\\\n",
    "      --evaluation_strategy \"no\" \\\n",
    "      --save_strategy \"no\" \\\n",
    "      --logging_strategy \"epoch\" \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --max_seq_length 384 \\\n",
    "      --doc_stride 128 \\\n",
    "      --output_dir {output_dir} \\\n",
    "      --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a396986d6b3a2375\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n"
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = covid_file\n",
    "\n",
    "covid_qa = load_dataset('custom_squad.py', data_files=data_files)['train']\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "covid_and_squad_dataset_path = \"../data/full_squad_covidQA\"\n",
    "\n",
    "squad_qa = datasets.Dataset.from_dict(squad_qa[:50])\n",
    "covid_qa = datasets.Dataset.from_dict(covid_qa[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:31:17 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:31:17 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\runs\\Jun29_14-31-17_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\runs\\Jun29_14-31-17_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:31:22 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-e0301d4c83a8b2f3.arrow\n",
      "06/29/2021 14:31:23 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-ca7f7a1f77567014.arrow\n",
      "{'loss': 5.2999, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.3134, 'train_samples_per_second': 63.316, 'train_steps_per_second': 2.036, 'train_loss': 5.299904232933407, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2999\n",
      "  train_runtime            = 0:00:10.31\n",
      "  train_samples            =        653\n",
      "  train_samples_per_second =     63.316\n",
      "  train_steps_per_second   =      2.036\n",
      "06/29/2021 14:31:36 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:31:36 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:31:36 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\eval_predictions.json.\n",
      "06/29/2021 14:31:36 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:31:36 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:31:37 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/29/2021 14:31:37 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\predict_predictions.json.\n",
      "06/29/2021 14:31:37 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-29 14:31:18,760 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:31:18,760 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 14:31:19,122 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 14:31:19,482 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:31:19,483 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:31:21,589 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:31:21,589 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:31:21,589 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:31:21,589 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:31:21,589 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:31:21,589 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 14:31:22,011 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 14:31:22,714 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 14:31:22,715 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.13ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.10ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:31:25,033 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:31:25,033 >>   Num examples = 653\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:31:25,034 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:31:25,034 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:31:25,034 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:31:25,034 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:31:25,034 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:27,  1.35s/it]\n",
      " 10%|9         | 2/21 [00:01<00:20,  1.09s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:16,  1.11it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:13,  1.31it/s]\n",
      " 24%|##3       | 5/21 [00:03<00:10,  1.49it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:09,  1.64it/s]\n",
      " 33%|###3      | 7/21 [00:04<00:07,  1.77it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  1.87it/s]\n",
      " 43%|####2     | 9/21 [00:05<00:06,  1.96it/s]\n",
      " 48%|####7     | 10/21 [00:05<00:05,  2.02it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.06it/s]\n",
      " 57%|#####7    | 12/21 [00:06<00:04,  2.10it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.11it/s]\n",
      " 67%|######6   | 14/21 [00:07<00:03,  2.13it/s]\n",
      " 71%|#######1  | 15/21 [00:07<00:02,  2.14it/s]\n",
      " 76%|#######6  | 16/21 [00:08<00:02,  2.15it/s]\n",
      " 81%|########  | 17/21 [00:08<00:01,  2.16it/s]\n",
      " 86%|########5 | 18/21 [00:09<00:01,  2.16it/s]\n",
      " 90%|######### | 19/21 [00:09<00:00,  2.17it/s]\n",
      " 95%|#########5| 20/21 [00:10<00:00,  2.17it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  2.58it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.58it/s][INFO|trainer.py:1349] 2021-06-29 14:31:35,347 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.58it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  2.04it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:31:35,348 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:31:35,349 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:31:36,093 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:31:36,093 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:31:36,094 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:31:36,174 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:31:36,176 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:31:36,176 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:31:36,176 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.75it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:31:36,867 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:31:36,869 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:31:36,869 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:31:36,869 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 13.70it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.76it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.76it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:31:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:31:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\runs\\Jun29_14-31-39_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\runs\\Jun29_14-31-39_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:31:41 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-49bfc8dc8d8adb2f.arrow\n",
      "06/29/2021 14:31:41 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-f497013eaf80977b.arrow\n",
      "06/29/2021 14:31:41 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-464e339fc655305a.arrow\n",
      "{'loss': 3.1484, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.222, 'train_samples_per_second': 62.903, 'train_steps_per_second': 2.054, 'train_loss': 3.148413521902902, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.1484\n",
      "  train_runtime            = 0:00:10.22\n",
      "  train_samples            =        643\n",
      "  train_samples_per_second =     62.903\n",
      "  train_steps_per_second   =      2.054\n",
      "06/29/2021 14:31:54 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:31:54 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:31:55 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\eval_predictions.json.\n",
      "06/29/2021 14:31:55 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:31:55 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:31:55 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/29/2021 14:31:55 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\predict_predictions.json.\n",
      "06/29/2021 14:31:55 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:31:40,232 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:31:40,233 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:31:40,242 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:40,242 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:40,242 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:40,242 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:40,242 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:40,242 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:40,242 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:31:40,313 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:31:41,039 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:31:41,039 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:31:43,419 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:31:43,419 >>   Num examples = 643\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:31:43,419 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:31:43,419 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:31:43,419 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:31:43,419 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:31:43,419 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:27,  1.36s/it]\n",
      " 10%|9         | 2/21 [00:01<00:20,  1.09s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:16,  1.10it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:13,  1.29it/s]\n",
      " 24%|##3       | 5/21 [00:03<00:10,  1.47it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:09,  1.62it/s]\n",
      " 33%|###3      | 7/21 [00:04<00:07,  1.76it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  1.87it/s]\n",
      " 43%|####2     | 9/21 [00:05<00:06,  1.95it/s]\n",
      " 48%|####7     | 10/21 [00:05<00:05,  2.02it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.06it/s]\n",
      " 57%|#####7    | 12/21 [00:06<00:04,  2.08it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.11it/s]\n",
      " 67%|######6   | 14/21 [00:07<00:03,  2.13it/s]\n",
      " 71%|#######1  | 15/21 [00:07<00:02,  2.14it/s]\n",
      " 76%|#######6  | 16/21 [00:08<00:02,  2.15it/s]\n",
      " 81%|########  | 17/21 [00:08<00:01,  2.16it/s]\n",
      " 86%|########5 | 18/21 [00:09<00:01,  2.16it/s]\n",
      " 90%|######### | 19/21 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 20/21 [00:10<00:00,  2.16it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.16it/s][INFO|trainer.py:1349] 2021-06-29 14:31:53,641 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.16it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  2.05it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:31:53,642 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:31:53,643 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:31:54,392 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:31:54,392 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:31:54,393 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:31:54,476 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:31:54,478 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:31:54,478 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:31:54,478 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.73it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:31:55,168 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:31:55,170 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:31:55,170 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:31:55,170 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 13.51it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.83it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.83it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:31:58 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:31:58 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\runs\\Jun29_14-31-57_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\runs\\Jun29_14-31-57_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:31:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-79f28314f3c192af.arrow\n",
      "06/29/2021 14:31:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-8f697b28e47e0051.arrow\n",
      "06/29/2021 14:31:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-d355f6c4ddef5440.arrow\n",
      "{'loss': 1.8732, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.0516, 'train_samples_per_second': 62.975, 'train_steps_per_second': 1.99, 'train_loss': 1.8732284545898437, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8732\n",
      "  train_runtime            = 0:00:10.05\n",
      "  train_samples            =        633\n",
      "  train_samples_per_second =     62.975\n",
      "  train_steps_per_second   =       1.99\n",
      "06/29/2021 14:32:12 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:32:12 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:32:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\eval_predictions.json.\n",
      "06/29/2021 14:32:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:32:12 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:32:13 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/29/2021 14:32:13 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\predict_predictions.json.\n",
      "06/29/2021 14:32:13 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:31:58,464 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:31:58,465 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:31:58,467 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:58,467 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:58,467 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:58,467 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:58,467 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:58,467 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:31:58,467 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:31:58,539 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:31:59,284 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:31:59,284 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:32:01,319 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:32:01,319 >>   Num examples = 633\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:32:01,319 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:32:01,319 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:32:01,319 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:32:01,319 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:32:01,319 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.32s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.06s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.13it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.76it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.86it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.95it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.01it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.05it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.08it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.11it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.15it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.15it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  2.28it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.28it/s][INFO|trainer.py:1349] 2021-06-29 14:32:11,371 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.28it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  1.99it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:32:11,372 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:32:11,373 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:32:12,194 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:32:12,195 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:32:12,195 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:32:12,280 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:32:12,282 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:32:12,282 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:32:12,282 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.66it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.62it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.62it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.61it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:32:12,985 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:32:12,987 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:32:12,987 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:32:12,987 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 13.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.24it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.24it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:32:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:32:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\runs\\Jun29_14-32-16_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\runs\\Jun29_14-32-16_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:32:17 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-06c156be0f2946a3.arrow\n",
      "06/29/2021 14:32:17 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-e4d76e09a0aea673.arrow\n",
      "06/29/2021 14:32:17 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-15ba932a470a1040.arrow\n",
      "{'loss': 1.1048, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.0917, 'train_samples_per_second': 61.734, 'train_steps_per_second': 1.982, 'train_loss': 1.1047846794128418, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.1048\n",
      "  train_runtime            = 0:00:10.09\n",
      "  train_samples            =        623\n",
      "  train_samples_per_second =     61.734\n",
      "  train_steps_per_second   =      1.982\n",
      "06/29/2021 14:32:30 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:32:31 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:32:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\eval_predictions.json.\n",
      "06/29/2021 14:32:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:32:31 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:32:32 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/29/2021 14:32:32 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\predict_predictions.json.\n",
      "06/29/2021 14:32:32 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:32:16,582 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:32:16,582 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:32:16,584 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:16,584 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:16,584 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:16,584 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:16,584 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:16,584 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:16,584 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:32:16,655 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:32:17,604 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:32:17,604 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:32:19,795 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:32:19,795 >>   Num examples = 623\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:32:19,795 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:32:19,795 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:32:19,795 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:32:19,795 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:32:19,795 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.35s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.11s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.08it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.27it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.41it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.57it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.71it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.82it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.91it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:05,  1.98it/s]\n",
      " 55%|#####5    | 11/20 [00:06<00:04,  2.04it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.08it/s]\n",
      " 65%|######5   | 13/20 [00:07<00:03,  2.11it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:08<00:02,  2.13it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.15it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  2.51it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.51it/s][INFO|trainer.py:1349] 2021-06-29 14:32:29,886 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.51it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  1.98it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:32:29,888 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:32:29,889 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:32:30,725 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:32:30,726 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:32:30,726 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:32:30,809 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:32:30,811 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:32:30,811 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:32:30,811 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  7.38it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  7.35it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  4.60it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:32:31,633 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:32:31,636 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:32:31,636 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:32:31,636 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 13.51it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.63it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.63it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:32:34 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:32:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\runs\\Jun29_14-32-34_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\runs\\Jun29_14-32-34_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:32:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-709488eaa3bdc2d4.arrow\n",
      "06/29/2021 14:32:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-eea4db4d745df6a1.arrow\n",
      "06/29/2021 14:32:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-92a5874a09a54604.arrow\n",
      "{'loss': 0.6285, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.9369, 'train_samples_per_second': 62.695, 'train_steps_per_second': 2.013, 'train_loss': 0.6284981727600097, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.6285\n",
      "  train_runtime            = 0:00:09.93\n",
      "  train_samples            =        623\n",
      "  train_samples_per_second =     62.695\n",
      "  train_steps_per_second   =      2.013\n",
      "06/29/2021 14:32:49 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:32:49 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:32:49 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\eval_predictions.json.\n",
      "06/29/2021 14:32:49 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:32:49 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:32:50 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/29/2021 14:32:50 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\predict_predictions.json.\n",
      "06/29/2021 14:32:50 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:32:34,990 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:32:34,991 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:32:34,993 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:34,993 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:34,993 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:34,993 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:34,993 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:34,993 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:32:34,993 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:32:35,063 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:32:35,785 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:32:35,786 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:32:38,434 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:32:38,434 >>   Num examples = 623\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:32:38,434 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:32:38,435 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:32:38,435 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:32:38,435 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:32:38,435 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.32s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.13it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.32it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.76it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.86it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.94it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:05,  2.00it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.05it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.08it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.10it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.11it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.13it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.14it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.15it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.15it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.49it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.49it/s][INFO|trainer.py:1349] 2021-06-29 14:32:48,371 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.49it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.01it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:32:48,373 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:32:48,374 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:32:49,058 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:32:49,059 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:32:49,059 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:32:49,143 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:32:49,144 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:32:49,144 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:32:49,144 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.71it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:32:49,835 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:32:49,837 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:32:49,837 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:32:49,837 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 13.70it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.90it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.83it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:32:52 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:32:52 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\runs\\Jun29_14-32-52_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\runs\\Jun29_14-32-52_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:32:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-c8cc08e8baa4b4e6.arrow\n",
      "06/29/2021 14:32:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-5061944aab53887d.arrow\n",
      "{'loss': 5.2692, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.2438, 'train_samples_per_second': 62.575, 'train_steps_per_second': 2.05, 'train_loss': 5.269199552990141, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2692\n",
      "  train_runtime            = 0:00:10.24\n",
      "  train_samples            =        641\n",
      "  train_samples_per_second =     62.575\n",
      "  train_steps_per_second   =       2.05\n",
      "06/29/2021 14:33:10 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:33:11 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:33:11 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\eval_predictions.json.\n",
      "06/29/2021 14:33:11 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:33:11 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:33:12 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:33:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\predict_predictions.json.\n",
      "06/29/2021 14:33:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-29 14:32:53,534 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:32:53,535 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 14:32:53,899 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 14:32:54,257 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:32:54,258 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:32:56,385 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:32:56,385 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:32:56,385 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:32:56,385 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:32:56,385 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:32:56,385 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 14:32:56,814 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 14:32:57,520 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 14:32:57,520 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.41ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:32:59,812 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:32:59,812 >>   Num examples = 641\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:32:59,812 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:32:59,812 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:32:59,812 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:32:59,812 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:32:59,812 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:27,  1.37s/it]\n",
      " 10%|9         | 2/21 [00:01<00:20,  1.10s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:16,  1.10it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:13,  1.29it/s]\n",
      " 24%|##3       | 5/21 [00:03<00:10,  1.46it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:09,  1.62it/s]\n",
      " 33%|###3      | 7/21 [00:04<00:07,  1.76it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  1.86it/s]\n",
      " 43%|####2     | 9/21 [00:05<00:06,  1.94it/s]\n",
      " 48%|####7     | 10/21 [00:05<00:05,  2.00it/s]\n",
      " 52%|#####2    | 11/21 [00:06<00:04,  2.05it/s]\n",
      " 57%|#####7    | 12/21 [00:06<00:04,  2.07it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.10it/s]\n",
      " 67%|######6   | 14/21 [00:07<00:03,  2.12it/s]\n",
      " 71%|#######1  | 15/21 [00:07<00:02,  2.14it/s]\n",
      " 76%|#######6  | 16/21 [00:08<00:02,  2.14it/s]\n",
      " 81%|########  | 17/21 [00:08<00:01,  2.14it/s]\n",
      " 86%|########5 | 18/21 [00:09<00:01,  2.15it/s]\n",
      " 90%|######### | 19/21 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 20/21 [00:10<00:00,  2.16it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.16it/s][INFO|trainer.py:1349] 2021-06-29 14:33:10,056 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.16it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  2.05it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:33:10,057 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:33:10,058 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:33:10,827 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:33:10,828 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:33:10,828 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:33:10,940 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:33:10,942 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:33:10,942 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:33:10,942 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.48it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:33:11,658 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:33:11,660 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:33:11,660 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:33:11,660 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.66it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:33:14 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:33:14 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\runs\\Jun29_14-33-14_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\runs\\Jun29_14-33-14_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:33:16 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-808e32908b4ffc43.arrow\n",
      "06/29/2021 14:33:16 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-6f318787ffa27643.arrow\n",
      "06/29/2021 14:33:16 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-8124f79e2f851a33.arrow\n",
      "{'loss': 3.0902, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.2217, 'train_samples_per_second': 61.731, 'train_steps_per_second': 1.957, 'train_loss': 3.0901586532592775, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.0902\n",
      "  train_runtime            = 0:00:10.22\n",
      "  train_samples            =        631\n",
      "  train_samples_per_second =     61.731\n",
      "  train_steps_per_second   =      1.957\n",
      "06/29/2021 14:33:29 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:33:29 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:33:29 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\eval_predictions.json.\n",
      "06/29/2021 14:33:29 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:33:29 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:33:30 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:33:30 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\predict_predictions.json.\n",
      "06/29/2021 14:33:30 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:33:15,088 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:33:15,088 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:33:15,090 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:15,091 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:15,091 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:15,091 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:15,091 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:15,091 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:15,091 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:33:15,163 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:33:15,998 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:33:15,998 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:33:18,065 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:33:18,065 >>   Num examples = 631\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:33:18,065 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:33:18,065 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:33:18,065 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:33:18,065 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:33:18,065 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.35s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.09s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.11it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.30it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.42it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.56it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.70it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.81it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.88it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:05,  1.95it/s]\n",
      " 55%|#####5    | 11/20 [00:06<00:04,  1.99it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.04it/s]\n",
      " 65%|######5   | 13/20 [00:07<00:03,  2.08it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.08it/s]\n",
      " 75%|#######5  | 15/20 [00:08<00:02,  2.10it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.12it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.13it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.14it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.15it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  2.32it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.32it/s][INFO|trainer.py:1349] 2021-06-29 14:33:28,287 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.32it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  1.96it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:33:28,289 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:33:28,290 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:33:29,051 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:33:29,052 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:33:29,052 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:33:29,135 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:33:29,137 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:33:29,137 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:33:29,137 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.68it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:33:29,835 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:33:29,837 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:33:29,837 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:33:29,837 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:33:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:33:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\runs\\Jun29_14-33-32_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\runs\\Jun29_14-33-32_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:33:34 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-8e1fac9f624895fd.arrow\n",
      "06/29/2021 14:33:34 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-af482c60e1e94513.arrow\n",
      "06/29/2021 14:33:34 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-8768eedb50e527f2.arrow\n",
      "{'loss': 1.9003, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.8451, 'train_samples_per_second': 63.077, 'train_steps_per_second': 2.031, 'train_loss': 1.90033016204834, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.9003\n",
      "  train_runtime            = 0:00:09.84\n",
      "  train_samples            =        621\n",
      "  train_samples_per_second =     63.077\n",
      "  train_steps_per_second   =      2.031\n",
      "06/29/2021 14:33:47 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:33:48 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:33:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\eval_predictions.json.\n",
      "06/29/2021 14:33:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:33:48 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:33:48 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:33:49 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\predict_predictions.json.\n",
      "06/29/2021 14:33:49 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:33:33,566 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:33:33,566 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:33:33,568 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:33,568 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:33,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:33,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:33,569 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:33,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:33,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:33:33,639 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:33:34,369 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:33:34,369 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:33:37,049 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:33:37,050 >>   Num examples = 621\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:33:37,050 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:33:37,050 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:33:37,050 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:33:37,050 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:33:37,050 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.33s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.12it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.88it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.14it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.57it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.57it/s][INFO|trainer.py:1349] 2021-06-29 14:33:46,895 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.57it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.03it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:33:46,896 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:33:46,897 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:33:47,623 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:33:47,623 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:33:47,624 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:33:47,707 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:33:47,708 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:33:47,709 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:33:47,709 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.74it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:33:48,399 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:33:48,401 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:33:48,401 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:33:48,401 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:33:51 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:33:51 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\runs\\Jun29_14-33-51_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\runs\\Jun29_14-33-51_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:33:53 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-b5bf6115a3dc2823.arrow\n",
      "06/29/2021 14:33:53 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-cb414f7cb60e2467.arrow\n",
      "06/29/2021 14:33:53 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-e68b89edd380b40e.arrow\n",
      "{'loss': 0.9972, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.721, 'train_samples_per_second': 62.853, 'train_steps_per_second': 2.057, 'train_loss': 0.9971704483032227, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.9972\n",
      "  train_runtime            = 0:00:09.72\n",
      "  train_samples            =        611\n",
      "  train_samples_per_second =     62.853\n",
      "  train_steps_per_second   =      2.057\n",
      "06/29/2021 14:34:05 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:34:06 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:34:06 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\eval_predictions.json.\n",
      "06/29/2021 14:34:06 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:34:06 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:34:07 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:34:07 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\predict_predictions.json.\n",
      "06/29/2021 14:34:07 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:33:52,366 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:33:52,367 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:33:52,369 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:52,369 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:52,369 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:52,369 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:52,369 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:52,370 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:33:52,370 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:33:52,447 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:33:53,180 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:33:53,180 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:33:55,398 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:33:55,398 >>   Num examples = 611\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:33:55,398 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:33:55,398 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:33:55,398 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:33:55,398 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:33:55,398 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.33s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.13it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.87it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.05it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.11it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.14it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.15it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.15it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.16it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.16it/s][INFO|trainer.py:1349] 2021-06-29 14:34:05,119 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.16it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.06it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:34:05,120 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:34:05,121 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:34:05,871 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:34:05,871 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:34:05,872 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:34:05,956 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:34:05,958 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:34:05,958 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:34:05,958 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.71it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:34:06,651 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:34:06,653 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:34:06,653 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:34:06,653 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:34:09 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:34:09 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\runs\\Jun29_14-34-09_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\runs\\Jun29_14-34-09_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:34:10 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-2ba882bafba55d91.arrow\n",
      "06/29/2021 14:34:10 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-4016765f70970a03.arrow\n",
      "06/29/2021 14:34:10 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-4178d1588ae38a5c.arrow\n",
      "{'loss': 0.7021, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.6969, 'train_samples_per_second': 63.01, 'train_steps_per_second': 2.063, 'train_loss': 0.7020606994628906, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.7021\n",
      "  train_runtime            = 0:00:09.69\n",
      "  train_samples            =        611\n",
      "  train_samples_per_second =      63.01\n",
      "  train_steps_per_second   =      2.063\n",
      "06/29/2021 14:34:23 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:34:24 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:34:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\eval_predictions.json.\n",
      "06/29/2021 14:34:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/29/2021 14:34:24 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:34:24 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:34:25 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\predict_predictions.json.\n",
      "06/29/2021 14:34:25 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:34:10,080 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:34:10,080 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:34:10,082 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:10,083 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:10,083 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:10,083 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:10,083 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:10,083 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:10,083 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:34:10,152 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:34:10,872 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:34:10,872 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:34:13,430 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:34:13,430 >>   Num examples = 611\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:34:13,431 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:34:13,431 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:34:13,431 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:34:13,431 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:34:13,431 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.32s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.13it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.32it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.50it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.65it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.78it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.88it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.17it/s][INFO|trainer.py:1349] 2021-06-29 14:34:23,128 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.06it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:34:23,130 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:34:23,131 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:34:23,739 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:34:23,740 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:34:23,740 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:34:23,822 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:34:23,824 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:34:23,824 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:34:23,824 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.64it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:34:24,523 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:34:24,525 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:34:24,525 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:34:24,525 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.58it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:34:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:34:27 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\runs\\Jun29_14-34-27_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\runs\\Jun29_14-34-27_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:34:33 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-8dafc60597774b58.arrow\n",
      "06/29/2021 14:34:33 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-b3707794488534c7.arrow\n",
      "{'loss': 5.3296, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.0677, 'train_samples_per_second': 63.47, 'train_steps_per_second': 1.987, 'train_loss': 5.329554748535156, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3296\n",
      "  train_runtime            = 0:00:10.06\n",
      "  train_samples            =        639\n",
      "  train_samples_per_second =      63.47\n",
      "  train_steps_per_second   =      1.987\n",
      "06/29/2021 14:34:46 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:34:46 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:34:47 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\eval_predictions.json.\n",
      "06/29/2021 14:34:47 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/29/2021 14:34:47 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:34:47 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:34:47 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\predict_predictions.json.\n",
      "06/29/2021 14:34:47 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-29 14:34:28,370 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:34:28,371 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 14:34:28,727 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 14:34:29,073 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:34:29,073 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:34:31,872 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:34:31,872 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:34:31,872 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:34:31,872 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:34:31,872 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:34:31,872 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 14:34:32,479 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 14:34:33,189 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 14:34:33,190 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.62ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.58ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:34:35,599 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:34:35,599 >>   Num examples = 639\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:34:35,599 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:34:35,599 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:34:35,599 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:34:35,600 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:34:35,600 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.33s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.12it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.88it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.11it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.14it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  2.20it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.20it/s][INFO|trainer.py:1349] 2021-06-29 14:34:45,668 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.20it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  1.99it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:34:45,669 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:34:45,670 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:34:46,271 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:34:46,271 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:34:46,272 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:34:46,351 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:34:46,353 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:34:46,353 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:34:46,353 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.88it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:34:47,029 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:34:47,031 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:34:47,031 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:34:47,031 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:34:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:34:50 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\runs\\Jun29_14-34-49_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\runs\\Jun29_14-34-49_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/29/2021 14:34:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-9e553808c928e67a.arrow\n",
      "06/29/2021 14:34:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-d3cd7eeb6c53bddf.arrow\n",
      "06/29/2021 14:34:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-1ec5d44515e20cfc.arrow\n",
      "{'loss': 3.1937, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.9247, 'train_samples_per_second': 63.377, 'train_steps_per_second': 2.015, 'train_loss': 3.193742561340332, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.1937\n",
      "  train_runtime            = 0:00:09.92\n",
      "  train_samples            =        629\n",
      "  train_samples_per_second =     63.377\n",
      "  train_steps_per_second   =      2.015\n",
      "06/29/2021 14:35:04 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:35:05 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:35:05 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\eval_predictions.json.\n",
      "06/29/2021 14:35:05 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/29/2021 14:35:05 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:35:05 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:35:05 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\predict_predictions.json.\n",
      "06/29/2021 14:35:05 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:34:50,480 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:34:50,481 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:34:50,483 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:50,483 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:50,483 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:50,483 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:50,483 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:50,483 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:34:50,483 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:34:50,556 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:34:51,284 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:34:51,284 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:34:53,884 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:34:53,884 >>   Num examples = 629\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:34:53,884 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:34:53,884 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:34:53,884 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:34:53,884 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:34:53,884 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.33s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.13it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.32it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.50it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.65it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.78it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.88it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.14it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.17it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.39it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.39it/s][INFO|trainer.py:1349] 2021-06-29 14:35:03,809 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.39it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.02it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:35:03,810 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:35:03,811 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:35:04,445 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:35:04,446 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:35:04,446 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:35:04,543 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:35:04,545 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:35:04,545 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:35:04,545 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.86it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:35:05,223 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:35:05,224 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:35:05,224 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:35:05,224 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:35:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:35:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\runs\\Jun29_14-35-08_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\runs\\Jun29_14-35-08_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.8815, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.8434, 'train_samples_per_second': 62.885, 'train_steps_per_second': 2.032, 'train_loss': 1.8815019607543946, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8815\n",
      "  train_runtime            = 0:00:09.84\n",
      "  train_samples            =        619\n",
      "  train_samples_per_second =     62.885\n",
      "  train_steps_per_second   =      2.032\n",
      "06/29/2021 14:35:22 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:35:23 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:35:23 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\eval_predictions.json.\n",
      "06/29/2021 14:35:23 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/29/2021 14:35:23 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:35:23 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:35:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\predict_predictions.json.\n",
      "06/29/2021 14:35:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:35:08,666 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:35:08,667 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:35:08,676 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:08,676 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:08,676 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:08,676 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:08,677 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:08,677 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:08,677 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:35:08,748 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:35:09,471 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:35:09,471 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.22ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.22ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.35ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.35ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:35:12,232 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:35:12,232 >>   Num examples = 619\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:35:12,232 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:35:12,232 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:35:12,232 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:35:12,232 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:35:12,232 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:26,  1.37s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.10s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.10it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.29it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.47it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.63it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.87it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.07it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.10it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.14it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.62it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.62it/s][INFO|trainer.py:1349] 2021-06-29 14:35:22,074 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.62it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.03it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:35:22,076 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:35:22,077 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:35:22,704 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:35:22,705 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:35:22,705 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:35:22,791 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:35:22,793 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:35:22,793 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:35:22,793 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.78it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:35:23,476 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:35:23,478 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:35:23,478 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:35:23,478 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:35:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:35:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\runs\\Jun29_14-35-26_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\runs\\Jun29_14-35-26_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.0059, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.6948, 'train_samples_per_second': 62.817, 'train_steps_per_second': 2.063, 'train_loss': 1.0058643341064453, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.0059\n",
      "  train_runtime            = 0:00:09.69\n",
      "  train_samples            =        609\n",
      "  train_samples_per_second =     62.817\n",
      "  train_steps_per_second   =      2.063\n",
      "06/29/2021 14:35:40 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:35:41 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:35:41 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\eval_predictions.json.\n",
      "06/29/2021 14:35:41 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/29/2021 14:35:41 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:35:41 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:35:42 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\predict_predictions.json.\n",
      "06/29/2021 14:35:42 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:35:26,926 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:35:26,927 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:35:26,936 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:26,936 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:26,936 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:26,936 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:26,936 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:26,937 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:26,937 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:35:27,007 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:35:27,728 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:35:27,728 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.33ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.33ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.25ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.25ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:35:30,243 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:35:30,243 >>   Num examples = 609\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:35:30,243 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:35:30,243 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:35:30,243 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:35:30,243 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:35:30,243 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.35s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.08s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.12it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.87it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.07it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.10it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.13it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.14it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.16it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.16it/s][INFO|trainer.py:1349] 2021-06-29 14:35:39,938 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.16it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.06it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:35:39,939 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:35:39,940 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:35:40,568 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:35:40,569 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:35:40,570 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:35:40,658 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:35:40,660 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:35:40,660 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:35:40,660 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.84it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:35:41,340 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:35:41,342 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:35:41,342 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:35:41,342 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:35:44 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:35:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\runs\\Jun29_14-35-44_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\runs\\Jun29_14-35-44_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.6221, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.695, 'train_samples_per_second': 62.816, 'train_steps_per_second': 2.063, 'train_loss': 0.6221190452575683, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.6221\n",
      "  train_runtime            = 0:00:09.69\n",
      "  train_samples            =        609\n",
      "  train_samples_per_second =     62.816\n",
      "  train_steps_per_second   =      2.063\n",
      "06/29/2021 14:35:58 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:35:59 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/29/2021 14:35:59 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\eval_predictions.json.\n",
      "06/29/2021 14:35:59 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/29/2021 14:35:59 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:35:59 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:35:59 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\predict_predictions.json.\n",
      "06/29/2021 14:35:59 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:35:44,774 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:35:44,775 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:35:44,784 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:44,784 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:44,784 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:44,784 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:44,784 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:44,784 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:35:44,784 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:35:44,858 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:35:45,585 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:35:45,586 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.22ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.22ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.35ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.35ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:35:48,148 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:35:48,148 >>   Num examples = 609\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:35:48,148 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:35:48,148 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:35:48,148 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:35:48,148 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:35:48,148 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.34s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.12it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.65it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.78it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.88it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.17it/s][INFO|trainer.py:1349] 2021-06-29 14:35:57,843 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.06it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:35:57,844 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:35:57,845 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:35:58,508 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:35:58,509 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:35:58,509 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:35:58,592 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:35:58,594 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:35:58,594 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:35:58,594 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.15it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.15it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.87it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:35:59,270 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:35:59,271 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:35:59,271 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:35:59,271 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.58it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.48it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.43it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:36:02 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:36:02 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\runs\\Jun29_14-36-02_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\runs\\Jun29_14-36-02_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 5.3183, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.0825, 'train_samples_per_second': 63.079, 'train_steps_per_second': 1.984, 'train_loss': 5.318293762207031, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3183\n",
      "  train_runtime            = 0:00:10.08\n",
      "  train_samples            =        636\n",
      "  train_samples_per_second =     63.079\n",
      "  train_steps_per_second   =      1.984\n",
      "06/29/2021 14:36:20 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:36:21 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:36:21 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\eval_predictions.json.\n",
      "06/29/2021 14:36:21 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/29/2021 14:36:21 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:36:21 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:36:21 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\predict_predictions.json.\n",
      "06/29/2021 14:36:21 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-29 14:36:03,199 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:36:03,200 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 14:36:03,562 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 14:36:03,923 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:36:03,924 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:36:06,063 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:36:06,064 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:36:06,064 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:36:06,064 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:36:06,064 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:36:06,064 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 14:36:06,483 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 14:36:07,198 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 14:36:07,198 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.27ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.27ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.01ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:36:09,771 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:36:09,771 >>   Num examples = 636\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:36:09,771 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:36:09,771 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:36:09,771 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:36:09,771 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:36:09,771 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:26,  1.37s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.10s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.10it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.29it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.47it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.63it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.76it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.87it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.95it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.01it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.11it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.14it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.15it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.16it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  2.24it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.24it/s][INFO|trainer.py:1349] 2021-06-29 14:36:19,853 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.24it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  1.98it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:36:19,855 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:36:19,856 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:36:20,454 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:36:20,455 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:36:20,455 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:36:20,543 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:36:20,545 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:36:20,545 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:36:20,545 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.46it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:36:21,261 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:36:21,263 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:36:21,263 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:36:21,263 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:36:24 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:36:24 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\runs\\Jun29_14-36-24_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\runs\\Jun29_14-36-24_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 3.2153, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.9066, 'train_samples_per_second': 63.19, 'train_steps_per_second': 2.019, 'train_loss': 3.2152862548828125, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2153\n",
      "  train_runtime            = 0:00:09.90\n",
      "  train_samples            =        626\n",
      "  train_samples_per_second =      63.19\n",
      "  train_steps_per_second   =      2.019\n",
      "06/29/2021 14:36:39 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:36:39 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:36:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\eval_predictions.json.\n",
      "06/29/2021 14:36:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/29/2021 14:36:39 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:36:40 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:36:40 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\predict_predictions.json.\n",
      "06/29/2021 14:36:40 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:36:24,733 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:36:24,733 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:36:24,735 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:24,735 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:24,735 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:24,735 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:24,735 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:24,735 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:24,735 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:36:24,806 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:36:25,532 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:36:25,532 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.24ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.24ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:36:28,259 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:36:28,259 >>   Num examples = 626\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:36:28,259 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:36:28,259 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:36:28,259 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:36:28,259 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:36:28,259 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.34s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.08s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.12it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.87it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.95it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.01it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.46it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.46it/s][INFO|trainer.py:1349] 2021-06-29 14:36:38,165 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.46it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.02it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:36:38,166 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:36:38,167 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:36:38,941 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:36:38,942 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:36:38,943 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:36:39,040 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:36:39,041 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:36:39,041 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:36:39,041 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.58it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.42it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:36:39,762 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:36:39,765 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:36:39,765 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:36:39,765 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:36:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:36:42 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\runs\\Jun29_14-36-42_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\runs\\Jun29_14-36-42_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.8734, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.775, 'train_samples_per_second': 63.018, 'train_steps_per_second': 2.046, 'train_loss': 1.8734224319458008, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8734\n",
      "  train_runtime            = 0:00:09.77\n",
      "  train_samples            =        616\n",
      "  train_samples_per_second =     63.018\n",
      "  train_steps_per_second   =      2.046\n",
      "06/29/2021 14:36:57 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:36:57 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:36:58 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\eval_predictions.json.\n",
      "06/29/2021 14:36:58 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/29/2021 14:36:58 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:36:58 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:36:58 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\predict_predictions.json.\n",
      "06/29/2021 14:36:58 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:36:43,220 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:36:43,221 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:36:43,230 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:43,230 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:43,230 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:43,230 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:43,231 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:43,231 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:36:43,231 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:36:43,302 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:36:44,042 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:36:44,042 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.95ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.94ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.62ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.62ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.09ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.09ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:36:46,941 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:36:46,941 >>   Num examples = 616\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:36:46,941 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:36:46,941 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:36:46,941 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:36:46,941 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:36:46,941 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.33s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.13it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.32it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.78it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.87it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.95it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.01it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.10it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.14it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.15it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.72it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.72it/s][INFO|trainer.py:1349] 2021-06-29 14:36:56,716 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.72it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.05it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:36:56,718 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:36:56,719 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:36:57,394 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:36:57,395 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:36:57,395 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:36:57,483 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:36:57,485 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:36:57,485 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:36:57,485 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.56it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:36:58,195 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:36:58,197 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:36:58,197 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:36:58,197 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:37:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:37:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\runs\\Jun29_14-37-01_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\runs\\Jun29_14-37-01_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.1853, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.5905, 'train_samples_per_second': 63.188, 'train_steps_per_second': 1.981, 'train_loss': 1.1853175916169818, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.1853\n",
      "  train_runtime            = 0:00:09.59\n",
      "  train_samples            =        606\n",
      "  train_samples_per_second =     63.188\n",
      "  train_steps_per_second   =      1.981\n",
      "06/29/2021 14:37:15 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:37:15 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:37:15 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\eval_predictions.json.\n",
      "06/29/2021 14:37:15 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/29/2021 14:37:15 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:37:16 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:37:16 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\predict_predictions.json.\n",
      "06/29/2021 14:37:16 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:37:01,625 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:37:01,626 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:37:01,635 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:01,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:01,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:01,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:01,635 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:01,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:01,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:37:01,706 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:37:02,423 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:37:02,423 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.33ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.31ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:37:04,930 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:37:04,930 >>   Num examples = 606\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:37:04,930 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:37:04,930 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:37:04,931 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:37:04,931 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:37:04,931 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.33s/it]\n",
      " 11%|#         | 2/19 [00:01<00:18,  1.07s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:14,  1.13it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:11,  1.32it/s]\n",
      " 26%|##6       | 5/19 [00:03<00:09,  1.49it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.65it/s]\n",
      " 37%|###6      | 7/19 [00:04<00:06,  1.78it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  1.88it/s]\n",
      " 47%|####7     | 9/19 [00:05<00:05,  1.96it/s]\n",
      " 53%|#####2    | 10/19 [00:05<00:04,  2.02it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.06it/s]\n",
      " 63%|######3   | 12/19 [00:06<00:03,  2.09it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.12it/s]\n",
      " 74%|#######3  | 14/19 [00:07<00:02,  2.14it/s]\n",
      " 79%|#######8  | 15/19 [00:07<00:01,  2.15it/s]\n",
      " 84%|########4 | 16/19 [00:08<00:01,  2.16it/s]\n",
      " 89%|########9 | 17/19 [00:08<00:00,  2.16it/s]\n",
      " 95%|#########4| 18/19 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 19/19 [00:09<00:00,  2.21it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:09<00:00,  2.21it/s][INFO|trainer.py:1349] 2021-06-29 14:37:14,522 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:09<00:00,  2.21it/s]\n",
      "100%|##########| 19/19 [00:09<00:00,  1.98it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:37:14,523 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:37:14,524 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:37:15,153 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:37:15,153 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:37:15,154 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:37:15,236 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:37:15,238 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:37:15,238 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:37:15,238 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.59it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:37:15,945 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:37:15,947 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:37:15,947 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:37:15,947 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:37:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:37:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\runs\\Jun29_14-37-18_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\runs\\Jun29_14-37-18_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.6738, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.5991, 'train_samples_per_second': 63.131, 'train_steps_per_second': 1.979, 'train_loss': 0.673768445065147, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.6738\n",
      "  train_runtime            = 0:00:09.59\n",
      "  train_samples            =        606\n",
      "  train_samples_per_second =     63.131\n",
      "  train_steps_per_second   =      1.979\n",
      "06/29/2021 14:37:33 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:37:33 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:37:33 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\eval_predictions.json.\n",
      "06/29/2021 14:37:33 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/29/2021 14:37:33 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:37:34 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/29/2021 14:37:34 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\predict_predictions.json.\n",
      "06/29/2021 14:37:34 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:37:19,371 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:37:19,372 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:37:19,381 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:19,381 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:19,381 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:19,381 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:19,381 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:19,381 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:37:19,381 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:37:19,454 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:37:20,179 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:37:20,179 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.31ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.31ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.77ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.77ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.30ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.30ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:37:22,768 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:37:22,768 >>   Num examples = 606\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:37:22,768 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:37:22,768 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:37:22,768 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:37:22,768 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:37:22,768 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:24,  1.33s/it]\n",
      " 11%|#         | 2/19 [00:01<00:18,  1.07s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:14,  1.12it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:11,  1.31it/s]\n",
      " 26%|##6       | 5/19 [00:03<00:09,  1.49it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.65it/s]\n",
      " 37%|###6      | 7/19 [00:04<00:06,  1.77it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  1.88it/s]\n",
      " 47%|####7     | 9/19 [00:05<00:05,  1.96it/s]\n",
      " 53%|#####2    | 10/19 [00:05<00:04,  2.02it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.06it/s]\n",
      " 63%|######3   | 12/19 [00:06<00:03,  2.10it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.12it/s]\n",
      " 74%|#######3  | 14/19 [00:07<00:02,  2.14it/s]\n",
      " 79%|#######8  | 15/19 [00:07<00:01,  2.14it/s]\n",
      " 84%|########4 | 16/19 [00:08<00:01,  2.15it/s]\n",
      " 89%|########9 | 17/19 [00:08<00:00,  2.16it/s]\n",
      " 95%|#########4| 18/19 [00:09<00:00,  2.16it/s]\n",
      "100%|##########| 19/19 [00:09<00:00,  2.21it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:09<00:00,  2.21it/s][INFO|trainer.py:1349] 2021-06-29 14:37:32,367 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:09<00:00,  2.21it/s]\n",
      "100%|##########| 19/19 [00:09<00:00,  1.98it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:37:32,369 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:37:32,370 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:37:32,985 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:37:32,986 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:37:32,987 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:37:33,069 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:37:33,071 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:37:33,071 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:37:33,071 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.56it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:37:33,777 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:37:33,779 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:37:33,779 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:37:33,779 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:37:36 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:37:36 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\runs\\Jun29_14-37-36_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\runs\\Jun29_14-37-36_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 5.3467, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.0574, 'train_samples_per_second': 63.535, 'train_steps_per_second': 1.989, 'train_loss': 5.346738052368164, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3467\n",
      "  train_runtime            = 0:00:10.05\n",
      "  train_samples            =        639\n",
      "  train_samples_per_second =     63.535\n",
      "  train_steps_per_second   =      1.989\n",
      "06/29/2021 14:37:56 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:37:57 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/29/2021 14:37:57 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\eval_predictions.json.\n",
      "06/29/2021 14:37:57 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/29/2021 14:37:57 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:37:57 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:37:57 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\predict_predictions.json.\n",
      "06/29/2021 14:37:57 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-29 14:37:38,778 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:37:38,778 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-29 14:37:39,246 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-29 14:37:39,599 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:37:39,600 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:37:41,732 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:37:41,732 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:37:41,732 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:37:41,732 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:37:41,732 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-29 14:37:41,732 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-29 14:37:42,143 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-29 14:37:42,863 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-29 14:37:42,863 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.07ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.07ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.30ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.30ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:37:45,618 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:37:45,618 >>   Num examples = 639\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:37:45,618 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:37:45,618 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:37:45,618 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:37:45,618 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:37:45,618 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.34s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.12it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.65it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.88it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.96it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.10it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.14it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.15it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.15it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.17it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  2.20it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.20it/s][INFO|trainer.py:1349] 2021-06-29 14:37:55,675 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:10<00:00,  2.20it/s]\n",
      "100%|##########| 20/20 [00:10<00:00,  1.99it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:37:55,677 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:37:55,677 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:37:56,456 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:37:56,457 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:37:56,457 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:37:56,541 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:37:56,543 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:37:56,543 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:37:56,543 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.58it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:37:57,247 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:37:57,249 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:37:57,249 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:37:57,249 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:38:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:38:00 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\runs\\Jun29_14-38-00_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\runs\\Jun29_14-38-00_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 3.2329, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.9432, 'train_samples_per_second': 63.259, 'train_steps_per_second': 2.011, 'train_loss': 3.2329483032226562, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2329\n",
      "  train_runtime            = 0:00:09.94\n",
      "  train_samples            =        629\n",
      "  train_samples_per_second =     63.259\n",
      "  train_steps_per_second   =      2.011\n",
      "06/29/2021 14:38:14 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:38:15 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/29/2021 14:38:15 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\eval_predictions.json.\n",
      "06/29/2021 14:38:15 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/29/2021 14:38:15 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:38:15 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:38:16 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\predict_predictions.json.\n",
      "06/29/2021 14:38:16 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:38:00,703 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:38:00,703 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:38:00,705 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:00,705 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:00,705 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:00,705 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:00,705 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:00,705 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:00,705 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:38:00,778 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:38:01,498 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:38:01,498 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:38:04,000 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:38:04,000 >>   Num examples = 629\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:38:04,000 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:38:04,000 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:38:04,000 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:38:04,000 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:38:04,000 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.34s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.12it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.31it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.65it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.87it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.95it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.14it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.14it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.15it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.40it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.40it/s][INFO|trainer.py:1349] 2021-06-29 14:38:13,943 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.40it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.01it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:38:13,944 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:38:13,945 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:38:14,538 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:38:14,539 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:38:14,539 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:38:14,624 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:38:14,626 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:38:14,626 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:38:14,626 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.58it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:38:15,332 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:38:15,334 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:38:15,334 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:38:15,334 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:38:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:38:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\runs\\Jun29_14-38-18_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\runs\\Jun29_14-38-18_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.8843, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.8228, 'train_samples_per_second': 63.016, 'train_steps_per_second': 2.036, 'train_loss': 1.884281539916992, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8843\n",
      "  train_runtime            = 0:00:09.82\n",
      "  train_samples            =        619\n",
      "  train_samples_per_second =     63.016\n",
      "  train_steps_per_second   =      2.036\n",
      "06/29/2021 14:38:32 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:38:33 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/29/2021 14:38:33 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\eval_predictions.json.\n",
      "06/29/2021 14:38:33 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/29/2021 14:38:33 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:38:33 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:38:34 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\predict_predictions.json.\n",
      "06/29/2021 14:38:34 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:38:18,803 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:38:18,803 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:38:18,813 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:18,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:18,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:18,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:18,813 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:18,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:18,813 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:38:18,888 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:38:19,604 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:38:19,604 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.37ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.35ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:38:22,138 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:38:22,138 >>   Num examples = 619\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:38:22,138 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:38:22,138 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:38:22,138 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:38:22,138 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:38:22,138 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.33s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.07s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.13it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.32it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.49it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.77it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.88it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.95it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.02it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.14it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.15it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.62it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.62it/s][INFO|trainer.py:1349] 2021-06-29 14:38:31,960 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.62it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.04it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:38:31,962 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:38:31,963 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:38:32,682 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:38:32,683 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:38:32,683 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:38:32,764 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:38:32,766 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:38:32,766 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:38:32,766 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.69it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:38:33,463 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:38:33,465 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:38:33,465 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:38:33,465 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:38:36 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:38:36 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\runs\\Jun29_14-38-36_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\runs\\Jun29_14-38-36_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.0007, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.7256, 'train_samples_per_second': 62.618, 'train_steps_per_second': 2.056, 'train_loss': 1.0006913185119628, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.0007\n",
      "  train_runtime            = 0:00:09.72\n",
      "  train_samples            =        609\n",
      "  train_samples_per_second =     62.618\n",
      "  train_steps_per_second   =      2.056\n",
      "06/29/2021 14:38:51 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:38:51 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/29/2021 14:38:52 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\eval_predictions.json.\n",
      "06/29/2021 14:38:52 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/29/2021 14:38:52 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:38:52 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:38:52 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\predict_predictions.json.\n",
      "06/29/2021 14:38:52 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:38:37,242 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:38:37,242 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:38:37,251 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:37,251 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:37,251 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:37,251 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:37,251 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:37,251 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:37,252 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:38:37,325 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:38:38,045 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:38:38,046 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.37ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.35ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.30ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.30ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:38:40,807 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:38:40,807 >>   Num examples = 609\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:38:40,807 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:38:40,807 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:38:40,807 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:38:40,807 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:38:40,807 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.36s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.09s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.11it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.30it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.48it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.64it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.76it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.87it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.95it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.01it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:04,  2.06it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.11it/s]\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.14it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.15it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.16it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.16it/s][INFO|trainer.py:1349] 2021-06-29 14:38:50,533 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.16it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.06it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:38:50,535 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:38:50,536 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:38:51,289 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:38:51,290 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:38:51,290 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:38:51,373 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:38:51,375 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:38:51,375 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:38:51,375 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.74it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.66it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:38:52,073 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:38:52,075 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:38:52,075 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:38:52,075 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/29/2021 14:38:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/29/2021 14:38:55 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\runs\\Jun29_14-38-54_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-29 14:38:55,472 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-29 14:38:55,473 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-29 14:38:55,482 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:55,482 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:55,482 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:55,482 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:55,482 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:55,482 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-29 14:38:55,482 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-29 14:38:55,552 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-29 14:38:56,290 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-29 14:38:56,290 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.29ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.93ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.93ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.35ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.35ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-29 14:38:58,989 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-29 14:38:58,990 >>   Num examples = 609\n",
      "[INFO|trainer.py:1155] 2021-06-29 14:38:58,990 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-29 14:38:58,990 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-29 14:38:58,990 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-29 14:38:58,990 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-29 14:38:58,990 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:26,  1.39s/it]\n",
      " 10%|#         | 2/20 [00:01<00:20,  1.11s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.09it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.28it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:10,  1.46it/s]\n",
      " 30%|###       | 6/20 [00:03<00:08,  1.62it/s]\n",
      " 35%|###5      | 7/20 [00:04<00:07,  1.75it/s]\n",
      " 40%|####      | 8/20 [00:04<00:06,  1.86it/s]\n",
      " 45%|####5     | 9/20 [00:05<00:05,  1.94it/s]\n",
      " 50%|#####     | 10/20 [00:05<00:04,  2.01it/s]\n",
      " 55%|#####5    | 11/20 [00:06<00:04,  2.05it/s]\n",
      " 60%|######    | 12/20 [00:06<00:03,  2.09it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:03,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Training Args:  TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\runs\\Jun29_14-38-54_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_4,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.5808, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.7522, 'train_samples_per_second': 62.447, 'train_steps_per_second': 2.051, 'train_loss': 0.5808442592620849, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.5808\n",
      "  train_runtime            = 0:00:09.75\n",
      "  train_samples            =        609\n",
      "  train_samples_per_second =     62.447\n",
      "  train_steps_per_second   =      2.051\n",
      "06/29/2021 14:39:09 - INFO - __main__ -   *** Evaluate ***\n",
      "06/29/2021 14:39:10 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/29/2021 14:39:10 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\eval_predictions.json.\n",
      "06/29/2021 14:39:10 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/29/2021 14:39:10 - INFO - __main__ -   *** Predict ***\n",
      "06/29/2021 14:39:10 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/29/2021 14:39:10 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\predict_predictions.json.\n",
      "06/29/2021 14:39:10 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|#######   | 14/20 [00:07<00:02,  2.13it/s]\n",
      " 75%|#######5  | 15/20 [00:07<00:02,  2.14it/s]\n",
      " 80%|########  | 16/20 [00:08<00:01,  2.16it/s]\n",
      " 85%|########5 | 17/20 [00:08<00:01,  2.16it/s]\n",
      " 90%|######### | 18/20 [00:09<00:00,  2.16it/s]\n",
      " 95%|#########5| 19/20 [00:09<00:00,  2.17it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.17it/s][INFO|trainer.py:1349] 2021-06-29 14:39:08,742 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:09<00:00,  2.17it/s]\n",
      "100%|##########| 20/20 [00:09<00:00,  2.05it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-29 14:39:08,744 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\n",
      "[INFO|configuration_utils.py:364] 2021-06-29 14:39:08,745 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-29 14:39:09,490 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-29 14:39:09,491 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-29 14:39:09,491 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_4\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-29 14:39:09,580 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:39:09,582 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:39:09,582 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:39:09,582 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 13.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.62it/s]\n",
      "[INFO|trainer.py:520] 2021-06-29 14:39:10,285 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-29 14:39:10,287 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-29 14:39:10,287 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-29 14:39:10,287 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 12.99it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\u001b[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.24it/s]\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "K = 5\n",
    "\n",
    "for i in range(k_fold):\n",
    "    covid_fold = covid_qa.shard(k_fold, i)\n",
    "\n",
    "    covid_test = covid_fold.shard(2, 0)\n",
    "    covid_val = covid_fold.shard(2, 1)\n",
    "    covid_train = concatenate_datasets([covid_qa.shard(k_fold, j) for j in range(k_fold) if j != i])\n",
    "\n",
    "    #make_and_save_full_dataset(covid_train, squad_qa, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "\n",
    "    checkpoint = 'roberta-base'\n",
    "    cur_dir = '../models/gradual_ft_baseline_lr1e-5_2/split_' + str(i)\n",
    "\n",
    "    squad_qa.shuffle()\n",
    "\n",
    "    for n in range(K):\n",
    "        output_dir = cur_dir + '/checkpoint_' + str(n)\n",
    "        #squad_qa = datasets.Dataset.from_dict(squad_qa[:-to_remove_per_step])\n",
    "\n",
    "        if n < K-1:\n",
    "            squad_qa_cur = concatenate_datasets([squad_qa.shard(K, j) for j in range(K-n-1)])\n",
    "            full_dataset = concatenate_datasets([squad_qa_cur, covid_train])\n",
    "        elif n < K:\n",
    "            squad_qa_cur = squad_qa.shard(K, 0)\n",
    "            full_dataset = concatenate_datasets([squad_qa_cur, covid_train])\n",
    "        else:\n",
    "            full_dataset = covid_train\n",
    "\n",
    "        make_and_save_full_dataset(full_dataset, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "\n",
    "        run_gradual_ft(output_dir, checkpoint, covid_val)\n",
    "\n",
    "        checkpoint = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
