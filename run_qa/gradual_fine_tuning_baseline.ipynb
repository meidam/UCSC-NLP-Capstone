{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "jsonfile = open(covid_file, 'r')\n",
    "\n",
    "covidQA = jsonfile.read()\n",
    "jsonfile.close()\n",
    "\n",
    "covid_data = json.loads(covidQA)\n",
    "def convert_to_squad_format(data):\n",
    "    covid_qa_squad_format = []\n",
    "    for rows in data['data']:\n",
    "      for context in rows['paragraphs']:\n",
    "        for qa_pairs in context['qas']:\n",
    "          features = {'id':str(context['document_id']),\n",
    "                      'title': 'COVID_19',\n",
    "                      'context':str(context['context']),\n",
    "                      'question':qa_pairs['question'],\n",
    "                      'answers':{'answer_start': np.array([qa_pairs['answers'][0]['answer_start']], dtype=np.int32),\n",
    "                                 'text':[qa_pairs['answers'][0]['text']]}}\n",
    "          covid_qa_squad_format.append(features)\n",
    "    covid_df = pd.DataFrame(covid_qa_squad_format)\n",
    "    return (datasets.Dataset.from_dict(covid_df))#.train_test_split(test_size=0.2)\n",
    "\n",
    "def make_and_save_full_dataset(covid,squad, path):\n",
    "    full_data = datasets.dataset_dict.DatasetDict({'covid':covid_qa, 'squad':squad_qa})\n",
    "    full_data.save_to_disk(\"../data/full_squad_covidQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "covid_qa = convert_to_squad_format(covid_data)\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "covid_and_squad_dataset_path = \"../data/full_squad_covidQA\"\n",
    "\n",
    "make_and_save_full_dataset(covid_qa,squad_qa,covid_and_squad_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_squad_covidQA/\n",
      "../data/full_squad_covidQA/\n",
      "06/20/2021 12:22:09 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/20/2021 12:22:09 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../models/gradual_ft_baseline, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=10, per_device_eval_batch_size=10, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs\\Jun20_12-22-09_DESKTOP-1CU6Q06, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "Training Args:  TrainingArguments(output_dir=../models/gradual_ft_baseline, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=10, per_device_eval_batch_size=10, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs\\Jun20_12-22-09_DESKTOP-1CU6Q06, logging_strategy=IntervalStrategy.EPOCH, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.NO, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../models/gradual_ft_baseline, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
      "made it to the datasets :  DatasetDict({\n",
      "    covid: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2019\n",
      "    })\n",
      "    squad: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 98169\n",
      "    })\n",
      "})\n",
      "its datasets are :  [Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 404\n",
      "}), Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 404\n",
      "}), Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 404\n",
      "}), Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 404\n",
      "}), Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 403\n",
      "})]\n",
      "outside the loop the cur_datasets are :  [Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 404\n",
      "}), Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 404\n",
      "}), Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 404\n",
      "}), Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 404\n",
      "}), Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 403\n",
      "})]\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:517] 2021-06-20 12:22:10,711 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-20 12:22:10,713 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-06-20 12:22:11,037 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-06-20 12:22:11,038 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-20 12:22:12,978 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-20 12:22:12,979 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-20 12:22:12,980 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-20 12:22:12,981 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-20 12:22:12,981 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-20 12:22:12,982 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1155] 2021-06-20 12:22:13,373 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1330] 2021-06-20 12:22:14,098 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1341] 2021-06-20 12:22:14,099 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets {'validation': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 202\n",
      "}), 'test': Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 202\n",
      "}), 'train': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 99784\n",
      "})}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f578bd3d0fab44da9c362b28d9d203f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ded1637654c440f8c64c32cf407c7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9faea061541548cf86f3736cb2d9f229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1156] 2021-06-20 12:23:26,638 >> ***** Running training *****\n",
      "[INFO|trainer.py:1157] 2021-06-20 12:23:26,658 >>   Num examples = 153013\n",
      "[INFO|trainer.py:1158] 2021-06-20 12:23:26,673 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1159] 2021-06-20 12:23:26,688 >>   Instantaneous batch size per device = 10\n",
      "[INFO|trainer.py:1160] 2021-06-20 12:23:26,689 >>   Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "[INFO|trainer.py:1161] 2021-06-20 12:23:26,691 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1162] 2021-06-20 12:23:26,692 >>   Total optimization steps = 15302\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1745' max='15302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1745/15302 21:53:45 < 170:18:20, 0.02 it/s, Epoch 0.11/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import run_qa\n",
    "%run run_qa.py \\\n",
    "  --model_name_or_path roberta-base \\\n",
    "  --dataset_name ../data/full_squad_covidQA/ \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_predict \\\n",
    "  --per_device_train_batch_size 10\\\n",
    "  --per_device_eval_batch_size 10\\\n",
    "  --evaluation_strategy \"no\" \\\n",
    "  --save_strategy \"no\" \\\n",
    "  --logging_strategy \"epoch\" \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --k_fold_cross_valid 5 \\\n",
    "  --output_dir ../models/gradual_ft_baseline \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #for making the squad dataset smaller we can do split up the full dataset like so\n",
    "# k_fold = 3\n",
    "# temp = squad_qa.train_test_split(test_size=0.5)\n",
    "# squad_qa = temp['train']\n",
    "#\n",
    "# make_and_save_full_dataset(covid_qa,squad_qa,covid_and_squad_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}