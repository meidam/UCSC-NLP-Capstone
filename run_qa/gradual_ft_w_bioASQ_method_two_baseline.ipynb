{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "squad_dataset = datasets.load_dataset('squad')\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "covid_file = '../data/COVID-QA.json'\n",
    "bio_file = '../bioASQ/bioASQ.json'\n",
    "\n",
    "def make_and_save_full_dataset(train, valid, test, path):\n",
    "    full_data = datasets.dataset_dict.DatasetDict({'train':train, 'validation':valid, 'test': test})\n",
    "    full_data.save_to_disk(path)\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return datasets.load_dataset('custom_squad.py', data_files= {'train':filename})['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import run_qa\n",
    "\n",
    "def run_gradual_ft(output_dir, checkpoint, covid_val):\n",
    "    !python run_qa.py \\\n",
    "      --model_name_or_path {checkpoint} \\\n",
    "      --dataset_name ../data/full_squad_covidQA/ \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --do_predict \\\n",
    "      --per_device_train_batch_size 32\\\n",
    "      --per_device_eval_batch_size 32\\\n",
    "      --evaluation_strategy \"no\" \\\n",
    "      --save_strategy \"no\" \\\n",
    "      --logging_strategy \"epoch\" \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 1 \\\n",
    "      --max_seq_length 384 \\\n",
    "      --doc_stride 128 \\\n",
    "      --output_dir {output_dir} \\\n",
    "      --overwrite_output_dir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a396986d6b3a2375\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-a396986d6b3a2375\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n",
      "Using custom data configuration default-8fdbe041288a2f4d\n",
      "Reusing dataset squad (C:\\Users\\unitu\\.cache\\huggingface\\datasets\\squad\\default-8fdbe041288a2f4d\\0.0.0\\cb00e306c4924563ce3d1292a1ce1b86b2753dab6285ce43c87b39c5bda3ef4e)\n"
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = covid_file\n",
    "\n",
    "covid_qa = get_dataset(covid_file)\n",
    "bio_qa = get_dataset(bio_file)\n",
    "\n",
    "squad_qa = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "covid_and_squad_dataset_path = \"../data/full_squad_covidQA\"\n",
    "\n",
    "squad_qa = datasets.Dataset.from_dict(squad_qa[:50])\n",
    "covid_qa = datasets.Dataset.from_dict(covid_qa[:20])\n",
    "bio_qa = datasets.Dataset.from_dict(bio_qa[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:52:17 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:52:17 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\runs\\Jun30_20-52-16_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 5.3034, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.8726, 'train_samples_per_second': 71.344, 'train_steps_per_second': 2.254, 'train_loss': 5.30341911315918, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3034\n",
      "  train_runtime            = 0:00:08.87\n",
      "  train_samples            =        633\n",
      "  train_samples_per_second =     71.344\n",
      "  train_steps_per_second   =      2.254\n",
      "06/30/2021 20:52:35 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:52:35 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:52:35 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 20:52:35 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:52:35 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:52:36 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/30/2021 20:52:36 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 20:52:36 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "squad:    test_f1          = 0.0\n",
      "50 n =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 20:52:17,865 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:52:17,866 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 20:52:18,190 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 20:52:18,516 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:52:18,516 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:52:20,848 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:52:20,848 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:52:20,848 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:52:20,848 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:52:20,849 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:52:20,849 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 20:52:21,244 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 20:52:22,183 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 20:52:22,183 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.65ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  3.65ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.85ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.85ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.62ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.62ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:52:25,478 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:52:25,478 >>   Num examples = 633\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:52:25,479 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:52:25,479 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:52:25,479 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:52:25,479 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:52:25,479 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:27,  1.44s/it]\n",
      " 10%|#         | 2/20 [00:01<00:20,  1.13s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.10it/s]\n",
      " 20%|##        | 4/20 [00:02<00:12,  1.32it/s]\n",
      " 25%|##5       | 5/20 [00:03<00:09,  1.55it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.75it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.93it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.08it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.20it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.29it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.36it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.41it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.44it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.47it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.48it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.50it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.51it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.51it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.52it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.68it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.68it/s][INFO|trainer.py:1349] 2021-06-30 20:52:34,351 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.68it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.25it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:52:34,352 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:52:34,353 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:52:35,027 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:52:35,028 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:52:35,028 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:52:35,152 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:52:35,154 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:52:35,154 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:52:35,154 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.93it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.02it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:52:35,796 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:52:35,798 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:52:35,798 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:52:35,798 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 15.75it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.35it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 12.35it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:52:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:52:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\runs\\Jun30_20-52-38_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 3.2323, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.0262, 'train_samples_per_second': 73.453, 'train_steps_per_second': 2.327, 'train_loss': 3.232264200846354, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2323\n",
      "  train_runtime            = 0:00:09.02\n",
      "  train_samples            =        663\n",
      "  train_samples_per_second =     73.453\n",
      "  train_steps_per_second   =      2.327\n",
      "06/30/2021 20:52:51 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:52:52 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:52:52 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 20:52:52 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:52:52 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:52:52 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/30/2021 20:52:53 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 20:52:53 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:52:38,888 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:52:38,888 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:52:38,896 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:38,896 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:38,897 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:38,897 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:38,897 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:38,897 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:38,897 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:52:38,975 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:52:39,685 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:52:39,685 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.26ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:52:42,190 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:52:42,190 >>   Num examples = 663\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:52:42,190 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:52:42,190 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:52:42,190 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:52:42,190 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:52:42,190 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:24,  1.25s/it]\n",
      " 10%|9         | 2/21 [00:01<00:18,  1.01it/s]\n",
      " 14%|#4        | 3/21 [00:02<00:14,  1.23it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:11,  1.46it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.67it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.86it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:06,  2.03it/s]\n",
      " 38%|###8      | 8/21 [00:03<00:06,  2.16it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.26it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.34it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.40it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.44it/s]\n",
      " 62%|######1   | 13/21 [00:05<00:03,  2.47it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.49it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.50it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:01,  2.51it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.52it/s]\n",
      " 86%|########5 | 18/21 [00:07<00:01,  2.53it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.53it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.53it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.73it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.73it/s][INFO|trainer.py:1349] 2021-06-30 20:52:51,216 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.73it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.33it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:52:51,218 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:52:51,219 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:52:51,781 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:52:51,781 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:52:51,782 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:52:51,882 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:52:51,884 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:52:51,884 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:52:51,884 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 15.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.16it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:52:52,515 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:52:52,517 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:52:52,517 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:52:52,517 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 15.87it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.35it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 12.35it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:52:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:52:55 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\runs\\Jun30_20-52-55_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "squad:  25 n =  3\n",
      "    })\n",
      "})\n",
      "{'loss': 1.6998, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.633, 'train_samples_per_second': 71.006, 'train_steps_per_second': 2.317, 'train_loss': 1.6998252868652344, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.6998\n",
      "  train_runtime            = 0:00:08.63\n",
      "  train_samples            =        613\n",
      "  train_samples_per_second =     71.006\n",
      "  train_steps_per_second   =      2.317\n",
      "06/30/2021 20:53:08 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:53:08 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:53:08 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 20:53:08 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:53:08 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:53:09 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/30/2021 20:53:09 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 20:53:09 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:52:55,562 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:52:55,562 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:52:55,564 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:55,564 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:55,564 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:55,564 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:55,564 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:55,564 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:52:55,564 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:52:55,648 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:52:56,342 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:52:56,343 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.55ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.55ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:52:58,857 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:52:58,857 >>   Num examples = 613\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:52:58,857 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:52:58,857 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:52:58,857 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:52:58,857 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:52:58,857 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:26,  1.42s/it]\n",
      " 10%|#         | 2/20 [00:01<00:20,  1.11s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:15,  1.11it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.34it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.56it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.77it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.94it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.09it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.20it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.29it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.36it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.40it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.44it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.47it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.49it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.50it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.51it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.51it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.52it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  3.24it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.24it/s][INFO|trainer.py:1349] 2021-06-30 20:53:07,490 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.24it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.32it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:53:07,491 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:53:07,492 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:53:08,051 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:53:08,051 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:53:08,052 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:53:08,152 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:53:08,154 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:53:08,154 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:53:08,154 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 15.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.19it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:53:08,787 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:53:08,789 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:53:08,789 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:53:08,789 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 15.75it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.35it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 12.35it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:53:11 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:53:11 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\runs\\Jun30_20-53-11_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.9685, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.461, 'train_samples_per_second': 72.45, 'train_steps_per_second': 2.364, 'train_loss': 0.9685427665710449, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.9685\n",
      "  train_runtime            = 0:00:08.46\n",
      "  train_samples            =        613\n",
      "  train_samples_per_second =      72.45\n",
      "  train_steps_per_second   =      2.364\n",
      "06/30/2021 20:53:24 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:53:24 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:53:24 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 20:53:24 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:53:24 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:53:25 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/30/2021 20:53:25 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 20:53:25 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "bio:  20 n=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:53:11,845 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:53:11,845 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:53:11,853 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:11,854 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:11,854 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:11,854 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:11,854 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:11,854 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:11,854 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:53:11,932 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:53:12,627 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:53:12,627 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.47ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.47ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:53:15,106 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:53:15,106 >>   Num examples = 613\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:53:15,106 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:53:15,106 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:53:15,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:53:15,106 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:53:15,106 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:23,  1.24s/it]\n",
      " 10%|#         | 2/20 [00:01<00:17,  1.02it/s]\n",
      " 15%|#5        | 3/20 [00:02<00:13,  1.24it/s]\n",
      " 20%|##        | 4/20 [00:02<00:10,  1.46it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:08,  1.67it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.86it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  2.02it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.15it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.25it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.33it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.38it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.43it/s]\n",
      " 65%|######5   | 13/20 [00:05<00:02,  2.46it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.48it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.49it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.50it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.51it/s]\n",
      " 90%|######### | 18/20 [00:07<00:00,  2.52it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.52it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  3.25it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.25it/s][INFO|trainer.py:1349] 2021-06-30 20:53:23,567 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.25it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.36it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:53:23,568 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:53:23,569 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:53:24,137 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:53:24,138 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:53:24,138 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:53:24,240 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:53:24,242 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:53:24,242 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:53:24,242 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.26it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.26it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.21it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:53:24,869 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:53:24,871 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:53:24,871 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:53:24,871 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 15.75it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.35it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 12.35it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:53:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:53:27 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\runs\\Jun30_20-53-27_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 5.2837, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.7071, 'train_samples_per_second': 71.321, 'train_steps_per_second': 2.297, 'train_loss': 5.2837474822998045, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2837\n",
      "  train_runtime            = 0:00:08.70\n",
      "  train_samples            =        621\n",
      "  train_samples_per_second =     71.321\n",
      "  train_steps_per_second   =      2.297\n",
      "06/30/2021 20:53:44 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:53:44 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:53:44 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 20:53:44 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:53:44 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:53:45 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:53:45 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 20:53:45 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 4.0\n",
      "squad:  50 n =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 20:53:28,301 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:53:28,302 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 20:53:28,621 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 20:53:28,956 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:53:28,956 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:53:30,913 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:53:30,913 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:53:30,913 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:53:30,913 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:53:30,913 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:53:30,913 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 20:53:31,303 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 20:53:31,987 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 20:53:31,987 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.24ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.24ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.93ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.93ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:53:34,813 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:53:34,814 >>   Num examples = 621\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:53:34,814 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:53:34,814 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:53:34,814 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:53:34,814 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:53:34,814 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.28s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.01s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.21it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.43it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.64it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.84it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.99it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.12it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.20it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.26it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.31it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.37it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.41it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.43it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.44it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.46it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.95it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.95it/s][INFO|trainer.py:1349] 2021-06-30 20:53:43,522 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.95it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.30it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:53:43,524 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:53:43,524 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:53:44,084 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:53:44,085 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:53:44,085 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:53:44,189 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:53:44,191 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:53:44,191 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:53:44,191 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 15.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.15it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.15it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.07it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:53:44,831 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:53:44,833 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:53:44,833 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:53:44,833 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.93it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.31it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.31it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:53:47 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:53:47 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\runs\\Jun30_20-53-47_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 3.2198, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.9916, 'train_samples_per_second': 72.401, 'train_steps_per_second': 2.336, 'train_loss': 3.2197901407877603, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2198\n",
      "  train_runtime            = 0:00:08.99\n",
      "  train_samples            =        651\n",
      "  train_samples_per_second =     72.401\n",
      "  train_steps_per_second   =      2.336\n",
      "06/30/2021 20:54:01 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:54:01 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:54:01 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 20:54:01 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:54:01 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:54:02 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:54:02 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 20:54:02 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:53:48,027 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:53:48,027 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:53:48,029 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:48,029 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:48,029 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:48,029 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:48,029 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:48,029 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:53:48,029 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:53:48,106 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:53:48,808 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:53:48,808 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.20ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.20ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.93ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.93ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:53:51,348 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:53:51,348 >>   Num examples = 651\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:53:51,348 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:53:51,348 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:53:51,348 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:53:51,348 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:53:51,348 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:25,  1.27s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.01s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:14,  1.21it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:11,  1.44it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.65it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.84it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:06,  2.00it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.13it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.24it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.31it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.37it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.41it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.44it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.47it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.48it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.49it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.50it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.51it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.51it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.51it/s]\n",
      "100%|##########| 21/21 [00:08<00:00,  3.02it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:08<00:00,  3.02it/s][INFO|trainer.py:1349] 2021-06-30 20:54:00,340 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:08<00:00,  3.02it/s]\n",
      "100%|##########| 21/21 [00:08<00:00,  2.34it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:54:00,341 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:54:00,342 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:54:00,912 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:54:00,913 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:54:00,913 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:54:01,012 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:54:01,014 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:54:01,014 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:54:01,014 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.12it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:54:01,649 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:54:01,650 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:54:01,650 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:54:01,650 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.93it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:54:04 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:54:04 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\runs\\Jun30_20-54-04_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "save_steps=500,\n",
      "squad: save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42, \n",
      "sharded_ddp=[],\n",
      "25 skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,n =  3\n",
      "\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.8247, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.3486, 'train_samples_per_second': 71.988, 'train_steps_per_second': 2.276, 'train_loss': 1.8246757105777138, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8247\n",
      "  train_runtime            = 0:00:08.34\n",
      "  train_samples            =        601\n",
      "  train_samples_per_second =     71.988\n",
      "  train_steps_per_second   =      2.276\n",
      "06/30/2021 20:54:17 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:54:17 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:54:17 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 20:54:17 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:54:17 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:54:18 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:54:18 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 20:54:18 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:54:04,826 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:54:04,826 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:54:04,834 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:04,834 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:04,834 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:04,834 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:04,834 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:04,834 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:04,834 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:54:04,910 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:54:05,610 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:54:05,610 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.44ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.44ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:54:08,157 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:54:08,157 >>   Num examples = 601\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:54:08,157 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:54:08,157 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:54:08,157 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:54:08,157 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:54:08,157 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:22,  1.27s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.01s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.21it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.43it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.65it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.84it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  2.00it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.13it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.23it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.31it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.37it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.41it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.44it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.46it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.48it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.49it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.50it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.50it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s][INFO|trainer.py:1349] 2021-06-30 20:54:16,506 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.28it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:54:16,507 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:54:16,508 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:54:17,078 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:54:17,079 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:54:17,079 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:54:17,171 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:54:17,173 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:54:17,173 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:54:17,173 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.93it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.15it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.15it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.19it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:54:17,804 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:54:17,805 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:54:17,805 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:54:17,805 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.26it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.26it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:54:20 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:54:20 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\runs\\Jun30_20-54-20_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.0772, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.3636, 'train_samples_per_second': 71.859, 'train_steps_per_second': 2.272, 'train_loss': 1.0772309554250616, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.0772\n",
      "  train_runtime            = 0:00:08.36\n",
      "  train_samples            =        601\n",
      "  train_samples_per_second =     71.859\n",
      "  train_steps_per_second   =      2.272\n",
      "06/30/2021 20:54:33 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:54:33 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:54:33 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 20:54:33 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:54:33 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:54:34 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:54:34 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 20:54:34 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "bio:  20 n=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:54:20,983 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:54:20,983 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:54:20,991 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:20,992 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:20,992 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:20,992 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:20,992 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:20,992 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:20,992 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:54:21,068 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:54:21,772 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:54:21,772 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.50ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.48ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:54:24,311 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:54:24,311 >>   Num examples = 601\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:54:24,311 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:54:24,311 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:54:24,311 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:54:24,311 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:54:24,311 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:22,  1.27s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.01s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.21it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.43it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.64it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.83it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.99it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.13it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.23it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.31it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.36it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.41it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.44it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.46it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.47it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.48it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.49it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.50it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s][INFO|trainer.py:1349] 2021-06-30 20:54:32,675 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.27it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:54:32,676 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:54:32,677 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:54:33,253 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:54:33,253 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:54:33,254 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:54:33,358 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:54:33,360 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:54:33,360 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:54:33,360 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.16it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:54:33,992 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:54:33,993 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:54:33,993 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:54:33,993 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.26it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.26it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:54:36 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:54:36 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\runs\\Jun30_20-54-36_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })squad: \n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    }) 50 \n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "n =  1        num_rows: 2\n",
      "    })\n",
      "\n",
      "})\n",
      "{'loss': 5.3238, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.6153, 'train_samples_per_second': 71.849, 'train_steps_per_second': 2.321, 'train_loss': 5.323825073242188, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3238\n",
      "  train_runtime            = 0:00:08.61\n",
      "  train_samples            =        619\n",
      "  train_samples_per_second =     71.849\n",
      "  train_steps_per_second   =      2.321\n",
      "06/30/2021 20:54:53 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:54:53 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:54:53 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 20:54:53 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/30/2021 20:54:54 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:54:54 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:54:54 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 20:54:54 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 20:54:37,568 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:54:37,568 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 20:54:37,899 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 20:54:38,225 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:54:38,225 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:54:40,187 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:54:40,187 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:54:40,187 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:54:40,187 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:54:40,187 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:54:40,187 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 20:54:40,575 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 20:54:41,262 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 20:54:41,262 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.31ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.31ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.43ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.35ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:54:44,094 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:54:44,094 >>   Num examples = 619\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:54:44,094 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:54:44,094 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:54:44,094 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:54:44,094 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:54:44,094 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.27s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.01s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.21it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.43it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.64it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.83it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  2.00it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.13it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.23it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.31it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.37it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.41it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.44it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.46it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.48it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.48it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.49it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.50it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.50it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  3.02it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.02it/s][INFO|trainer.py:1349] 2021-06-30 20:54:52,709 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.02it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.32it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:54:52,710 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:54:52,711 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:54:53,275 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:54:53,276 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:54:53,276 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:54:53,363 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:54:53,365 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:54:53,365 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:54:53,365 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.20it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.20it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.12it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:54:54,001 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:54:54,003 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:54:54,003 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:54:54,003 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:54:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:54:56 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\runs\\Jun30_20-54-56_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 3.2306, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.0068, 'train_samples_per_second': 72.057, 'train_steps_per_second': 2.332, 'train_loss': 3.2305741083054316, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2306\n",
      "  train_runtime            = 0:00:09.00\n",
      "  train_samples            =        649\n",
      "  train_samples_per_second =     72.057\n",
      "  train_steps_per_second   =      2.332\n",
      "06/30/2021 20:55:10 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:55:10 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:55:10 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 20:55:10 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/30/2021 20:55:10 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:55:11 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:55:11 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 20:55:11 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:54:57,214 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:54:57,215 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:54:57,216 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:57,217 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:57,217 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:57,217 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:57,217 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:57,217 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:54:57,217 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:54:57,290 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:54:57,990 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:54:57,991 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.22ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.20ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:55:00,498 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:55:00,498 >>   Num examples = 649\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:55:00,498 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:55:00,498 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:55:00,498 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:55:00,498 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:55:00,498 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:25,  1.28s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.02s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.20it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:11,  1.42it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.64it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.83it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.99it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.12it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.23it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.30it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.36it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.41it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.44it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.46it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.47it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.48it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.49it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.49it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.49it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.50it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  3.09it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  3.09it/s][INFO|trainer.py:1349] 2021-06-30 20:55:09,505 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  3.09it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.33it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:55:09,507 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:55:09,508 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:55:10,076 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:55:10,076 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:55:10,077 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:55:10,166 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:55:10,168 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:55:10,168 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:55:10,168 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.96it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:55:10,817 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:55:10,819 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:55:10,819 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:55:10,819 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:55:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:55:13 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\runs\\Jun30_20-55-13_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "squad:  25 n =          num_rows: 2\n",
      "    })3\n",
      "\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.7699, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.3808, 'train_samples_per_second': 71.473, 'train_steps_per_second': 2.267, 'train_loss': 1.769936009457237, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.7699\n",
      "  train_runtime            = 0:00:08.38\n",
      "  train_samples            =        599\n",
      "  train_samples_per_second =     71.473\n",
      "  train_steps_per_second   =      2.267\n",
      "06/30/2021 20:55:26 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:55:26 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:55:27 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 20:55:27 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/30/2021 20:55:27 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:55:27 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:55:27 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 20:55:27 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:55:14,061 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:55:14,061 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:55:14,069 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:14,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:14,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:14,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:14,070 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:14,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:14,070 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:55:14,141 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:55:14,855 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:55:14,855 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.42ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.41ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.52ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.43ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.36ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.36ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:55:17,364 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:55:17,364 >>   Num examples = 599\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:55:17,364 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:55:17,364 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:55:17,364 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:55:17,364 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:55:17,364 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:22,  1.27s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.01s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.21it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.44it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.65it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.83it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.99it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.12it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.22it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.30it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.36it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.40it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.43it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.45it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.47it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.47it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.48it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s][INFO|trainer.py:1349] 2021-06-30 20:55:25,746 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.27it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:55:25,747 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:55:25,748 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:55:26,325 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:55:26,325 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:55:26,326 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:55:26,422 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:55:26,424 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:55:26,424 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:55:26,424 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.20it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:55:27,060 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:55:27,062 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:55:27,062 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:55:27,062 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:55:29 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:55:29 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\runs\\Jun30_20-55-29_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.9736, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.3781, 'train_samples_per_second': 71.496, 'train_steps_per_second': 2.268, 'train_loss': 0.9736481716758326, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.9736\n",
      "  train_runtime            = 0:00:08.37\n",
      "  train_samples            =        599\n",
      "  train_samples_per_second =     71.496\n",
      "  train_steps_per_second   =      2.268\n",
      "06/30/2021 20:55:42 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:55:43 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:55:43 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 20:55:43 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/30/2021 20:55:43 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:55:44 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:55:44 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 20:55:44 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "bio:  20 n=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:55:30,369 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:55:30,370 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:55:30,378 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:30,378 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:30,378 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:30,379 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:30,379 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:30,379 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:55:30,379 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:55:30,446 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:55:31,148 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:55:31,148 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.50ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.50ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:55:33,920 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:55:33,920 >>   Num examples = 599\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:55:33,920 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:55:33,920 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:55:33,920 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:55:33,920 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:55:33,920 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:22,  1.27s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.01s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.21it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.43it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.64it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.83it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.99it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.12it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.22it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.30it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.35it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.39it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.42it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.45it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.46it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.47it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.48it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.49it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.69it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.69it/s][INFO|trainer.py:1349] 2021-06-30 20:55:42,298 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.69it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.27it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:55:42,300 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:55:42,300 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:55:42,867 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:55:42,868 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:55:42,868 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:55:42,949 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:55:42,951 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:55:42,951 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:55:42,951 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.15it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.15it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.21it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:55:43,581 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:55:43,582 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:55:43,582 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:55:43,582 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:55:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:55:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\runs\\Jun30_20-55-46_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 5.297, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.6247, 'train_samples_per_second': 71.422, 'train_steps_per_second': 2.319, 'train_loss': 5.296953582763672, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      5.297\n",
      "  train_runtime            = 0:00:08.62\n",
      "  train_samples            =        616\n",
      "  train_samples_per_second =     71.422\n",
      "  train_steps_per_second   =      2.319\n",
      "06/30/2021 20:56:02 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:56:03 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:56:03 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 20:56:03 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/30/2021 20:56:03 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:56:03 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:56:04 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 20:56:04 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "squad:  50 n =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 20:55:47,162 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:55:47,162 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 20:55:47,486 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 20:55:47,822 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:55:47,822 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:55:49,822 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:55:49,822 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:55:49,822 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:55:49,822 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:55:49,822 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:55:49,822 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 20:55:50,207 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 20:55:50,894 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 20:55:50,894 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.46ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.01ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.01ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.41ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:55:53,534 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:55:53,534 >>   Num examples = 616\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:55:53,534 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:55:53,534 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:55:53,534 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:55:53,534 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:55:53,534 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.28s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.02s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.20it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.43it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.64it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.83it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.99it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.12it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.22it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.30it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.35it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.40it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.43it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.44it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.45it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.47it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  3.11it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.11it/s][INFO|trainer.py:1349] 2021-06-30 20:56:02,158 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.11it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.32it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:56:02,160 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:56:02,161 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:56:02,726 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:56:02,726 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:56:02,726 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:56:02,805 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:56:02,807 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:56:02,807 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:56:02,807 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.82it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.85it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:56:03,465 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:56:03,467 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:56:03,467 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:56:03,467 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:56:06 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:56:06 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\runs\\Jun30_20-56-06_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 3.302, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.9823, 'train_samples_per_second': 71.92, 'train_steps_per_second': 2.338, 'train_loss': 3.3020241146995906, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      3.302\n",
      "  train_runtime            = 0:00:08.98\n",
      "  train_samples            =        646\n",
      "  train_samples_per_second =      71.92\n",
      "  train_steps_per_second   =      2.338\n",
      "06/30/2021 20:56:19 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:56:20 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:56:20 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 20:56:20 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/30/2021 20:56:20 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:56:20 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:56:20 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 20:56:20 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:56:06,658 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:56:06,658 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:56:06,660 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:06,660 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:06,660 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:06,660 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:06,660 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:06,660 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:06,660 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:56:06,727 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:56:07,429 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:56:07,429 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.44ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.44ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.35ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.58ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.58ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:56:09,942 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:56:09,942 >>   Num examples = 646\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:56:09,942 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:56:09,942 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:56:09,942 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:56:09,942 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:56:09,942 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:25,  1.27s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.01s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:14,  1.21it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:11,  1.43it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.64it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.83it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.99it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.12it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.22it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.30it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.36it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.40it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.43it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.45it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.47it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.47it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.48it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.49it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.49it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.50it/s]\n",
      "100%|##########| 21/21 [00:08<00:00,  3.18it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:08<00:00,  3.18it/s][INFO|trainer.py:1349] 2021-06-30 20:56:18,924 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:08<00:00,  3.18it/s]\n",
      "100%|##########| 21/21 [00:08<00:00,  2.34it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:56:18,925 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:56:18,926 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:56:19,491 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:56:19,492 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:56:19,492 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:56:19,572 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:56:19,573 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:56:19,574 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:56:19,574 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.96it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:56:20,223 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:56:20,225 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:56:20,225 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:56:20,225 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:56:22 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:56:22 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\runs\\Jun30_20-56-22_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2squad: \n",
      "    })\n",
      " 25 n = })\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included 3\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.8646, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.3526, 'train_samples_per_second': 71.355, 'train_steps_per_second': 2.275, 'train_loss': 1.8645904942562705, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8646\n",
      "  train_runtime            = 0:00:08.35\n",
      "  train_samples            =        596\n",
      "  train_samples_per_second =     71.355\n",
      "  train_steps_per_second   =      2.275\n",
      "06/30/2021 20:56:35 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:56:36 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:56:36 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 20:56:36 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/30/2021 20:56:36 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:56:36 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:56:36 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 20:56:36 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:56:23,433 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:56:23,433 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:56:23,435 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:23,435 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:23,435 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:23,435 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:23,436 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:23,436 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:23,436 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:56:23,504 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:56:24,206 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:56:24,206 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.44ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.42ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:56:26,686 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:56:26,686 >>   Num examples = 596\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:56:26,686 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:56:26,686 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:56:26,686 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:56:26,686 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:56:26,686 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:22,  1.27s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.01s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.21it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.43it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.64it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.83it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.99it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.12it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.22it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.29it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.35it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.39it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.42it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.45it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.46it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.47it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.48it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.49it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.76it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.76it/s][INFO|trainer.py:1349] 2021-06-30 20:56:35,039 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.76it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.28it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:56:35,040 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:56:35,041 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:56:35,609 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:56:35,610 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:56:35,610 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:56:35,691 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:56:35,693 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:56:35,693 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:56:35,693 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.98it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:56:36,342 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:56:36,344 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:56:36,344 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:56:36,344 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:56:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:56:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\runs\\Jun30_20-56-39_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.1159, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.3561, 'train_samples_per_second': 71.325, 'train_steps_per_second': 2.274, 'train_loss': 1.1158967269094366, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.1159\n",
      "  train_runtime            = 0:00:08.35\n",
      "  train_samples            =        596\n",
      "  train_samples_per_second =     71.325\n",
      "  train_steps_per_second   =      2.274\n",
      "06/30/2021 20:56:51 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:56:52 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:56:52 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 20:56:52 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/30/2021 20:56:52 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:56:52 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 20:56:53 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 20:56:53 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "bio:  20 n=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:56:39,529 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:56:39,529 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:56:39,537 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:39,537 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:39,537 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:39,537 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:39,537 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:39,537 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:56:39,537 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:56:39,604 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:56:40,302 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:56:40,302 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.52ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.50ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.77ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  8.77ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:56:42,782 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:56:42,782 >>   Num examples = 596\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:56:42,782 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:56:42,782 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:56:42,782 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:56:42,782 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:56:42,782 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.28s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.02s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.20it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.42it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.63it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.82it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.98it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.12it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.22it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.29it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.35it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.40it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.42it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.45it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.46it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.47it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.48it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.49it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.76it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.76it/s][INFO|trainer.py:1349] 2021-06-30 20:56:51,138 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.76it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.27it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:56:51,140 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:56:51,141 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:56:51,707 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:56:51,707 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:56:51,708 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:56:51,788 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:56:51,789 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:56:51,789 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:56:51,789 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.94it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:56:52,440 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:56:52,442 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:56:52,442 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:56:52,442 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:56:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:56:55 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\runs\\Jun30_20-56-55_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "squad: 88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111 50 n =  1\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 5.3216, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.6492, 'train_samples_per_second': 71.567, 'train_steps_per_second': 2.312, 'train_loss': 5.321617889404297, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3216\n",
      "  train_runtime            = 0:00:08.64\n",
      "  train_samples            =        619\n",
      "  train_samples_per_second =     71.567\n",
      "  train_steps_per_second   =      2.312\n",
      "06/30/2021 20:57:11 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:57:12 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/30/2021 20:57:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 20:57:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/30/2021 20:57:12 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:57:12 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:57:13 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 20:57:13 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 20:56:56,029 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:56:56,029 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 20:56:56,349 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 20:56:56,687 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:56:56,687 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:56:58,687 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:56:58,687 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:56:58,687 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:56:58,687 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:56:58,687 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:56:58,688 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 20:56:59,066 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 20:56:59,753 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 20:56:59,754 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.44ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.44ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.52ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:57:02,583 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:57:02,583 >>   Num examples = 619\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:57:02,583 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:57:02,583 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:57:02,583 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:57:02,583 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:57:02,583 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.27s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.01s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.21it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.43it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.64it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.83it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.99it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.12it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.22it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.30it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.36it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.40it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.43it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.45it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.46it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.47it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.48it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.48it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.49it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  3.00it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.00it/s][INFO|trainer.py:1349] 2021-06-30 20:57:11,232 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.00it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.31it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:57:11,234 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:57:11,234 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:57:11,794 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:57:11,795 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:57:11,795 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:57:11,875 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:57:11,877 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:57:11,877 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:57:11,877 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.96it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:57:12,526 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:57:12,528 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:57:12,528 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:57:12,528 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.26it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.26it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:57:15 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:57:15 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\runs\\Jun30_20-57-15_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 3.2231, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.0415, 'train_samples_per_second': 71.78, 'train_steps_per_second': 2.323, 'train_loss': 3.2230518886021207, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2231\n",
      "  train_runtime            = 0:00:09.04\n",
      "  train_samples            =        649\n",
      "  train_samples_per_second =      71.78\n",
      "  train_steps_per_second   =      2.323\n",
      "06/30/2021 20:57:28 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:57:29 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/30/2021 20:57:29 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 20:57:29 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/30/2021 20:57:29 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:57:29 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:57:29 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 20:57:29 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:57:15,738 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:57:15,738 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:57:15,740 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:15,740 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:15,740 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:15,740 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:15,740 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:15,740 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:15,740 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:57:15,808 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:57:16,514 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:57:16,514 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.39ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.39ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.43ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.43ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:57:18,986 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:57:18,986 >>   Num examples = 649\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:57:18,986 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:57:18,986 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:57:18,986 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:57:18,986 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:57:18,986 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:25,  1.28s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.02s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.20it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:11,  1.42it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.63it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.82it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.98it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.11it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.22it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.29it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.35it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.39it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.42it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.45it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.46it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.47it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.48it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.49it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.49it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.49it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  3.08it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  3.08it/s][INFO|trainer.py:1349] 2021-06-30 20:57:28,027 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  3.08it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.32it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:57:28,029 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:57:28,030 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:57:28,596 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:57:28,597 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:57:28,597 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:57:28,676 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:57:28,679 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:57:28,679 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:57:28,679 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.07it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:57:29,322 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:57:29,324 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:57:29,324 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:57:29,324 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:57:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:57:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\runs\\Jun30_20-57-32_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "squad: })\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      " 88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "1111111111111111111111111111111111111111111111111111111111111111111125\n",
      " n = ***************************\n",
      " Split  1\n",
      "***************************3\n",
      "\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 1.7774, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.4158, 'train_samples_per_second': 71.176, 'train_steps_per_second': 2.258, 'train_loss': 1.777440121299342, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.7774\n",
      "  train_runtime            = 0:00:08.41\n",
      "  train_samples            =        599\n",
      "  train_samples_per_second =     71.176\n",
      "  train_steps_per_second   =      2.258\n",
      "06/30/2021 20:57:45 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:57:45 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/30/2021 20:57:45 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 20:57:45 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/30/2021 20:57:45 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:57:46 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:57:46 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 20:57:46 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:57:32,501 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:57:32,501 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:57:32,509 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:32,510 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:32,510 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:32,510 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:32,510 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:32,510 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:32,510 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:57:32,576 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:57:33,273 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:57:33,273 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.48ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.48ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.26ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.17ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.46ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:57:36,048 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:57:36,048 >>   Num examples = 599\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:57:36,048 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:57:36,048 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:57:36,048 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:57:36,048 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:57:36,048 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.28s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.02s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.20it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.42it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.63it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.82it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.98it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.11it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.22it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.29it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.35it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.39it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.43it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.45it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.46it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.47it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.48it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.65it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.65it/s][INFO|trainer.py:1349] 2021-06-30 20:57:44,463 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.65it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.26it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:57:44,465 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:57:44,466 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:57:45,072 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:57:45,073 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:57:45,073 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:57:45,162 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:57:45,165 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:57:45,165 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:57:45,165 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.94it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:57:45,822 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:57:45,824 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:57:45,824 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:57:45,824 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:57:48 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:57:48 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\runs\\Jun30_20-57-48_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:57:49,481 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:57:49,481 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:57:49,489 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:49,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:49,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:49,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:49,490 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:49,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:57:49,490 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:57:49,566 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:57:50,283 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:57:50,283 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.33ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  4.33ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  9.09ba/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.35ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  7.35ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:57:52,789 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:57:52,789 >>   Num examples = 599\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:57:52,789 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:57:52,790 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:57:52,790 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:57:52,790 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:57:52,790 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.30s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.03s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.19it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.41it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.62it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.81it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.97it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.10it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.20it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.28it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.34it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.38it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.45it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.46it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.46it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s][INFO|trainer.py:1349] 2021-06-30 20:58:01,242 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "{'loss': 0.9482, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.452, 'train_samples_per_second': 70.871, 'train_steps_per_second': 2.248, 'train_loss': 0.9481653916208368, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.9482\n",
      "  train_runtime            = 0:00:08.45\n",
      "  train_samples            =        599\n",
      "  train_samples_per_second =     70.871\n",
      "  train_steps_per_second   =      2.248\n",
      "06/30/2021 20:58:01 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:58:02 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/30/2021 20:58:02 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 20:58:02 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/30/2021 20:58:02 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:58:03 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:58:03 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 20:58:03 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.25it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:58:01,243 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:58:01,244 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:58:01,836 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:58:01,837 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:58:01,837 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:58:01,930 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:58:01,932 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:58:01,932 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:58:01,932 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.52it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.48it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.88it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:58:02,592 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:58:02,593 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:58:02,593 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:58:02,593 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.52it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.52it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bio:  20 n=  0\n",
      "06/30/2021 20:58:05 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:58:05 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\runs\\Jun30_20-58-05_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 20:58:10 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-b3b7a8d49b46faf2.arrow\n",
      "06/30/2021 20:58:10 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-77c112c6ad537089.arrow\n",
      "{'loss': 5.3034, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.848, 'train_samples_per_second': 71.542, 'train_steps_per_second': 2.26, 'train_loss': 5.30341911315918, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3034\n",
      "  train_runtime            = 0:00:08.84\n",
      "  train_samples            =        633\n",
      "  train_samples_per_second =     71.542\n",
      "  train_steps_per_second   =       2.26\n",
      "06/30/2021 20:58:22 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:58:22 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:58:22 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 20:58:22 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:58:22 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:58:23 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/30/2021 20:58:23 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 20:58:23 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "squad:  50 n =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 20:58:06,379 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:58:06,380 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 20:58:06,716 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 20:58:07,036 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:58:07,037 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:58:09,054 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:58:09,054 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:58:09,054 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:58:09,054 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:58:09,054 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:58:09,054 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 20:58:09,441 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 20:58:10,138 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 20:58:10,138 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.67ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.67ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:58:12,527 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:58:12,527 >>   Num examples = 633\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:58:12,527 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:58:12,527 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:58:12,527 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:58:12,527 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:58:12,527 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.29s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.02s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.20it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.42it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.63it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.82it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.97it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.10it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.21it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.28it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.34it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.38it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.41it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.44it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.45it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.47it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.64it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.64it/s][INFO|trainer.py:1349] 2021-06-30 20:58:21,375 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.64it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.26it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:58:21,376 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:58:21,377 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:58:21,987 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:58:21,987 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:58:21,988 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:58:22,078 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:58:22,080 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:58:22,080 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:58:22,080 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.62it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.62it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.98it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:58:22,730 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:58:22,732 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:58:22,732 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:58:22,732 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 15.63it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.76it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:58:25 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:58:25 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\runs\\Jun30_20-58-25_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 20:58:26 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-2b88328e78116ed3.arrow\n",
      "06/30/2021 20:58:26 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-27ea423c36dafc21.arrow\n",
      "06/30/2021 20:58:26 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-8639512de9a05da5.arrow\n",
      "{'loss': 3.2323, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.2587, 'train_samples_per_second': 71.608, 'train_steps_per_second': 2.268, 'train_loss': 3.232264200846354, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2323\n",
      "  train_runtime            = 0:00:09.25\n",
      "  train_samples            =        663\n",
      "  train_samples_per_second =     71.608\n",
      "  train_steps_per_second   =      2.268\n",
      "06/30/2021 20:58:38 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:58:39 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:58:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 20:58:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:58:39 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:58:39 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/30/2021 20:58:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 20:58:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:58:25,892 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:58:25,892 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:58:25,894 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:25,894 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:25,894 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:25,894 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:25,894 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:25,894 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:25,894 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:58:25,968 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:58:26,692 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:58:26,692 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:58:28,672 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:58:28,672 >>   Num examples = 663\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:58:28,672 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:58:28,672 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:58:28,672 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:58:28,672 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:58:28,672 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:25,  1.29s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.02s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.20it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:12,  1.42it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.63it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.81it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.97it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.10it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.21it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.28it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.34it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.38it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.41it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.44it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.44it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.45it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.45it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.45it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.46it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.46it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.66it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.66it/s][INFO|trainer.py:1349] 2021-06-30 20:58:37,931 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.66it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.27it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:58:37,933 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:58:37,933 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:58:38,607 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:58:38,607 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:58:38,608 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:58:38,706 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:58:38,708 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:58:38,708 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:58:38,708 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.02it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:58:39,354 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:58:39,355 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:58:39,356 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:58:39,356 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 15.50it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 11.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 11.90it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:58:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:58:42 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\runs\\Jun30_20-58-42_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "squad: max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=, 25 n = \n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      " 3\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 20:58:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-465827352040f2ee.arrow\n",
      "06/30/2021 20:58:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-2b2419cfc1f07237.arrow\n",
      "06/30/2021 20:58:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-69dc1a41a42eb1c1.arrow\n",
      "{'loss': 1.6998, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.6501, 'train_samples_per_second': 70.867, 'train_steps_per_second': 2.312, 'train_loss': 1.6998252868652344, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.6998\n",
      "  train_runtime            = 0:00:08.65\n",
      "  train_samples            =        613\n",
      "  train_samples_per_second =     70.867\n",
      "  train_steps_per_second   =      2.312\n",
      "06/30/2021 20:58:54 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:58:55 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:58:55 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 20:58:55 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:58:55 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:58:55 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/30/2021 20:58:55 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 20:58:55 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:58:42,566 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:58:42,566 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:58:42,568 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:42,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:42,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:42,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:42,569 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:42,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:42,569 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:58:42,640 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:58:43,358 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:58:43,358 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:58:45,392 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:58:45,392 >>   Num examples = 613\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:58:45,392 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:58:45,392 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:58:45,392 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:58:45,392 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:58:45,392 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.31s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.04s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.18it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.41it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.61it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.80it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.97it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.10it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.20it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.28it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.34it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.38it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.41it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.43it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.45it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.47it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  3.19it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.19it/s][INFO|trainer.py:1349] 2021-06-30 20:58:54,042 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.19it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.31it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:58:54,043 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:58:54,044 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:58:54,690 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:58:54,690 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:58:54,691 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:58:54,780 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:58:54,782 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:58:54,782 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:58:54,782 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.07it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:58:55,424 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:58:55,425 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:58:55,425 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:58:55,425 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 15.50it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 12.05it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:58:58 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:58:58 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\runs\\Jun30_20-58-58_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 20:58:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-decb865b27cb1a0f.arrow\n",
      "06/30/2021 20:58:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-f1201cd02b28fe45.arrow\n",
      "06/30/2021 20:58:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-d259ca3ae54bb9c5.arrow\n",
      "{'loss': 0.9685, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.6523, 'train_samples_per_second': 70.848, 'train_steps_per_second': 2.312, 'train_loss': 0.9685427665710449, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.9685\n",
      "  train_runtime            = 0:00:08.65\n",
      "  train_samples            =        613\n",
      "  train_samples_per_second =     70.848\n",
      "  train_steps_per_second   =      2.312\n",
      "06/30/2021 20:59:10 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:59:11 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:59:11 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 20:59:11 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:59:11 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:59:11 - INFO - utils_qa -   Post-processing 2 example predictions split into 63 features.\n",
      "06/30/2021 20:59:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 20:59:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  63\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "bio:  20 n=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:58:58,586 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:58:58,586 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:58:58,588 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:58,588 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:58,588 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:58,588 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:58,588 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:58,588 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:58:58,588 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:58:58,658 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:58:59,387 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:58:59,387 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:59:01,550 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:59:01,550 >>   Num examples = 613\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:59:01,550 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:59:01,550 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:59:01,550 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:59:01,550 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:59:01,550 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.30s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.03s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.18it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.40it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.61it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.80it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.97it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.10it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.20it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.28it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.34it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.38it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.41it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.43it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.45it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.47it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  3.19it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.19it/s][INFO|trainer.py:1349] 2021-06-30 20:59:10,203 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.19it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.31it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:59:10,204 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:59:10,205 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:59:10,825 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:59:10,826 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:59:10,826 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_0/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:59:10,914 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:59:10,916 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:59:10,916 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:59:10,916 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.04it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:59:11,560 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:59:11,562 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:59:11,562 >>   Num examples = 63\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:59:11,562 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|##########| 2/2 [00:00<00:00, 15.50it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 12.12it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 12.12it/s]\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:59:14 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:59:14 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\runs\\Jun30_20-59-14_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 20:59:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-ae018a47116dc3ae.arrow\n",
      "06/30/2021 20:59:19 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-01e80e3a8a34f17a.arrow\n",
      "{'loss': 5.2837, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.778, 'train_samples_per_second': 70.745, 'train_steps_per_second': 2.278, 'train_loss': 5.2837474822998045, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.2837\n",
      "  train_runtime            = 0:00:08.77\n",
      "  train_samples            =        621\n",
      "  train_samples_per_second =     70.745\n",
      "  train_steps_per_second   =      2.278\n",
      "06/30/2021 20:59:30 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:59:31 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:59:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\eval_predictions.json.squad:  50 \n",
      "06/30/2021 20:59:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\eval_nbest_predictions.json.\n",
      "n = ***** eval metrics *****\n",
      "   epoch            = 1.01\n",
      "\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:59:31 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:59:31 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:59:32 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 20:59:32 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 20:59:15,115 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:59:15,115 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 20:59:15,445 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 20:59:15,785 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:59:15,785 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:59:17,736 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:59:17,736 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:59:17,736 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:59:17,736 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:59:17,736 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 20:59:17,736 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 20:59:18,123 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 20:59:18,842 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 20:59:18,842 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.76ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.76ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:59:21,250 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:59:21,250 >>   Num examples = 621\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:59:21,250 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:59:21,250 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:59:21,250 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:59:21,250 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:59:21,250 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.34s/it]\n",
      " 10%|#         | 2/20 [00:01<00:19,  1.06s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.16it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.38it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.59it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.78it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.95it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.08it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.19it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.27it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.33it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.38it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.41it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.43it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.45it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.47it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.94it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.94it/s][INFO|trainer.py:1349] 2021-06-30 20:59:30,028 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.94it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.28it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:59:30,029 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:59:30,030 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:59:30,668 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:59:30,668 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:59:30,669 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:59:30,764 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:59:30,765 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:59:30,765 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:59:30,765 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.05it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:59:31,408 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:59:31,410 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:59:31,410 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:59:31,410 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:59:34 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:59:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\runs\\Jun30_20-59-34_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 20:59:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-2ed10d36b1c2fba4.arrow\n",
      "06/30/2021 20:59:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-802baaf5fce410a8.arrow\n",
      "06/30/2021 20:59:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-4ff30e05eb1ebaa3.arrow\n",
      "{'loss': 3.2198, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.1297, 'train_samples_per_second': 71.306, 'train_steps_per_second': 2.3, 'train_loss': 3.2197901407877603, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2198\n",
      "  train_runtime            = 0:00:09.12\n",
      "  train_samples            =        651\n",
      "  train_samples_per_second =     71.306\n",
      "  train_steps_per_second   =        2.3\n",
      "06/30/2021 20:59:47 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 20:59:47 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 20:59:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 20:59:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 20:59:48 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 20:59:48 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 20:59:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 20:59:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:59:34,668 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:59:34,668 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:59:34,670 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:34,670 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:34,671 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:34,671 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:34,671 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:34,671 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:34,671 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:59:34,741 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:59:35,466 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:59:35,466 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:59:37,564 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:59:37,564 >>   Num examples = 651\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:59:37,564 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:59:37,564 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:59:37,564 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:59:37,564 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:59:37,564 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:26,  1.31s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.03s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.18it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:12,  1.40it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.61it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.80it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.96it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.10it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.20it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.28it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.34it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.38it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.41it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.43it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.45it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.46it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.47it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.47it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.48it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.99it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.99it/s][INFO|trainer.py:1349] 2021-06-30 20:59:46,693 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  2.99it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.30it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 20:59:46,695 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 20:59:46,696 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 20:59:47,332 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 20:59:47,332 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 20:59:47,332 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 20:59:47,429 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:59:47,430 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:59:47,430 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:59:47,431 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.04it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 20:59:48,075 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 20:59:48,077 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 20:59:48,077 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 20:59:48,077 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.49it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 20:59:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 20:59:50 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\runs\\Jun30_20-59-50_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "squad: max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      " 25 n =  mp_parameters=,3\n",
      "\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 20:59:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-15a40e50f0401df3.arrow\n",
      "06/30/2021 20:59:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-bc5dcf634e32134c.arrow\n",
      "06/30/2021 20:59:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-19f35eaca0feafa8.arrow\n",
      "{'loss': 1.8247, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.503, 'train_samples_per_second': 70.681, 'train_steps_per_second': 2.235, 'train_loss': 1.8246757105777138, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8247\n",
      "  train_runtime            = 0:00:08.50\n",
      "  train_samples            =        601\n",
      "  train_samples_per_second =     70.681\n",
      "  train_steps_per_second   =      2.235\n",
      "06/30/2021 21:00:03 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:00:03 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 21:00:04 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 21:00:04 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 21:00:04 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:00:04 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 21:00:04 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 21:00:04 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 20:59:51,369 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 20:59:51,370 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 20:59:51,372 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:51,372 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:51,372 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:51,372 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:51,372 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:51,372 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 20:59:51,372 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 20:59:51,443 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 20:59:52,171 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 20:59:52,171 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 20:59:54,232 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 20:59:54,232 >>   Num examples = 601\n",
      "[INFO|trainer.py:1155] 2021-06-30 20:59:54,232 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 20:59:54,232 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 20:59:54,232 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 20:59:54,232 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 20:59:54,232 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.33s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.05s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.17it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.39it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.60it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.79it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.95it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.09it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.19it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.27it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.33it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.38it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.44it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.45it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.46it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.63it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.63it/s][INFO|trainer.py:1349] 2021-06-30 21:00:02,735 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.63it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.24it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:00:02,737 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:00:02,737 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:00:03,381 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:00:03,382 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:00:03,382 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:00:03,465 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:00:03,467 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:00:03,467 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:00:03,467 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.06it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:00:04,109 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:00:04,111 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:00:04,111 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:00:04,111 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:00:06 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:00:06 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\runs\\Jun30_21-00-06_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:00:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-b346cdf772aafbcc.arrow\n",
      "06/30/2021 21:00:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-ebe55fbb786a6e06.arrow\n",
      "06/30/2021 21:00:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-a22966f7b8cf3c62.arrow\n",
      "{'loss': 1.0772, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.479, 'train_samples_per_second': 70.881, 'train_steps_per_second': 2.241, 'train_loss': 1.0772309554250616, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.0772\n",
      "  train_runtime            = 0:00:08.47\n",
      "  train_samples            =        601\n",
      "  train_samples_per_second =     70.881\n",
      "  train_steps_per_second   =      2.241\n",
      "06/30/2021 21:00:19 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:00:19 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 21:00:19 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 21:00:19 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  76\n",
      "06/30/2021 21:00:20 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:00:20 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 21:00:20 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 21:00:20 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  75\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "bio:  20 n=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:00:07,401 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:00:07,401 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:00:07,403 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:07,403 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:07,403 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:07,404 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:07,404 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:07,404 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:07,404 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:00:07,474 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:00:08,211 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:00:08,211 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:00:10,181 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:00:10,181 >>   Num examples = 601\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:00:10,181 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:00:10,181 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:00:10,181 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:00:10,181 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:00:10,182 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.31s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.04s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.18it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.40it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.61it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.80it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.96it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.10it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.20it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.28it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.34it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.38it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.44it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.46it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.46it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.63it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.63it/s][INFO|trainer.py:1349] 2021-06-30 21:00:18,661 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.63it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.24it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:00:18,662 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:00:18,663 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:00:19,280 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:00:19,281 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:00:19,281 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_1/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:00:19,368 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:00:19,370 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:00:19,370 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:00:19,370 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.04it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:00:20,015 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:00:20,017 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:00:20,017 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:00:20,017 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:00:22 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:00:22 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\runs\\Jun30_21-00-22_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:00:27 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-adf086bfa98ed9cf.arrow\n",
      "06/30/2021 21:00:27 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-08ea23aa93ac187f.arrow\n",
      "{'loss': 5.3238, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.686, 'train_samples_per_second': 71.264, 'train_steps_per_second': 2.303, 'train_loss': 5.323825073242188, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3238\n",
      "  train_runtime            = 0:00:08.68\n",
      "  train_samples            =        619\n",
      "  train_samples_per_second =     71.264\n",
      "  train_steps_per_second   =      2.303\n",
      "06/30/2021 21:00:39 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:00:39 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 21:00:39 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 21:00:39 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/30/2021 21:00:39 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:00:40 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:00:40 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 21:00:40 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "squad:  50 n =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 21:00:23,691 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:00:23,692 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 21:00:24,022 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 21:00:24,357 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:00:24,358 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:00:26,334 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:00:26,334 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:00:26,334 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:00:26,335 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:00:26,335 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:00:26,335 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 21:00:26,722 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 21:00:27,431 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 21:00:27,431 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.45ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.45ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:00:29,776 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:00:29,776 >>   Num examples = 619\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:00:29,776 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:00:29,776 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:00:29,776 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:00:29,776 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:00:29,777 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.27s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.01s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.21it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.43it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.64it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.82it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.98it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.11it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.21it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.29it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.34it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.38it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.41it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.43it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.45it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.47it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.48it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.99it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.99it/s][INFO|trainer.py:1349] 2021-06-30 21:00:38,463 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.99it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.30it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:00:38,464 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:00:38,465 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:00:39,084 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:00:39,085 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:00:39,086 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:00:39,171 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:00:39,173 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:00:39,173 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:00:39,173 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.95it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.13it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:00:39,811 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:00:39,812 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:00:39,812 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:00:39,812 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:00:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:00:42 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\runs\\Jun30_21-00-42_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:00:44 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-9f726535b3cb8d53.arrow\n",
      "06/30/2021 21:00:44 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-af2e9b2946b89313.arrow\n",
      "06/30/2021 21:00:44 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-587a05b1b392ee63.arrow\n",
      "{'loss': 3.2306, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.122, 'train_samples_per_second': 71.147, 'train_steps_per_second': 2.302, 'train_loss': 3.2305741083054316, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2306\n",
      "  train_runtime            = 0:00:09.12\n",
      "  train_samples            =        649\n",
      "  train_samples_per_second =     71.147\n",
      "  train_steps_per_second   =      2.302\n",
      "06/30/2021 21:00:55 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:00:56 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 21:00:56 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 21:00:56 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/30/2021 21:00:56 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:00:56 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:00:57 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 21:00:57 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:00:43,160 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:00:43,160 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:00:43,163 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:43,163 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:43,163 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:43,163 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:43,163 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:43,163 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:43,163 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:00:43,235 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:00:43,958 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:00:43,958 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:00:45,983 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:00:45,983 >>   Num examples = 649\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:00:45,983 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:00:45,983 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:00:45,983 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:00:45,983 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:00:45,983 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:26,  1.32s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.05s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.17it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:12,  1.39it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.60it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.79it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.96it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.09it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.20it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.27it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.33it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.38it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.41it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.43it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.45it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.46it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.46it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.47it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  3.06it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  3.06it/s][INFO|trainer.py:1349] 2021-06-30 21:00:55,105 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  3.06it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.30it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:00:55,107 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:00:55,108 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:00:55,740 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:00:55,741 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:00:55,741 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:00:55,825 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:00:55,826 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:00:55,826 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:00:55,826 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.11it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:00:56,465 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:00:56,467 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:00:56,467 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:00:56,467 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:00:59 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:00:59 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\runs\\Jun30_21-00-59_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "squad:         num_rows: 16 25 n =  3\n",
      "\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:01:00 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-5457f21c48b0ab3b.arrow\n",
      "06/30/2021 21:01:00 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-70bd3ec68a1e4525.arrow\n",
      "06/30/2021 21:01:00 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-d7a72e92a8beed36.arrow\n",
      "{'loss': 1.7699, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.441, 'train_samples_per_second': 70.963, 'train_steps_per_second': 2.251, 'train_loss': 1.769936009457237, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.7699\n",
      "  train_runtime            = 0:00:08.44\n",
      "  train_samples            =        599\n",
      "  train_samples_per_second =     70.963\n",
      "  train_steps_per_second   =      2.251\n",
      "06/30/2021 21:01:11 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:01:12 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 21:01:12 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 21:01:12 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/30/2021 21:01:12 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:01:12 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:01:13 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 21:01:13 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:00:59,763 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:00:59,764 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:00:59,765 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:59,766 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:59,766 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:59,766 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:59,766 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:59,766 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:00:59,766 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:00:59,837 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:01:00,566 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:01:00,566 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:01:02,588 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:01:02,588 >>   Num examples = 599\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:01:02,588 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:01:02,588 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:01:02,588 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:01:02,588 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:01:02,588 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.29s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.02s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.19it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.41it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.62it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.81it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.97it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.10it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.21it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.28it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.34it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.38it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.45it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.46it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.47it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s][INFO|trainer.py:1349] 2021-06-30 21:01:11,029 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.25it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:01:11,031 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:01:11,032 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:01:11,652 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:01:11,652 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:01:11,653 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:01:11,737 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:01:11,739 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:01:11,739 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:01:11,739 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.09it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:01:12,379 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:01:12,381 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:01:12,381 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:01:12,381 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.90it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:01:15 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:01:15 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\runs\\Jun30_21-01-15_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:01:16 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-afbb4b44da8d820b.arrow\n",
      "06/30/2021 21:01:16 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-05fac95f5809f3dd.arrow\n",
      "06/30/2021 21:01:16 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-16673c8560e30225.arrow\n",
      "{'loss': 0.9736, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.455, 'train_samples_per_second': 70.846, 'train_steps_per_second': 2.247, 'train_loss': 0.9736481716758326, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.9736\n",
      "  train_runtime            = 0:00:08.45\n",
      "  train_samples            =        599\n",
      "  train_samples_per_second =     70.846\n",
      "  train_steps_per_second   =      2.247\n",
      "06/30/2021 21:01:27 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:01:27 - INFO - utils_qa -   Post-processing 2 example predictions split into 75 features.\n",
      "06/30/2021 21:01:28 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 21:01:28 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  75\n",
      "06/30/2021 21:01:28 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:01:28 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:01:28 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 21:01:28 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "bio:  20 n=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:01:15,633 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:01:15,633 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:01:15,635 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:15,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:15,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:15,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:15,635 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:15,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:15,635 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:01:15,703 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:01:16,414 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:01:16,414 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:01:18,338 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:01:18,339 >>   Num examples = 599\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:01:18,339 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:01:18,339 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:01:18,339 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:01:18,339 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:01:18,339 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.29s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.03s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.19it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.41it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.62it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.81it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.97it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.10it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.20it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.27it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.33it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.37it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.44it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.45it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.46it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s][INFO|trainer.py:1349] 2021-06-30 21:01:26,794 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.66it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.25it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:01:26,795 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:01:26,796 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:01:27,433 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:01:27,434 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:01:27,434 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_2/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:01:27,526 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:01:27,528 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:01:27,528 >>   Num examples = 75\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:01:27,528 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.49it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  6.11it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:01:28,166 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:01:28,167 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:01:28,167 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:01:28,167 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:01:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:01:31 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\runs\\Jun30_21-01-30_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:01:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-bc6dd3b1abe54326.arrow\n",
      "06/30/2021 21:01:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-0a3c948d36f7d6af.arrow\n",
      "{'loss': 5.297, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.6969, 'train_samples_per_second': 70.83, 'train_steps_per_second': 2.3, 'train_loss': 5.296953582763672, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      5.297\n",
      "  train_runtime            = 0:00:08.69\n",
      "  train_samples            =        616\n",
      "  train_samples_per_second =      70.83\n",
      "  train_steps_per_second   =        2.3\n",
      "06/30/2021 21:01:47 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:01:47 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:01:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 21:01:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/30/2021 21:01:48 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:01:48 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:01:48 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 21:01:48 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "squad:  50 n =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 21:01:31,816 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:01:31,817 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 21:01:32,145 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 21:01:32,474 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:01:32,474 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:01:34,423 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:01:34,423 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:01:34,423 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:01:34,423 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:01:34,423 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:01:34,423 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 21:01:34,808 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 21:01:35,512 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 21:01:35,512 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.29ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.29ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:01:37,908 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:01:37,908 >>   Num examples = 616\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:01:37,908 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:01:37,908 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:01:37,908 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:01:37,908 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:01:37,908 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:25,  1.32s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.04s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.18it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.40it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.61it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.80it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.96it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.09it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:05,  2.20it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.27it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.33it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.38it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.41it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.43it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.45it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.47it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  3.10it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.10it/s][INFO|trainer.py:1349] 2021-06-30 21:01:46,605 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  3.10it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.30it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:01:46,607 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:01:46,608 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:01:47,257 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:01:47,257 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:01:47,258 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:01:47,352 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:01:47,354 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:01:47,354 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:01:47,354 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.26it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.26it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.67it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:01:48,030 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:01:48,032 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:01:48,032 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:01:48,032 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:01:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:01:50 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\runs\\Jun30_21-01-50_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:01:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-e4bdb516a555451e.arrow\n",
      "06/30/2021 21:01:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-067c035beab34611.arrow\n",
      "06/30/2021 21:01:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-549c93bd46e01b9d.arrow\n",
      "{'loss': 3.302, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.0659, 'train_samples_per_second': 71.256, 'train_steps_per_second': 2.316, 'train_loss': 3.3020241146995906, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      3.302\n",
      "  train_runtime            = 0:00:09.06\n",
      "  train_samples            =        646\n",
      "  train_samples_per_second =     71.256\n",
      "  train_steps_per_second   =      2.316\n",
      "06/30/2021 21:02:03 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:02:04 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:02:04 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 21:02:04 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/30/2021 21:02:04 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:02:04 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:02:05 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 21:02:05 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:01:51,244 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:01:51,244 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:01:51,246 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:51,246 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:51,246 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:51,246 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:51,246 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:51,246 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:01:51,247 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:01:51,315 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:01:52,038 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:01:52,038 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:01:54,038 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:01:54,038 >>   Num examples = 646\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:01:54,038 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:01:54,038 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:01:54,038 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:01:54,038 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:01:54,038 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:26,  1.30s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.03s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.19it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:12,  1.41it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.62it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.81it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.97it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.10it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.20it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.28it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.34it/s]\n",
      " 57%|#####7    | 12/21 [00:05<00:03,  2.38it/s]\n",
      " 62%|######1   | 13/21 [00:06<00:03,  2.41it/s]\n",
      " 67%|######6   | 14/21 [00:06<00:02,  2.43it/s]\n",
      " 71%|#######1  | 15/21 [00:06<00:02,  2.45it/s]\n",
      " 76%|#######6  | 16/21 [00:07<00:02,  2.46it/s]\n",
      " 81%|########  | 17/21 [00:07<00:01,  2.47it/s]\n",
      " 86%|########5 | 18/21 [00:08<00:01,  2.47it/s]\n",
      " 90%|######### | 19/21 [00:08<00:00,  2.48it/s]\n",
      " 95%|#########5| 20/21 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  3.15it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  3.15it/s][INFO|trainer.py:1349] 2021-06-30 21:02:03,104 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:09<00:00,  3.15it/s]\n",
      "100%|##########| 21/21 [00:09<00:00,  2.32it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:02:03,105 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:02:03,106 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:02:03,757 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:02:03,757 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:02:03,757 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:02:03,853 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:02:03,854 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:02:03,854 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:02:03,854 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.85it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:02:04,514 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:02:04,515 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:02:04,515 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:02:04,515 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:02:07 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:02:07 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\runs\\Jun30_21-02-07_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:02:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-3b5b4d2ec0bc5118.arrow\n",
      "06/30/2021 21:02:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-02cb0dd4255f3a45.arrow\n",
      "06/30/2021 21:02:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-5210a4b31d1e533d.arrow\n",
      "{'loss': 1.8646, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.398, 'train_samples_per_second': 70.969, 'train_steps_per_second': 2.262, 'train_loss': 1.8645904942562705, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8646\n",
      "  train_runtime            = 0:00:08.39\n",
      "  train_samples            =        596\n",
      "  train_samples_per_second =     70.969\n",
      "  train_steps_per_second   =      2.262\n",
      "06/30/2021 21:02:19 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:02:20 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:02:20 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 21:02:20 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/30/2021 21:02:20 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:02:20 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:02:20 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 21:02:20 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "squad:  25 n =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:02:07,788 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:02:07,788 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:02:07,790 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:07,791 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:07,791 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:07,791 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:07,791 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:07,791 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:07,791 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:02:07,860 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:02:08,578 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:02:08,578 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:02:10,544 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:02:10,544 >>   Num examples = 596\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:02:10,544 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:02:10,544 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:02:10,544 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:02:10,545 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:02:10,545 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.28s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.02s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.20it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.42it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.63it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.82it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.97it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.10it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.21it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.28it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.34it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.38it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.45it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.46it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.47it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.75it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.75it/s][INFO|trainer.py:1349] 2021-06-30 21:02:18,943 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.75it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.26it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:02:18,944 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:02:18,945 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:02:19,604 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:02:19,605 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:02:19,605 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:02:19,694 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:02:19,696 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:02:19,696 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:02:19,696 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.87it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:02:20,355 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:02:20,357 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:02:20,357 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:02:20,357 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:02:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:02:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\runs\\Jun30_21-02-23_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:02:24 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-e5a5623d3c00c3c8.arrow\n",
      "06/30/2021 21:02:24 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-4efb2d70225c1f9a.arrow\n",
      "06/30/2021 21:02:24 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-06ae8a55842f2546.arrow\n",
      "{'loss': 1.1159, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.4124, 'train_samples_per_second': 70.848, 'train_steps_per_second': 2.259, 'train_loss': 1.1158967269094366, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.1159\n",
      "  train_runtime            = 0:00:08.41\n",
      "  train_samples            =        596\n",
      "  train_samples_per_second =     70.848\n",
      "  train_steps_per_second   =      2.259\n",
      "06/30/2021 21:02:35 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:02:36 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:02:36 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 21:02:36 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  78\n",
      "06/30/2021 21:02:36 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:02:36 - INFO - utils_qa -   Post-processing 2 example predictions split into 78 features.\n",
      "06/30/2021 21:02:37 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 21:02:37 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  78\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "bio:  20 n=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:02:23,591 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:02:23,591 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:02:23,599 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:23,600 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:23,600 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:23,600 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:23,600 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:23,600 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:23,600 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:02:23,673 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:02:24,391 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:02:24,391 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:02:26,593 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:02:26,593 >>   Num examples = 596\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:02:26,593 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:02:26,593 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:02:26,593 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:02:26,593 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:02:26,593 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.30s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.03s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.19it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.41it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.62it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.81it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.97it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.10it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.21it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.28it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.34it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.38it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.44it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.46it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.47it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.75it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.75it/s][INFO|trainer.py:1349] 2021-06-30 21:02:35,004 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.75it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.26it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:02:35,006 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:02:35,007 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:02:35,645 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:02:35,646 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:02:35,646 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_3/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:02:35,737 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:02:35,739 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:02:35,739 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:02:35,739 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.76it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.88it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:02:36,396 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:02:36,398 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:02:36,398 >>   Num examples = 78\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:02:36,398 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.85it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:02:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:02:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\runs\\Jun30_21-02-39_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_0,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  roberta-base\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:02:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-ae4aa0111fe85b4c.arrow\n",
      "06/30/2021 21:02:43 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-41cd112b5e4173ce.arrow\n",
      "{'loss': 5.3216, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.703, 'train_samples_per_second': 71.125, 'train_steps_per_second': 2.298, 'train_loss': 5.321617889404297, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3216\n",
      "  train_runtime            = 0:00:08.70\n",
      "  train_samples            =        619\n",
      "  train_samples_per_second =     71.125\n",
      "  train_steps_per_second   =      2.298\n",
      "06/30/2021 21:02:55 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:02:56 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/30/2021 21:02:56 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\eval_predictions.json.\n",
      "06/30/2021 21:02:56 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/30/2021 21:02:56 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:02:56 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 21:02:57 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\predict_predictions.json.\n",
      "06/30/2021 21:02:57 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n",
      "squad:  50 n =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:530] 2021-06-30 21:02:40,006 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:02:40,006 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-06-30 21:02:40,330 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:530] 2021-06-30 21:02:40,651 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:02:40,651 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:02:42,615 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:02:42,615 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:02:42,615 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:02:42,615 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:02:42,615 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-30 21:02:42,615 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1161] 2021-06-30 21:02:43,002 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\unitu/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1336] 2021-06-30 21:02:43,708 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1347] 2021-06-30 21:02:43,708 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.33ba/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  6.33ba/s]\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:02:46,400 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:02:46,400 >>   Num examples = 619\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:02:46,400 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:02:46,400 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:02:46,400 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:02:46,400 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:02:46,401 >>   Total optimization steps = 20\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/20 [00:01<00:24,  1.28s/it]\n",
      " 10%|#         | 2/20 [00:01<00:18,  1.02s/it]\n",
      " 15%|#5        | 3/20 [00:02<00:14,  1.20it/s]\n",
      " 20%|##        | 4/20 [00:02<00:11,  1.42it/s]\n",
      " 25%|##5       | 5/20 [00:02<00:09,  1.63it/s]\n",
      " 30%|###       | 6/20 [00:03<00:07,  1.82it/s]\n",
      " 35%|###5      | 7/20 [00:03<00:06,  1.98it/s]\n",
      " 40%|####      | 8/20 [00:04<00:05,  2.11it/s]\n",
      " 45%|####5     | 9/20 [00:04<00:04,  2.21it/s]\n",
      " 50%|#####     | 10/20 [00:04<00:04,  2.29it/s]\n",
      " 55%|#####5    | 11/20 [00:05<00:03,  2.34it/s]\n",
      " 60%|######    | 12/20 [00:05<00:03,  2.38it/s]\n",
      " 65%|######5   | 13/20 [00:06<00:02,  2.41it/s]\n",
      " 70%|#######   | 14/20 [00:06<00:02,  2.43it/s]\n",
      " 75%|#######5  | 15/20 [00:06<00:02,  2.45it/s]\n",
      " 80%|########  | 16/20 [00:07<00:01,  2.46it/s]\n",
      " 85%|########5 | 17/20 [00:07<00:01,  2.47it/s]\n",
      " 90%|######### | 18/20 [00:08<00:00,  2.47it/s]\n",
      " 95%|#########5| 19/20 [00:08<00:00,  2.48it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.98it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.98it/s][INFO|trainer.py:1349] 2021-06-30 21:02:55,103 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 20/20 [00:08<00:00,  2.98it/s]\n",
      "100%|##########| 20/20 [00:08<00:00,  2.30it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:02:55,105 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:02:55,106 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:02:55,754 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:02:55,754 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:02:55,755 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:02:55,842 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:02:55,844 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:02:55,844 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:02:55,844 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.66it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.85it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:02:56,504 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:02:56,505 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:02:56,505 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:02:56,505 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.60it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.00it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:02:59 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:02:59 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\runs\\Jun30_21-02-59_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_1,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:03:00 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-9fdedc4693bc52dc.arrow\n",
      "06/30/2021 21:03:00 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-1efa336d35a8a423.arrow\n",
      "06/30/2021 21:03:00 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-47ffe8f4f8c2a86b.arrow\n",
      "{'loss': 3.2231, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 10.9033, 'train_samples_per_second': 59.523, 'train_steps_per_second': 1.926, 'train_loss': 3.2230518886021207, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2231\n",
      "  train_runtime            = 0:00:10.90\n",
      "  train_samples            =        649\n",
      "  train_samples_per_second =     59.523\n",
      "  train_steps_per_second   =      1.926\n",
      "06/30/2021 21:03:14 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:03:14 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/30/2021 21:03:14 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\eval_predictions.json.\n",
      "06/30/2021 21:03:14 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/30/2021 21:03:14 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:03:15 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 21:03:15 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\predict_predictions.json.\n",
      "06/30/2021 21:03:15 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:02:59,696 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:02:59,696 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:02:59,698 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:59,699 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:59,699 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:59,699 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:59,699 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:59,699 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:02:59,699 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:02:59,767 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:03:00,482 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:03:00,483 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:03:02,488 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:03:02,488 >>   Num examples = 649\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:03:02,488 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:03:02,488 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:03:02,488 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:03:02,488 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:03:02,488 >>   Total optimization steps = 21\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "  5%|4         | 1/21 [00:01<00:25,  1.29s/it]\n",
      " 10%|9         | 2/21 [00:01<00:19,  1.02s/it]\n",
      " 14%|#4        | 3/21 [00:02<00:15,  1.19it/s]\n",
      " 19%|#9        | 4/21 [00:02<00:12,  1.41it/s]\n",
      " 24%|##3       | 5/21 [00:02<00:09,  1.62it/s]\n",
      " 29%|##8       | 6/21 [00:03<00:08,  1.81it/s]\n",
      " 33%|###3      | 7/21 [00:03<00:07,  1.97it/s]\n",
      " 38%|###8      | 8/21 [00:04<00:06,  2.10it/s]\n",
      " 43%|####2     | 9/21 [00:04<00:05,  2.20it/s]\n",
      " 48%|####7     | 10/21 [00:04<00:04,  2.28it/s]\n",
      " 52%|#####2    | 11/21 [00:05<00:04,  2.34it/s]\n",
      " 57%|#####7    | 12/21 [00:07<00:08,  1.04it/s]\n",
      " 62%|######1   | 13/21 [00:07<00:06,  1.26it/s]\n",
      " 67%|######6   | 14/21 [00:08<00:04,  1.48it/s]\n",
      " 71%|#######1  | 15/21 [00:08<00:03,  1.68it/s]\n",
      " 76%|#######6  | 16/21 [00:09<00:02,  1.86it/s]\n",
      " 81%|########  | 17/21 [00:09<00:01,  2.01it/s]\n",
      " 86%|########5 | 18/21 [00:09<00:01,  2.13it/s]\n",
      " 90%|######### | 19/21 [00:10<00:00,  2.23it/s]\n",
      " 95%|#########5| 20/21 [00:10<00:00,  2.30it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  2.87it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.87it/s][INFO|trainer.py:1349] 2021-06-30 21:03:13,391 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 21/21 [00:10<00:00,  2.87it/s]\n",
      "100%|##########| 21/21 [00:10<00:00,  1.93it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:03:13,392 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:03:13,393 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:03:14,001 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:03:14,002 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:03:14,002 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:03:14,085 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:03:14,087 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:03:14,087 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:03:14,087 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.80it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.98it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:03:14,736 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:03:14,738 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:03:14,738 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:03:14,738 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:03:17 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:03:17 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\runs\\Jun30_21-03-17_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,squad: \n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      " 25 run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2,\n",
      "save_steps=500,\n",
      "n =  3\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:03:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-3cc9ba9d1b4d731c.arrow\n",
      "06/30/2021 21:03:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-5c952923b0e48025.arrow\n",
      "06/30/2021 21:03:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-d263c4e923996a8d.arrow\n",
      "{'loss': 1.7774, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.4726, 'train_samples_per_second': 70.698, 'train_steps_per_second': 2.243, 'train_loss': 1.777440121299342, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.7774\n",
      "  train_runtime            = 0:00:08.47\n",
      "  train_samples            =        599\n",
      "  train_samples_per_second =     70.698\n",
      "  train_steps_per_second   =      2.243\n",
      "06/30/2021 21:03:30 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:03:30 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/30/2021 21:03:30 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\eval_predictions.json.\n",
      "06/30/2021 21:03:30 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/30/2021 21:03:30 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:03:31 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 21:03:31 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\predict_predictions.json.\n",
      "06/30/2021 21:03:31 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:03:18,072 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:03:18,073 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_0\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:03:18,075 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:18,075 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:18,075 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:18,075 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:18,075 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:18,075 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:18,075 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:03:18,146 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:03:18,860 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:03:18,860 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:03:20,833 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:03:20,833 >>   Num examples = 599\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:03:20,833 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:03:20,833 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:03:20,833 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:03:20,833 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:03:20,833 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.32s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.04s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.18it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.40it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.61it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.80it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.96it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.10it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.20it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.27it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.33it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.37it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.45it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.46it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.46it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s][INFO|trainer.py:1349] 2021-06-30 21:03:29,306 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.24it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:03:29,307 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:03:29,308 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:03:29,929 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:03:29,929 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:03:29,930 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:03:30,014 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:03:30,016 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:03:30,016 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:03:30,016 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.94it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:03:30,673 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:03:30,675 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:03:30,675 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:03:30,675 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.05it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/30/2021 21:03:33 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/30/2021 21:03:33 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\runs\\Jun30_21-03-33_DESKTOP-1CU6Q06,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=checkpoint_3,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "made it to the datasets :  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-06-30 21:03:33,892 >> loading configuration file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\config.json\n",
      "[INFO|configuration_utils.py:566] 2021-06-30 21:03:33,893 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-06-30 21:03:33,901 >> Didn't find file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:33,901 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:33,901 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:33,901 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:33,901 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:33,901 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-06-30 21:03:33,902 >> loading file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1159] 2021-06-30 21:03:33,968 >> loading weights file ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-06-30 21:03:34,673 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1353] 2021-06-30 21:03:34,673 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "[INFO|trainer.py:1153] 2021-06-30 21:03:36,649 >> ***** Running training *****\n",
      "[INFO|trainer.py:1154] 2021-06-30 21:03:36,649 >>   Num examples = 599\n",
      "[INFO|trainer.py:1155] 2021-06-30 21:03:36,649 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1156] 2021-06-30 21:03:36,649 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1157] 2021-06-30 21:03:36,649 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1158] 2021-06-30 21:03:36,649 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1159] 2021-06-30 21:03:36,649 >>   Total optimization steps = 19\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      "  5%|5         | 1/19 [00:01<00:23,  1.29s/it]\n",
      " 11%|#         | 2/19 [00:01<00:17,  1.02s/it]\n",
      " 16%|#5        | 3/19 [00:02<00:13,  1.19it/s]\n",
      " 21%|##1       | 4/19 [00:02<00:10,  1.41it/s]\n",
      " 26%|##6       | 5/19 [00:02<00:08,  1.62it/s]\n",
      " 32%|###1      | 6/19 [00:03<00:07,  1.81it/s]\n",
      " 37%|###6      | 7/19 [00:03<00:06,  1.98it/s]\n",
      " 42%|####2     | 8/19 [00:04<00:05,  2.11it/s]\n",
      " 47%|####7     | 9/19 [00:04<00:04,  2.21it/s]\n",
      " 53%|#####2    | 10/19 [00:04<00:03,  2.29it/s]\n",
      " 58%|#####7    | 11/19 [00:05<00:03,  2.34it/s]\n",
      " 63%|######3   | 12/19 [00:05<00:02,  2.39it/s]\n",
      " 68%|######8   | 13/19 [00:06<00:02,  2.41it/s]\n",
      " 74%|#######3  | 14/19 [00:06<00:02,  2.43it/s]\n",
      " 79%|#######8  | 15/19 [00:06<00:01,  2.45it/s]\n",
      " 84%|########4 | 16/19 [00:07<00:01,  2.46it/s]\n",
      " 89%|########9 | 17/19 [00:07<00:00,  2.47it/s]\n",
      " 95%|#########4| 18/19 [00:08<00:00,  2.47it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s]\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s][INFO|trainer.py:1349] 2021-06-30 21:03:45,084 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                               \n",
      "\n",
      "100%|##########| 19/19 [00:08<00:00,  2.67it/s]\n",
      "100%|##########| 19/19 [00:08<00:00,  2.25it/s]\n",
      "[INFO|trainer.py:1900] 2021-06-30 21:03:45,086 >> Saving model checkpoint to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\n",
      "[INFO|configuration_utils.py:364] 2021-06-30 21:03:45,087 >> Configuration saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\config.json\n",
      "[INFO|modeling_utils.py:898] 2021-06-30 21:03:45,730 >> Model weights saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-06-30 21:03:45,731 >> tokenizer config file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-06-30 21:03:45,731 >> Special tokens file saved in ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\special_tokens_map.json\n",
      "[INFO|trainer.py:520] 2021-06-30 21:03:45,819 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "[INFO|trainer.py:2146] 2021-06-30 21:03:45,820 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:03:45,820 >>   Num examples = 77\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:03:45,820 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00,  9.71it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.87it/s]\n",
      "[INFO|trainer.py:520] 2021-06-30 21:03:46,479 >> The following columns in the test set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "                 BioASQ data not included\n",
      "88888888888888888888888888888888888888888888888888888888888888888888\n",
      "11111111111111111111111111111111111111111111111111111111111111111111\n",
      "***************************\n",
      "Split  1\n",
      "***************************\n",
      "model path ====  ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_2\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Datasets DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "06/30/2021 21:03:34 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/train\\cache-2d86ca6ec4ac3fc2.arrow\n",
      "06/30/2021 21:03:34 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/validation\\cache-2665b37b98c1de4b.arrow\n",
      "06/30/2021 21:03:34 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../data/full_squad_covidQA/test\\cache-23f0911e270b36f6.arrow\n",
      "{'loss': 0.9482, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 8.435, 'train_samples_per_second': 71.014, 'train_steps_per_second': 2.253, 'train_loss': 0.9481653916208368, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.9482\n",
      "  train_runtime            = 0:00:08.43\n",
      "  train_samples            =        599\n",
      "  train_samples_per_second =     71.014\n",
      "  train_steps_per_second   =      2.253\n",
      "06/30/2021 21:03:45 - INFO - __main__ -   *** Evaluate ***\n",
      "06/30/2021 21:03:46 - INFO - utils_qa -   Post-processing 2 example predictions split into 77 features.\n",
      "06/30/2021 21:03:46 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\eval_predictions.json.\n",
      "06/30/2021 21:03:46 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\eval_nbest_predictions.json.\n",
      "***** eval metrics *****\n",
      "  epoch            = 1.0\n",
      "  eval_exact_match = 0.0\n",
      "  eval_f1          = 0.0\n",
      "  eval_samples     =  77\n",
      "06/30/2021 21:03:46 - INFO - __main__ -   *** Predict ***\n",
      "06/30/2021 21:03:46 - INFO - utils_qa -   Post-processing 2 example predictions split into 76 features.\n",
      "06/30/2021 21:03:47 - INFO - utils_qa -   Saving predictions to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\predict_predictions.json.\n",
      "06/30/2021 21:03:47 - INFO - utils_qa -   Saving nbest_preds to ../models/gradual_ft_baseline_lr1e-5_2/split_4/checkpoint_3\\predict_nbest_predictions.json.\n",
      "***** predict metrics *****\n",
      "  predict_samples  =  76\n",
      "  test_exact_match = 0.0\n",
      "  test_f1          = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2146] 2021-06-30 21:03:46,481 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2148] 2021-06-30 21:03:46,481 >>   Num examples = 76\n",
      "[INFO|trainer.py:2151] 2021-06-30 21:03:46,481 >>   Batch size = 32\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|######6   | 2/3 [00:00<00:00, 14.71it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\u001B[A\n",
      "100%|##########| 2/2 [00:00<00:00, 10.10it/s]\n",
      "\n",
      "100%|##########| 3/3 [00:00<00:00,  5.65it/s]\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "K = 6\n",
    "\n",
    "for i in range(k_fold):\n",
    "    covid_fold = covid_qa.shard(k_fold, i)\n",
    "\n",
    "    covid_test = covid_fold.shard(2, 0)\n",
    "    covid_val = covid_fold.shard(2, 1)\n",
    "    covid_train = concatenate_datasets([covid_qa.shard(k_fold, j) for j in range(k_fold) if j != i])\n",
    "\n",
    "    #make_and_save_full_dataset(covid_train, squad_qa, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "\n",
    "    checkpoint = 'roberta-base'\n",
    "    cur_dir = '../models/gradual_ft_baseline_lr1e-5_2/split_' + str(i)\n",
    "\n",
    "    squad_qa.shuffle()\n",
    "    bio_qa.shuffle()\n",
    "\n",
    "    num_of_shards = int(K/2)\n",
    "    squad_qa_shards = [squad_qa.shard(num_of_shards,i) for i in range(num_of_shards)]\n",
    "    bio_qa_shards = [bio_qa.shard(num_of_shards,i) for i in range(num_of_shards)]\n",
    "    for n in range(K):\n",
    "        output_dir = cur_dir + '/checkpoint_' + str(n)\n",
    "        try:\n",
    "            if n%2:\n",
    "                squad_qa_cur = concatenate_datasets(squad_qa_shards)\n",
    "                full_dataset = concatenate_datasets([squad_qa_cur, covid_train])\n",
    "                print('squad: ', squad_qa_cur.num_rows, 'n = ',n)\n",
    "                _ = squad_qa_shards.pop()\n",
    "            else:\n",
    "                bio_qa_cur = concatenate_datasets(bio_qa_shards)\n",
    "                full_dataset = concatenate_datasets([bio_qa_cur, covid_train])\n",
    "                print('bio: ', bio_qa_cur.num_rows, 'n= ', n)\n",
    "            _ = bio_qa_shards.pop()\n",
    "        except:\n",
    "            full_dataset = covid_train\n",
    "\n",
    "        make_and_save_full_dataset(full_dataset, covid_val, covid_test, covid_and_squad_dataset_path)\n",
    "\n",
    "        run_gradual_ft(output_dir, checkpoint, covid_val)\n",
    "\n",
    "        checkpoint = output_dir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in n < K-1 40\n",
      "in n < K-1 30\n",
      "in n < K-1 20\n",
      "in n < K-1 10\n",
      "10\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}