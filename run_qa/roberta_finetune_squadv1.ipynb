{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c34b944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/15/2021 17:19:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 6distributed training: False, 16-bits training: False\n",
      "05/15/2021 17:19:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=models, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May15_17-19-32_nlp-gpu-01.soe.ucsc.edu, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=models, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=6, mp_parameters=)\n",
      "05/15/2021 17:19:33 - WARNING - datasets.builder -   Reusing dataset squad (/soe/meidam/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n",
      "[INFO|configuration_utils.py:517] 2021-05-15 17:19:34,923 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-05-15 17:19:34,925 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:517] 2021-05-15 17:19:35,142 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /soe/meidam/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:553] 2021-05-15 17:19:35,143 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 17:19:36,412 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /soe/meidam/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 17:19:36,412 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /soe/meidam/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 17:19:36,412 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /soe/meidam/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 17:19:36,412 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 17:19:36,412 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-15 17:19:36,413 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1149] 2021-05-15 17:19:36,746 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /soe/meidam/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:1322] 2021-05-15 17:19:38,696 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1333] 2021-05-15 17:19:38,696 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████| 88/88 [00:49<00:00,  1.78ba/s]\n",
      "100%|███████████████████████████████████████████| 11/11 [00:16<00:00,  1.48s/ba]\n",
      "[INFO|trainer.py:1144] 2021-05-15 17:20:49,987 >> ***** Running training *****\n",
      "[INFO|trainer.py:1145] 2021-05-15 17:20:49,987 >>   Num examples = 88567\n",
      "[INFO|trainer.py:1146] 2021-05-15 17:20:49,987 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1147] 2021-05-15 17:20:49,987 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1148] 2021-05-15 17:20:49,987 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:1149] 2021-05-15 17:20:49,987 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1150] 2021-05-15 17:20:49,987 >>   Total optimization steps = 7382\n",
      "  0%|                                                  | 0/7382 [00:00<?, ?it/s]/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6406, 'learning_rate': 2.7968030344080195e-05, 'epoch': 0.14}        \n",
      "  7%|██▌                                   | 500/7382 [06:54<1:29:29,  1.28it/s][INFO|trainer.py:1873] 2021-05-15 17:27:44,523 >> Saving model checkpoint to models/checkpoint-500\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 17:27:44,524 >> Configuration saved in models/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 17:27:45,353 >> Model weights saved in models/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 17:27:45,354 >> tokenizer config file saved in models/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 17:27:45,354 >> Special tokens file saved in models/checkpoint-500/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.1199, 'learning_rate': 2.5936060688160393e-05, 'epoch': 0.27}        \n",
      " 14%|█████                                | 1000/7382 [13:24<1:16:56,  1.38it/s][INFO|trainer.py:1873] 2021-05-15 17:34:14,170 >> Saving model checkpoint to models/checkpoint-1000\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 17:34:14,171 >> Configuration saved in models/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 17:34:14,924 >> Model weights saved in models/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 17:34:14,925 >> tokenizer config file saved in models/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 17:34:14,925 >> Special tokens file saved in models/checkpoint-1000/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.0076, 'learning_rate': 2.3904091032240584e-05, 'epoch': 0.41}        \n",
      " 20%|███████▌                             | 1500/7382 [19:50<1:15:05,  1.31it/s][INFO|trainer.py:1873] 2021-05-15 17:40:40,349 >> Saving model checkpoint to models/checkpoint-1500\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 17:40:40,351 >> Configuration saved in models/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 17:40:41,166 >> Model weights saved in models/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 17:40:41,166 >> tokenizer config file saved in models/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 17:40:41,167 >> Special tokens file saved in models/checkpoint-1500/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.9737, 'learning_rate': 2.187212137632078e-05, 'epoch': 0.54}         \n",
      " 27%|██████████                           | 2000/7382 [26:04<1:05:27,  1.37it/s][INFO|trainer.py:1873] 2021-05-15 17:46:54,163 >> Saving model checkpoint to models/checkpoint-2000\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 17:46:54,164 >> Configuration saved in models/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 17:46:54,793 >> Model weights saved in models/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 17:46:54,793 >> tokenizer config file saved in models/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 17:46:54,793 >> Special tokens file saved in models/checkpoint-2000/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.936, 'learning_rate': 1.9840151720400976e-05, 'epoch': 0.68}         \n",
      " 34%|████████████▌                        | 2500/7382 [32:18<1:03:44,  1.28it/s][INFO|trainer.py:1873] 2021-05-15 17:53:08,710 >> Saving model checkpoint to models/checkpoint-2500\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 17:53:08,711 >> Configuration saved in models/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 17:53:09,383 >> Model weights saved in models/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 17:53:09,384 >> tokenizer config file saved in models/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 17:53:09,384 >> Special tokens file saved in models/checkpoint-2500/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.9265, 'learning_rate': 1.7808182064481173e-05, 'epoch': 0.81}        \n",
      " 41%|███████████████▊                       | 3000/7382 [38:32<52:15,  1.40it/s][INFO|trainer.py:1873] 2021-05-15 17:59:22,497 >> Saving model checkpoint to models/checkpoint-3000\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 17:59:22,497 >> Configuration saved in models/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 17:59:23,181 >> Model weights saved in models/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 17:59:23,182 >> tokenizer config file saved in models/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 17:59:23,182 >> Special tokens file saved in models/checkpoint-3000/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.8913, 'learning_rate': 1.5776212408561364e-05, 'epoch': 0.95}        \n",
      " 47%|██████████████████▍                    | 3500/7382 [44:48<50:22,  1.28it/s][INFO|trainer.py:1873] 2021-05-15 18:05:38,127 >> Saving model checkpoint to models/checkpoint-3500\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:05:38,128 >> Configuration saved in models/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:05:38,741 >> Model weights saved in models/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:05:38,741 >> tokenizer config file saved in models/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:05:38,741 >> Special tokens file saved in models/checkpoint-3500/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.7729, 'learning_rate': 1.374424275264156e-05, 'epoch': 1.08}         \n",
      " 54%|█████████████████████▏                 | 4000/7382 [51:03<42:26,  1.33it/s][INFO|trainer.py:1873] 2021-05-15 18:11:53,292 >> Saving model checkpoint to models/checkpoint-4000\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:11:53,293 >> Configuration saved in models/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:11:53,968 >> Model weights saved in models/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:11:53,969 >> tokenizer config file saved in models/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:11:53,970 >> Special tokens file saved in models/checkpoint-4000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.705, 'learning_rate': 1.1712273096721756e-05, 'epoch': 1.22}         \n",
      " 61%|███████████████████████▊               | 4500/7382 [57:17<35:07,  1.37it/s][INFO|trainer.py:1873] 2021-05-15 18:18:07,247 >> Saving model checkpoint to models/checkpoint-4500\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:18:07,248 >> Configuration saved in models/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:18:08,012 >> Model weights saved in models/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:18:08,012 >> tokenizer config file saved in models/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:18:08,013 >> Special tokens file saved in models/checkpoint-4500/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.6837, 'learning_rate': 9.68030344080195e-06, 'epoch': 1.35}          \n",
      " 68%|█████████████████████████            | 5000/7382 [1:03:32<30:47,  1.29it/s][INFO|trainer.py:1873] 2021-05-15 18:24:22,945 >> Saving model checkpoint to models/checkpoint-5000\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:24:22,946 >> Configuration saved in models/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:24:23,673 >> Model weights saved in models/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:24:23,674 >> tokenizer config file saved in models/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:24:23,674 >> Special tokens file saved in models/checkpoint-5000/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.6921, 'learning_rate': 7.648333784882147e-06, 'epoch': 1.49}         \n",
      " 75%|███████████████████████████▌         | 5500/7382 [1:09:50<24:26,  1.28it/s][INFO|trainer.py:1873] 2021-05-15 18:30:40,992 >> Saving model checkpoint to models/checkpoint-5500\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:30:40,992 >> Configuration saved in models/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:30:41,727 >> Model weights saved in models/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:30:41,728 >> tokenizer config file saved in models/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:30:41,728 >> Special tokens file saved in models/checkpoint-5500/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.6776, 'learning_rate': 5.616364128962341e-06, 'epoch': 1.63}         \n",
      " 81%|██████████████████████████████       | 6000/7382 [1:16:15<17:18,  1.33it/s][INFO|trainer.py:1873] 2021-05-15 18:37:05,184 >> Saving model checkpoint to models/checkpoint-6000\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:37:05,186 >> Configuration saved in models/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:37:05,976 >> Model weights saved in models/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:37:05,977 >> tokenizer config file saved in models/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:37:05,977 >> Special tokens file saved in models/checkpoint-6000/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.6795, 'learning_rate': 3.584394473042536e-06, 'epoch': 1.76}         \n",
      " 88%|████████████████████████████████▌    | 6500/7382 [1:22:45<11:50,  1.24it/s][INFO|trainer.py:1873] 2021-05-15 18:43:35,043 >> Saving model checkpoint to models/checkpoint-6500\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:43:35,044 >> Configuration saved in models/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:43:36,083 >> Model weights saved in models/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:43:36,084 >> tokenizer config file saved in models/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:43:36,084 >> Special tokens file saved in models/checkpoint-6500/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.6613, 'learning_rate': 1.552424817122731e-06, 'epoch': 1.9}          \n",
      " 95%|███████████████████████████████████  | 7000/7382 [1:29:15<04:54,  1.30it/s][INFO|trainer.py:1873] 2021-05-15 18:50:05,530 >> Saving model checkpoint to models/checkpoint-7000\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:50:05,531 >> Configuration saved in models/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:50:06,414 >> Model weights saved in models/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:50:06,414 >> tokenizer config file saved in models/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:50:06,415 >> Special tokens file saved in models/checkpoint-7000/special_tokens_map.json\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|█████████████████████████████████████| 7382/7382 [1:34:18<00:00,  1.53it/s][INFO|trainer.py:1340] 2021-05-15 18:55:08,692 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5658.7051, 'train_samples_per_second': 1.305, 'epoch': 2.0}   \n",
      "100%|█████████████████████████████████████| 7382/7382 [1:34:18<00:00,  1.30it/s]\n",
      "[INFO|trainer.py:1873] 2021-05-15 18:55:08,696 >> Saving model checkpoint to models\n",
      "[INFO|configuration_utils.py:351] 2021-05-15 18:55:08,697 >> Configuration saved in models/config.json\n",
      "[INFO|modeling_utils.py:883] 2021-05-15 18:55:09,674 >> Model weights saved in models/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1924] 2021-05-15 18:55:09,675 >> tokenizer config file saved in models/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1930] 2021-05-15 18:55:09,675 >> Special tokens file saved in models/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:907] 2021-05-15 18:55:09,816 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-15 18:55:09,817 >>   epoch                    =        2.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-15 18:55:09,817 >>   train_runtime            = 1:34:18.70\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-15 18:55:09,817 >>   train_samples            =      88567\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-15 18:55:09,817 >>   train_samples_per_second =      1.305\n",
      "05/15/2021 18:55:09 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:514] 2021-05-15 18:55:09,818 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "[INFO|trainer.py:2496] 2021-05-15 18:55:09,822 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2497] 2021-05-15 18:55:09,822 >>   Num examples = 10790\n",
      "[INFO|trainer.py:2498] 2021-05-15 18:55:09,822 >>   Batch size = 48\n",
      "/data/users/meidam/miniconda3/lib/python3.8/site-packages/transformers/trainer_pt_utils.py:366: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 225/225 [01:59<00:00,  1.92it/s]05/15/2021 18:57:21 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10790 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                      | 42/10570 [00:00<00:25, 416.63it/s]\u001b[A\n",
      "  1%|▎                                      | 76/10570 [00:00<00:26, 389.51it/s]\u001b[A\n",
      "  1%|▍                                     | 125/10570 [00:00<00:25, 413.77it/s]\u001b[A\n",
      "  2%|▋                                     | 174/10570 [00:00<00:24, 432.96it/s]\u001b[A\n",
      "  2%|▊                                     | 220/10570 [00:00<00:23, 440.66it/s]\u001b[A\n",
      "  2%|▉                                     | 258/10570 [00:00<00:36, 279.92it/s]\u001b[A\n",
      "  3%|█                                     | 289/10570 [00:00<00:44, 231.88it/s]\u001b[A\n",
      "  3%|█▏                                    | 327/10570 [00:01<00:39, 261.82it/s]\u001b[A\n",
      "  3%|█▎                                    | 366/10570 [00:01<00:35, 288.70it/s]\u001b[A\n",
      "  4%|█▍                                    | 402/10570 [00:01<00:33, 304.55it/s]\u001b[A\n",
      "  4%|█▌                                    | 446/10570 [00:01<00:30, 335.43it/s]\u001b[A\n",
      "  5%|█▊                                    | 489/10570 [00:01<00:28, 358.59it/s]\u001b[A\n",
      "  5%|█▉                                    | 550/10570 [00:01<00:24, 408.52it/s]\u001b[A\n",
      "  6%|██▏                                   | 595/10570 [00:01<00:24, 401.05it/s]\u001b[A\n",
      "  6%|██▎                                   | 638/10570 [00:01<00:27, 363.08it/s]\u001b[A\n",
      "  6%|██▍                                   | 678/10570 [00:01<00:29, 339.49it/s]\u001b[A\n",
      "  7%|██▌                                   | 728/10570 [00:02<00:26, 375.38it/s]\u001b[A\n",
      "  7%|██▊                                   | 782/10570 [00:02<00:23, 412.72it/s]\u001b[A\n",
      "  8%|██▉                                   | 827/10570 [00:02<00:23, 421.96it/s]\u001b[A\n",
      "  8%|███▏                                  | 884/10570 [00:02<00:21, 457.22it/s]\u001b[A\n",
      "  9%|███▎                                  | 933/10570 [00:02<00:25, 372.14it/s]\u001b[A\n",
      "  9%|███▌                                  | 975/10570 [00:02<00:36, 266.16it/s]\u001b[A\n",
      " 10%|███▌                                 | 1025/10570 [00:02<00:30, 308.96it/s]\u001b[A\n",
      " 10%|███▊                                 | 1077/10570 [00:03<00:27, 350.52it/s]\u001b[A\n",
      " 11%|███▉                                 | 1120/10570 [00:03<00:25, 365.76it/s]\u001b[A\n",
      " 11%|████                                 | 1163/10570 [00:03<00:24, 382.69it/s]\u001b[A\n",
      " 12%|████▎                                | 1219/10570 [00:03<00:22, 422.17it/s]\u001b[A\n",
      " 12%|████▍                                | 1277/10570 [00:03<00:20, 459.34it/s]\u001b[A\n",
      " 13%|████▋                                | 1335/10570 [00:03<00:18, 488.73it/s]\u001b[A\n",
      " 13%|████▊                                | 1388/10570 [00:03<00:22, 407.77it/s]\u001b[A\n",
      " 14%|█████                                | 1434/10570 [00:03<00:21, 418.76it/s]\u001b[A\n",
      " 14%|█████▏                               | 1493/10570 [00:03<00:19, 457.73it/s]\u001b[A\n",
      " 15%|█████▍                               | 1551/10570 [00:03<00:18, 486.66it/s]\u001b[A\n",
      " 15%|█████▌                               | 1603/10570 [00:04<00:18, 494.94it/s]\u001b[A\n",
      " 16%|█████▊                               | 1655/10570 [00:04<00:18, 489.59it/s]\u001b[A\n",
      " 16%|█████▉                               | 1709/10570 [00:04<00:17, 503.16it/s]\u001b[A\n",
      " 17%|██████▏                              | 1769/10570 [00:04<00:16, 526.25it/s]\u001b[A\n",
      " 17%|██████▍                              | 1826/10570 [00:04<00:16, 536.78it/s]\u001b[A\n",
      " 18%|██████▌                              | 1881/10570 [00:04<00:17, 490.79it/s]\u001b[A\n",
      " 18%|██████▊                              | 1932/10570 [00:04<00:18, 471.99it/s]\u001b[A\n",
      " 19%|██████▉                              | 1981/10570 [00:04<00:18, 477.03it/s]\u001b[A\n",
      " 19%|███████▏                             | 2039/10570 [00:04<00:16, 502.33it/s]\u001b[A\n",
      " 20%|███████▎                             | 2100/10570 [00:05<00:16, 528.62it/s]\u001b[A\n",
      " 20%|███████▌                             | 2154/10570 [00:05<00:16, 509.50it/s]\u001b[A\n",
      " 21%|███████▋                             | 2208/10570 [00:05<00:16, 517.34it/s]\u001b[A\n",
      " 21%|███████▉                             | 2261/10570 [00:05<00:19, 426.20it/s]\u001b[A\n",
      " 22%|████████                             | 2307/10570 [00:05<00:19, 427.49it/s]\u001b[A\n",
      " 22%|████████▎                            | 2365/10570 [00:05<00:17, 463.64it/s]\u001b[A\n",
      " 23%|████████▍                            | 2420/10570 [00:05<00:16, 485.66it/s]\u001b[A\n",
      " 23%|████████▋                            | 2471/10570 [00:05<00:16, 487.28it/s]\u001b[A\n",
      " 24%|████████▊                            | 2522/10570 [00:05<00:17, 464.07it/s]\u001b[A\n",
      " 24%|████████▉                            | 2570/10570 [00:06<00:19, 408.91it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2613/10570 [00:06<00:19, 410.41it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2659/10570 [00:06<00:18, 423.54it/s]\u001b[A\n",
      " 26%|█████████▍                           | 2705/10570 [00:06<00:18, 432.18it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2753/10570 [00:06<00:17, 443.67it/s]\u001b[A\n",
      " 27%|█████████▊                           | 2805/10570 [00:06<00:16, 462.93it/s]\u001b[A\n",
      " 27%|██████████                           | 2859/10570 [00:06<00:16, 481.81it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2910/10570 [00:06<00:15, 488.84it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2960/10570 [00:06<00:15, 476.66it/s]\u001b[A\n",
      " 28%|██████████▌                          | 3009/10570 [00:07<00:16, 471.02it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3064/10570 [00:07<00:15, 491.79it/s]\u001b[A\n",
      " 29%|██████████▉                          | 3114/10570 [00:07<00:15, 478.78it/s]\u001b[A\n",
      " 30%|███████████                          | 3163/10570 [00:07<00:16, 438.61it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3208/10570 [00:07<00:16, 436.11it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3257/10570 [00:07<00:16, 449.69it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3312/10570 [00:07<00:15, 475.57it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3370/10570 [00:07<00:14, 501.72it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3422/10570 [00:07<00:14, 502.22it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3473/10570 [00:08<00:16, 442.69it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3520/10570 [00:08<00:17, 398.70it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3574/10570 [00:08<00:16, 432.25it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3629/10570 [00:08<00:15, 461.66it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3678/10570 [00:08<00:14, 467.23it/s]\u001b[A\n",
      " 35%|█████████████                        | 3727/10570 [00:08<00:14, 460.89it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3775/10570 [00:08<00:14, 463.87it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3823/10570 [00:08<00:14, 464.47it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3870/10570 [00:08<00:17, 392.28it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3915/10570 [00:09<00:16, 405.79it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3959/10570 [00:09<00:15, 414.66it/s]\u001b[A\n",
      " 38%|██████████████                       | 4005/10570 [00:09<00:15, 425.12it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4062/10570 [00:09<00:14, 459.62it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4114/10570 [00:09<00:13, 475.82it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4163/10570 [00:09<00:14, 433.57it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4208/10570 [00:09<00:19, 327.77it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4256/10570 [00:09<00:17, 360.12it/s]\u001b[A\n",
      " 41%|███████████████                      | 4297/10570 [00:10<00:20, 306.70it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4333/10570 [00:10<00:20, 307.12it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4367/10570 [00:10<00:20, 298.54it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4410/10570 [00:10<00:18, 328.11it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4459/10570 [00:10<00:16, 362.90it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4502/10570 [00:10<00:16, 378.21it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4545/10570 [00:10<00:15, 391.77it/s]\u001b[A\n",
      " 43%|████████████████                     | 4592/10570 [00:10<00:14, 411.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|████████████████▏                    | 4635/10570 [00:10<00:14, 399.90it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4677/10570 [00:11<00:14, 395.80it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4735/10570 [00:11<00:13, 435.63it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4781/10570 [00:11<00:13, 435.67it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4839/10570 [00:11<00:12, 466.56it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4888/10570 [00:11<00:13, 419.85it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4932/10570 [00:11<00:14, 384.92it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4985/10570 [00:11<00:13, 418.78it/s]\u001b[A\n",
      " 48%|█████████████████▌                   | 5035/10570 [00:11<00:12, 437.99it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5088/10570 [00:11<00:11, 461.93it/s]\u001b[A\n",
      " 49%|█████████████████▉                   | 5141/10570 [00:12<00:11, 478.83it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5191/10570 [00:12<00:11, 483.98it/s]\u001b[A\n",
      " 50%|██████████████████▎                  | 5241/10570 [00:12<00:12, 437.51it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5287/10570 [00:12<00:13, 387.49it/s]\u001b[A\n",
      " 51%|██████████████████▋                  | 5341/10570 [00:12<00:12, 421.65it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5386/10570 [00:12<00:12, 423.09it/s]\u001b[A\n",
      " 51%|███████████████████                  | 5430/10570 [00:12<00:12, 419.66it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5474/10570 [00:12<00:12, 415.84it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5520/10570 [00:13<00:11, 427.47it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5564/10570 [00:13<00:13, 383.49it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5604/10570 [00:13<00:13, 359.65it/s]\u001b[A\n",
      " 53%|███████████████████▊                 | 5643/10570 [00:13<00:13, 368.20it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5685/10570 [00:13<00:12, 381.92it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5739/10570 [00:13<00:11, 417.38it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5792/10570 [00:13<00:10, 443.49it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5838/10570 [00:13<00:10, 443.92it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5884/10570 [00:13<00:11, 411.61it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5927/10570 [00:14<00:12, 361.26it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5978/10570 [00:14<00:11, 394.99it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6020/10570 [00:14<00:11, 400.46it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6062/10570 [00:14<00:11, 398.69it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6112/10570 [00:14<00:10, 423.61it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6161/10570 [00:14<00:10, 439.73it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 6206/10570 [00:14<00:10, 436.08it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6251/10570 [00:14<00:10, 422.17it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6294/10570 [00:14<00:10, 412.73it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6336/10570 [00:15<00:12, 349.99it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6373/10570 [00:15<00:12, 331.69it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6414/10570 [00:15<00:11, 351.64it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6467/10570 [00:15<00:10, 391.01it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6521/10570 [00:15<00:09, 425.94it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6570/10570 [00:15<00:09, 441.19it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6617/10570 [00:15<00:09, 431.83it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6664/10570 [00:15<00:08, 440.97it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6710/10570 [00:15<00:09, 389.39it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6751/10570 [00:16<00:11, 346.15it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6788/10570 [00:16<00:11, 315.36it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6822/10570 [00:16<00:11, 317.92it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6856/10570 [00:16<00:11, 318.39it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6889/10570 [00:16<00:11, 317.91it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6922/10570 [00:16<00:13, 267.28it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6951/10570 [00:16<00:15, 238.93it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6984/10570 [00:16<00:13, 260.26it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7029/10570 [00:17<00:11, 296.36it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7062/10570 [00:17<00:11, 299.85it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7096/10570 [00:17<00:11, 309.49it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7129/10570 [00:17<00:11, 297.17it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7160/10570 [00:17<00:12, 270.07it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7189/10570 [00:17<00:12, 271.07it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7220/10570 [00:17<00:11, 281.33it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7250/10570 [00:17<00:11, 285.94it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7280/10570 [00:17<00:11, 289.70it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7332/10570 [00:18<00:09, 331.91it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7379/10570 [00:18<00:08, 362.41it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7421/10570 [00:18<00:08, 377.20it/s]\u001b[A\n",
      " 71%|██████████████████████████           | 7461/10570 [00:18<00:09, 337.22it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7497/10570 [00:18<00:09, 316.32it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7533/10570 [00:18<00:09, 325.76it/s]\u001b[A\n",
      " 72%|██████████████████████████▍          | 7567/10570 [00:18<00:09, 327.39it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7606/10570 [00:18<00:08, 341.41it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7641/10570 [00:18<00:09, 320.35it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7679/10570 [00:19<00:08, 335.79it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7734/10570 [00:19<00:07, 379.76it/s]\u001b[A\n",
      " 74%|███████████████████████████▏         | 7775/10570 [00:19<00:08, 337.29it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7812/10570 [00:19<00:08, 311.33it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7848/10570 [00:19<00:08, 324.29it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7902/10570 [00:19<00:07, 368.34it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7957/10570 [00:19<00:06, 408.35it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8010/10570 [00:19<00:05, 437.82it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8057/10570 [00:20<00:06, 372.32it/s]\u001b[A\n",
      " 77%|████████████████████████████▎        | 8099/10570 [00:20<00:06, 354.53it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8146/10570 [00:20<00:06, 382.52it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8192/10570 [00:20<00:05, 401.80it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8245/10570 [00:20<00:05, 432.64it/s]\u001b[A\n",
      " 78%|█████████████████████████████        | 8291/10570 [00:20<00:06, 353.01it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8331/10570 [00:20<00:06, 329.99it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8368/10570 [00:20<00:06, 328.92it/s]\u001b[A\n",
      " 80%|█████████████████████████████▍       | 8404/10570 [00:21<00:06, 312.58it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8438/10570 [00:21<00:06, 317.54it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8471/10570 [00:21<00:06, 311.09it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8513/10570 [00:21<00:06, 336.35it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8569/10570 [00:21<00:05, 380.83it/s]\u001b[A\n",
      " 81%|██████████████████████████████▏      | 8610/10570 [00:21<00:05, 374.73it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8650/10570 [00:21<00:05, 339.65it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8702/10570 [00:21<00:04, 378.62it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8745/10570 [00:21<00:04, 391.81it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8787/10570 [00:22<00:04, 359.95it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████▉      | 8825/10570 [00:22<00:05, 337.20it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8878/10570 [00:22<00:04, 378.29it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8932/10570 [00:22<00:03, 415.58it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8977/10570 [00:22<00:04, 373.14it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9018/10570 [00:22<00:04, 377.69it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9062/10570 [00:22<00:03, 393.49it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9105/10570 [00:22<00:03, 403.71it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9147/10570 [00:22<00:03, 408.11it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9192/10570 [00:23<00:03, 417.34it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9240/10570 [00:23<00:03, 434.31it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9294/10570 [00:23<00:02, 460.35it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9345/10570 [00:23<00:02, 473.83it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9394/10570 [00:23<00:02, 449.26it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9440/10570 [00:23<00:02, 392.63it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9490/10570 [00:23<00:02, 417.88it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9546/10570 [00:23<00:02, 452.29it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9602/10570 [00:23<00:02, 478.16it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9660/10570 [00:24<00:01, 503.88it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9713/10570 [00:24<00:01, 483.68it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▏  | 9763/10570 [00:24<00:02, 397.90it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9808/10570 [00:24<00:01, 410.90it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9862/10570 [00:24<00:01, 442.00it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9909/10570 [00:24<00:01, 441.04it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9955/10570 [00:24<00:01, 444.52it/s]\u001b[A\n",
      " 95%|██████████████████████████████████  | 10001/10570 [00:24<00:01, 440.58it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10046/10570 [00:24<00:01, 411.48it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10089/10570 [00:25<00:01, 397.35it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10133/10570 [00:25<00:01, 408.14it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10175/10570 [00:25<00:00, 406.78it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10227/10570 [00:25<00:00, 435.12it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10280/10570 [00:25<00:00, 457.53it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10335/10570 [00:25<00:00, 480.73it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10385/10570 [00:25<00:00, 468.00it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10433/10570 [00:25<00:00, 417.42it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10483/10570 [00:25<00:00, 437.40it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:26<00:00, 404.65it/s]\u001b[A\n",
      "05/15/2021 18:57:47 - INFO - utils_qa -   Saving predictions to models/eval_predictions.json.\n",
      "05/15/2021 18:57:47 - INFO - utils_qa -   Saving nbest_preds to models/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 225/225 [02:42<00:00,  1.39it/s]\n",
      "[INFO|trainer_pt_utils.py:907] 2021-05-15 18:57:52,609 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-15 18:57:52,609 >>   epoch        =     2.0\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-15 18:57:52,609 >>   eval_samples =   10790\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-15 18:57:52,609 >>   exact_match  = 86.1968\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-15 18:57:52,609 >>   f1           = 92.3072\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path roberta-base \\\n",
    "  --dataset_name squad \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 4 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
